{"rhgIgTSSxW": {"paper_info": {"Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "tabular, tabular data, architecture, deep learning, neural networks", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "TL;DR": "TabR is a new tabular DL model with a k-nearest-neighbors-like component and strong results on public benchmarks.", "Abstract": "Deep learning (DL) models for tabular data problems (e.g. classification, regression) are currently receiving increasingly more attention from researchers.\nHowever, despite the recent efforts, the non-DL algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution for these problems.\nOne of the research directions aimed at improving the position of tabular DL involves designing so-called retrieval-augmented models.\nFor a target object, such models retrieve other objects (e.g. the nearest neighbors) from the available training data and use their features and labels to make a better prediction.In this work, we present TabR -- essentially, a feed-forward network with a custom k-Nearest-Neighbors-like component in the middle.\nOn a set of public benchmarks with datasets up to several million objects, TabR marks a big step forward for tabular DL: it demonstrates the best average performance among tabular DL models, becomes the new state-of-the-art on several datasets, and even outperforms GBDT models on the recently proposed \"GBDT-friendly\" benchmark (see Figure 1).\nAmong the important findings and technical details powering TabR, the main ones lie in the attention-like mechanism that is responsible for retrieving the nearest neighbors and extracting valuable signal from them.\nIn addition to the higher performance, TabR is simple and significantly more efficient compared to prior retrieval-based tabular DL models.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "Supplementary Material": "zip", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Primary Area": "general machine learning (i.e., none of the above)", "Submission Number": "9502", "PDF Url": "https://openreview.net/pdf?id=rhgIgTSSxW"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:53 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nAccept (poster)"}, {"Heading": "Meta Review of Submission9502 by Area Chair 6yPy", "Subheading": "Meta ReviewbyArea Chair 6yPy08 Dec 2023, 10:53 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThis submission contributes a neural architecture dedicated to tabular learning based on combining a feed-forward network with a nearest neighbor mechanism. The submission generated many solid discussions and was seen as an interesting addition to the tabular-learning literature. The reviewers appreciated the extensive experiments, the writing, and the reproducibility of the work. More baselines could have been added, and more attention to categorical variables.\nJustification For Why Not Higher Score:\nThe paper is already borderline with regards to acceptance. I do not think that we can push it further.\nJustification For Why Not Lower Score:\nThe work seems solid, as acknowledged by 3 of the 4 reviewers. The answers to the review by the authors were also solid. The fourth reviewer seems unfair, and is the author of one of the competing methods puts forward in the critical review. The work seems to honestly position itself relative to this prior art."}, {"Heading": "Dear Reviewers (rebuttal)", "Subheading": "Official CommentbyAuthors18 Nov 2023, 11:47Everyone", "Content": "Comment:\nWe thank the reviewers for the feedback!\nIn this post\n:\n(1) We summarize positive things from the reviews.\n(2) We summarize the changes to the updated PDF document.\nIn the individual replies\n, we address other comments.\n(1) Positive things\nStrong empirical performance of the proposed method\naaV3\n:\n\"significant advance over prior work ... the first deep learning model to outperform GBDT on an ensemble of datasets.\"\nLy5o\n:\n\"TabR demonstrates superior performance compared to GBDT and other retrieval-based tabular deep learning models ...\"\nEnVq\n:\n\"method has managed to outperform tree based models like XGBoost on middle-scale datasets. ... This model is the best-performing retrieval based model.\"\nfJsx\n:\n\"Strong results relative to both deep learning and boosted tree methods ... TabR-S's relatively strong performance relative to out-of-the-box boosted tree libraries\"\nThe proposed method\nfJsx\n:\n\"this [strong performance] suggests this isn't just excessive parameter tweaking and overfitting via architecture search.\"\nLy5o\n:\n\"The new similarity module in TabR has a reasonable intuitive motivation, allowing it to find and exploit natural hints in the data for better predictions.\"\naaV3\n:\n\"the techniques are overall not too complex.\"\nExperimental setup\nEnVq\n:\n\"the experiments are comprehensive\"\naaV3\n:\n\"The experiments and analysis are quite extensive. Multiple datasets of different kinds of data ...\"\nPresentation\nfJsx\n:\n\"Easy to read, with key pieces of information generally emphasized appropriately.\"\naaV3\n:\n\"Clear articulation of which techniques helped.\"\nEnVq\n:\n\"Overall, the presentation is clear ... The details are clear ...\"\nReproducibility\nfJsx\n:\n\"The extensive amount of open-sourcing and experiment reproducibility is greatly appreciated.\"\nEnVq\n:\n\"the model is highly reproducible\"\n(2) Changes to the PDF\nMain text\nOnly small fixes and wording improvements:\n[fJsx]\n(Section 2) An improved sentence contrasting our model TabR against prior solutions (the\n\"Retrieval-augmented models for tabular data problems\"\nparagraph).\n[fJsx]\n(Section 3.1) A small fix to notation.\n[fJsx]\n(Section 3.2) A similar notation-related fix (the \"Retrieval module\" paragraph).\n[fJsx, EnVq]\n(Section 3.2) An improved sentence for a smoother transition from Step-2 to Step-3 (the \"Step-3\" paragraph).\n[EnVq]\n(Section 3.1) Fixed the typo (the \"Notation\" paragraph).\n[EnVq]\n(Section 3.2) Point to the available analysis in Appendix (the \"Step-2\" and \"Step-3\" paragraphs).\n[EnVq]\n(Section 2) Added citations of TANGOS\n[1]\nand TabPFN\n[2]\n(the \"Parametric deep learning models\" paragraph was renamed to \"Tabular deep learning\").\nAppendix\nThe content added based on the reviews:\n[Ly5o]\n(Section A.6) The ablation study for the context size $m=96$ used in the paper.\n[EnVq]\n(Section A.2.1) The extended motivation for the value module $\\mathcal{V}$ (and, for the sake of symmetry, we changed to name of Section A.1.1 to \"Motivation\").\n[EnVq]\n(a new bullet in Section A.8) The explanation for why \"LinearWithoutBias\" is used in Equation 4.\n[aaV3, Ly5o]\n(Section A.4.2) The comparison of the inference efficiency of TabR and XGBoost.\nReferences\n[1]\n\"TANGOS: Regularizing Tabular Neural Networks through Gradient Orthogonalization and Specialization\", ICLR 2023, Jeffares et ak.\n[2]\n\"TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second\", ICLR 2023, Hollmann et al."}, {"Heading": "Official Review of Submission9502 by Reviewer aaV3", "Subheading": "Official ReviewbyReviewer aaV331 Oct 2023, 23:36 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper considers the problem of making predictions on tabular data. The authors propose a retrieval-augmented approach where a predictor takes the representation not of the table being predicted but also the representation of the nearest neighbors from a training dataset. The encoding representations and the predictors are training together and use straightforward architecture architectures. The main result is that a combination of the carefully crafted techniques outperforms GBDT on an ensemble of tasks. The training time is higher than GBDT but not unreasonable, and better compared to prior deep learning methods. The prediction times are better\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nThe results seem to be a significant advance over prior work in tabular data predictions. In particular, the first deep learning model to outperform GBDT on an ensemble of datasets.\nThe experiments and analysis are quite extensive. Multiple datasets of different kinds of data, analysis of training and prediction times.\nClear articulation of which techniques helped. the techniques are overall not too complex.\nWeaknesses:\nA comparison of the inference and query complexity between the methods is lacking.\nQuestions:\nInference time and compexity -- are the studies based on normalized inference time between models? If not, could you comment more? How does the inference complexity depend on the size of the table data?\nCould a different selection of datasets prove that the tabR is not superior to GBDT? In other words, are these datasets highly representative?\nIs it not surprising that Step-1 (adding context labels) did not help that much? One would guess that this is a big component of signal in retrieval augmentation.\nNot a question, but the methodology here reminds one of extreme classification and specifically this paper.\nhttps://arxiv.org/abs/2207.04452\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n8: accept, good paper\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Rebuttal (part 1/2)", "Subheading": "Official CommentbyAuthors18 Nov 2023, 11:40Everyone", "Content": "Comment:\nWe thank the reviewer for the positive feedback!\nA comparison of the inference and query complexity between the methods is lacking.\nIn the new PDF, this is addressed in Section A.4.2.\nFor convenience, here, we provide a summary.\nBelow, we report the inference throughput of TabR and XGBoost.\n(The technical setup:\nXGBoost and TabR-S with tuned hyperparameters as in Table 4 of the main text\nComputation is performed on NVIDIA 2080 Ti.\nFor both models, objects are passed by batches of 4096 objects.)\nThe key observations:\nOn the considered datasets, the throughputs of TabR and XGBoost are mostly comparable.\nImportant\n: our implementation of TabR is naive and lacks even basic optimizations.\nCH\nCA\nHO\nAD\nDI\nOT\nHI\nBL\nWE\nCO\nMI\n#trainingObjects\n6400\n13209\n14581\n26048\n34521\n39601\n62751\n106764\n296554\n371847\n723412\n#features\n11\n8\n16\n14\n9\n93\n28\n9\n119\n54\n136\nn_estimators\nin XGBoost\n121\n3997\n1328\n988\n802\n524\n1040\n1751\n3999\n1258\n3814\nmax_depth\nin XGBoost\n5\n9\n7\n10\n13\n13\n11\n8\n13\n12\n12\nXGBoost throughput (obj./sec.)\n2197k\n33k\n179k\n131k\n417k\n19k\n72k\n84k\n15k\n10k\n14k\nTabR-S throughput (obj./sec.)\n35k\n35k\n55k\n33k\n43k\n40k\n37k\n27k\n34k\n23k\n11k\nOverhead\n62.3\n0.9\n3.3\n3.9\n9.6\n0.5\n1.9\n3.1\n0.5\n0.4\n1.2\nIn more detail:\nOn \"non-simple\" tasks (CA, OT, WE, CO), TabR is faster (\"non-simple\" = XGBoost needs many trees AND/OR XGBoost needs high depth AND/OR a dataset has more features).\nOn \"simple\" tasks, XGBoost is faster (\"simple\" = XGBoost is shallow AND/OR dataset has few features).\nWith the growth of training size (MI), TabR may become slower because of the retrieval, however, there is\na lot\nof room for optimizations (caching candidate representations instead of recomputing them on each forward pass; using float16 instead of the current float32; doing approximate search instead of the current brute force; using only a subset of the training data as candidates, etc.)\nInference time and compexity -- are the studies based on normalized inference time between models? If not, could you comment more?\nStrictly speaking, the results reported in all main tables are obtained without fixing the inference time budget: all methods can spend any time they need to make predictions.\nHowever,\ngenerally, the efficiency of TabR vs. prior work is an important storyline of our paper with significant wins over prior work, and, in the considered scope of datasets, the inference time of TabR is always reasonable (as indicated by the above table).\nHow does the inference complexity depend on the size of the table data?\nAs indicated by the above table, for datasets of sizes up to (roughly) 500K objects, for TabR-S, the dataset size is not a major factor defining the inference throughput. For larger datasets, the dataset size becomes a more important factor (formally, for the current brute force search, the search complexity grows linearly with the dataset size). Luckily, the current implementation of TabR has a lot of room for simple optimizations:\ncaching candidate key representations instead of recomputing them on each forward pass.\nperforming the search in float16 instead of the current float32.\nperforming approximate similarity search instead of the current brute force.\nusing only a subset of the training data as candidates.\netc."}, {"Heading": "Rebuttal (part 2/2)", "Subheading": "Official CommentbyAuthors18 Nov 2023, 11:41Everyone", "Content": "Comment:\nare these datasets highly representative?\nWe did our best to build a representative benchmark using\n~50 datasets from prior work\n.\nIn particular, we ensured that there are\nmany challenging tasks that favour GBDT over traditional neural networks.\nExamples:\nWe use the widely cited\n[1]\nwhere GBDT was shown to be superior to neural networks.\nWe use some datasets from\n[2]\nincluding multiple GBDT-friendly tasks.\nWe also use the real world large scale dataset for weather prediction from\n[3]\n.\nTo put our work in the context of prior work, here are the main properties of the benchmarks used in some prior work on tabular deep learning:\nModel\n[citation]\nThe number of datasets used in the paper\nTabR\n[ours]\n~50 datasets (roughly,\n[1]\n+ some datasets from\n[2]\nand\n[3]\n)\nNODE\n[7]\n6 datasets\nTabNet\n[5]\n<10 datasets\nFT-Transformer\n[4]\n11 datasets\nSAINT\n[6]\n30 datasets\n<survey>\n[11]\n5 datasets\nMLP-PLR\n[2]\n11 datasets\nTabPFN\n[8]\n67 small datasets (different scope)\n<benchmark>\n[1]\n<50 *unique* datasets (>50 with multiple versions of the same datasets)\nTANGOS\n[9]\n20 datasets\nT2G-FORMER\n[10]\n12 datasets\nIs it not surprising that Step-1 (adding context labels) did not help that much?\nThis is discussed in the \"Step-1\" paragraph on Page 5:\n> \"Table 2 shows no improvements from using labels, which is counter-intuitive. Perhaps, the similarity module taken from the vanilla attention does not allow benefiting from such a valuable signal as labels.\"\nIn other words, yes, this is indeed surprising! However, after the subsequent Step-2 in Section 3.2 (and, in particular, after the ablation study in Section A.3), we can confidently say that\nto benefit from a valuable signal (such as labels), good similarity module is required.\nReferences\n[1]\n\"Why do tree-based models still outperform deep learning on tabular data?\", NeurIPS 2022 Datasets and Benchmarks, Grinsztajn et al.\n[2]\n\"On Embeddings for Numerical Features in Tabular Deep Learning\", NeurIPS 2022, Gorishniy et al.\n[3]\n\"Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks\", NeurIPS 2021 Datasets and Benchmarks , Malinin et al.\n[4]\n\"Revisiting Deep Learning Models for Tabular Data\", NeurIPS 2021, Gorishniy et al.\n[5]\n\"TabNet: Attentive Interpretable Tabular Learning\", AAAI 2021, Arik et al.\n[6]\n\"SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training\", ICLR 2022 submission, Somepalli et al.\n[7]\n\"Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data\", ICLR 2020, Popov et al.\n[8]\n\"TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second\", ICLR 2023, Hollmann et al.\n[9]\n\"TANGOS: Regularizing Tabular Neural Networks through Gradient Orthogonalization and Specialization\", ICLR 2023, Jeffares et al.\n[10]\n\"T2G-FORMER: Organizing Tabular Features into Relation Graphs Promotes Heterogeneous Feature Interaction\", AAAI 2023, Yan et al.\n[11]\n\"Deep Neural Networks and Tabular Data: A Survey\""}, {"Heading": "inference time comparison", "Subheading": "Official CommentbyReviewer aaV320 Nov 2023, 17:02Everyone", "Content": "Comment:\nIs it reasonable to compare XGBoost with deep learning with 4096 batch size on GPU. How do the inference times compare on CPU or with small batching as is the case with inference in reality?"}, {"Heading": "Inference efficiency: extended analysis", "Subheading": "Official CommentbyAuthors22 Nov 2023, 19:18Everyone", "Content": "Comment:\nWe thank the reviewer for the questions!\nWe are excited to present an\nextended study on the inference throughput covering SEVEN models in TWO modes.\nWe did not update the PDF document with the new content, but we are committed to do that based on feedback.\nThe main observations\n:\nTabR is faster than prior retrieval-based tabular DL (SAINT).\nTabR is slower than simple retrieval-free models.\nNotably, TabR is only moderately slower than FT-Transformer (a non-retrieval model).\n(not reflected in the tables below)\nOn large dataset, where the retrieval becomes the bottleneck, TabR can be made\nsignificantly\nfaster (e.g. by order of magnitude) by using approximate nearest neighbor search instead of the current brute force.\nTechnical details:\nWe did\nnot\nuse DL-specific optimizations like\ntorch.compile\n, mixed precision computation, pruning and other techniques.\nModels with tuned hyperparameters are used (that is, from Table 3 and Table 4). For FT-Transformer, the tuned hyperparameters were taken from\n[1]\nand\n[2]\nfor most datasets, and the default hyperparemeters were used on two datasets.\nThe first mode: \"Online predictions\"\nDevice: CPU Intel Core i7-7800X 3.50GHz\nThread count: 1\nBatch size: 1\nNotation\nValues are aggregated over the 11 datasets from Table 1 (the \"default\" benchmark)\n(A) ~ Absolute throughput (objects per second)\n(R) ~ Relative throughput w.r.t. TabR-S\n[CPU & Batch size = 1]\n(A) Min\n(A) Median\n(A) Max\n(R) Min\n(R) Median\n(R) Max\nTabR-S\n31\n470\n2.0K\n1\n1\n1\nTabR\n26\n236\n1.2K\n0.2\n0.5\n1.3\nSAINT\n< 1\n15\n59\n< 0.01\n0.02\n0.1\nXGBoost\n270\n2.2K\n10K\n0.6\n5.5\n17.5\nMLP\n2.6K\n4.8K\n15K\n4.0\n13.9\n387.1\nMLP-PLR\n532\n1.6K\n5.1K\n0.9\n5.2\n47.2\nFT-Transformer\n219\n694\n1.7K\n0.4\n1.6\n11.6\nThe second mode: \"Offline batch processing\"\nDevice: GPU NVIDIA 2080Ti 12GB\nBatch size: the largest possible batch size for a given model (but no larger then\n2 ** 17 ~= 128_000\n)\nIt turns out that XGBoost can achieve better numbers in this mode than with the fixed batch size 4096.\nNotation\nValues are aggregated over the 11 datasets from Table 1 (the \"default\" benchmark)\n(A) ~ Absolute throughput (objects per second)\n(R) ~ Relative throughput w.r.t. TabR-S\n[CUDA & Batch size = max]\n(A) Min\n(A) Median\n(A) Max\n(R) Min\n(R) Median\n(R) Max\nTabR-S\n24K\n123K\n319K\n1\n1\n1\nTabR\n28K\n91K\n308K\n0.4\n0.8\n2.5\nSAINT\n2.1K\n32K\n133K\n0.05\n0.3\n0.8\nXGBoost\n63K\n1.4M\n16.6M\n0.9\n11.4\n126.8\nMLP\n3.2M\n7.7M\n47.8M\n23.9\n67.4\n512\nMLP-PLR\n63K\n1.1M\n8.5M\n0.5\n6.7\n101.7\nFT-Transformer\n21K\n198K\n752K\n0.2\n2\n4.5\nReferences\n[1]\n\"Revisiting Deep Learning Models for Tabular Data\", Gorishniy et al.\n[2]\n\"On Embeddings for Numerical Features in Tabular Deep Learning\", Gorishniy et al."}]}, {"Heading": "Official Review of Submission9502 by Reviewer fJsx", "Subheading": "Official ReviewbyReviewer fJsx31 Oct 2023, 23:13 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis work proposes a retrieval-augmented deep learning architecture for tabular regression/classification. The model passes $x$, the row to be classified/predicted, as well as additional retrieval context rows, through a learned encoder. TabR then retrieves the rows most similar to the encoded form of $x$, where similarity is defined as the Euclidean distance between the encoded versions of two rows, mapped through a linear layer. The top retrieval candidates and their respective labels are then sent through some more learned transformations before being aggregated and combined with the encoded form of the row to be classified/regressed. This combined embedding goes through more MLP layers to result in the output.\nThe paper goes through variants of the architecture and how each respective change impacts performance. It then compares against other deep learning-based models as well as gradient boosted decision trees. In both default-hyperparameter and tuned-hyperparameter settings, TabR performs well.\nSoundness:\n4 excellent\nPresentation:\n4 excellent\nContribution:\n2 fair\nStrengths:\nThe extensive amount of open-sourcing and experiment reproducibility is greatly appreciated.\nStrong results relative to both deep learning and boosted tree methods, and TabR-S's relatively strong performance relative to out-of-the-box boosted tree libraries suggests this isn't just excessive parameter tweaking and overfitting via architecture search.\nEasy to read, with key pieces of information generally emphasized appropriately.\nWeaknesses:\nPaper doesn't go into detail describing differences with prior deep learning-based tabular methods. What might explain the performance differences? Ex. \"prior work, where several layers with multi-head attention between objects and features are often used\" but was this what led to retrieval's low benefit in the past?\nInsufficient discussion of categorical variables. Is accuracy or training time particularly affected by their relative abundance relative to numerical features?\nThe steps of Section 3.2 seem rather arbitrary. Some of the detail could be compressed to make room for more intuition why the final architecture makes more sense (content from A.1.1). Description of architectural changes that didn't work would also be very insightful.\nPaper describes training times in A.4, but I believe a summary of this is important enough to warrant inclusion in the main paper. Something like a mention of the geometric mean (over the datasets) of the ratio between TabR's training time to a gradient boosted methods, described in the conclusion, would be sufficient. While the ratio is likely >1, it is better to acknowledge this weakness than to hide it.\nQuestions:\nSee weaknesses. Also, what is $I_{cand}$? Is it all rows of the table that labels have been provided for? It's mentioned in page 3 that \"we use the same set of candidates for all input objects\" but what it the set of candidates exactly?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Rebuttal (part 1/2)", "Subheading": "Official CommentbyAuthors18 Nov 2023, 11:42 (modified: 18 Nov 2023, 11:44)EveryoneRevisions", "Content": "Comment:\nWe thank the reviewer for the comments and questions!\nWhat might explain the performance differences? ... but was this what led to retrieval's low benefit in the past?\n(1)\nTL;DR: yes, the main reason behind the difference in the performance lies in the retrieval module designed in Section 3.2.\nA quick summary of the main differences in the retrieval-related functionality:\nPrior work:\nmultiple multi-head vanilla\nattention modules, where the attention is performed\nbetween objects and features\n.\nTabR:\none single-head custom\nattention-like module, where the attention is performed\nonly between objects\n.\nIn the new PDF, in Section 2 on Page 3, we improved the wording and the formatting to better contrast TabR against prior solutions.\n(2)\nThe relevant parts from the original PDF\n(here, we uppercased the key pieces for convenience):\n(Paragraph \"Step 2\" Page 5, where we improved the similarity module commonly used in prior work)\nThis change is a turning point in our story, WHICH WAS OVERLOOKED IN PRIOR WORK.\n(abstract)\n\"NOVEL FINDINGS ... LIE IN THE ATTENTION-LIKE MECHANISM that is responsible for retrieving the nearest neighbors and extracting valuable signal from them.\"\n(Section 2 Page 3)\n\"Compared to prior work, where SEVERAL LAYERS WITH MULTI-HEAD ATTENTION ... TabR implements its retrieval component with just ONE SINGLE-HEAD ATTENTION-LIKE MODULE. Importantly ... module of TabR is CUSTOMIZED in a way that makes it BETTER SUITED FOR TABULAR DATA PROBLEMS\"\netc.\nInsufficient discussion of categorical variables.\nHowever, as indicated by Figure 3 and its caption,\nTabR is equally capable of handling all feature types, there is no conceptual difference between them.\nIn particular:\nCategorical features are encoded with the one-hot encoding (as mentioned in the caption of Figure 3 and in Section D6). This is a simple and efficient operation, and one can switch to any other encoding scheme if needed.\nContinuous features are normalized and, optionally, transformed with embeddings from\n[2]\n(which, in fact, makes continuous features more demanding than categorical features in terms of compute).\nJust in case, we will mention that\nthe used datasets include both continuous and categorical features\n:\nThe default benchmark: Table 1 provides information on how many features of each type is presented in each dataset.\nThe benchmark from\n[1]\nalso includes many datasets with categorical features.\nIs accuracy or training time particularly affected by their relative abundance relative to numerical features?\nWe did not notice any different behaviour caused by the presence of categorical features, which is consistent with the explanation provided above (\n\"... for TabR, there is no conceptual difference between feature types ...\"\n).\nThe steps of Section 3.2 seem rather arbitrary.\nFirst\n, just in case, we provide a quick informal recap of the story behind the design steps in Section 3.2:\n\"Step-0. The vanilla-attention-like baseline\" is motivated by prior work.\n\"Step-1. Adding context labels\" is a natural attempt to use the available labels of the neighbors. The important outcome:\n\"the similarity module $\\mathcal{S}$ taken from the vanilla attention does not allow benefiting from such a valuable signal as labels\"\n.\n\"Step-2. Improving the similarity module $\\mathcal{S}$\" is fully motivated by the quoted outcome of Step-1.\n\"Step-3. Improving the value module $\\mathcal{V}$\" is inspired by a simple observation that $\\mathcal{V}$ can be made more expressive by adding the dependency on the target object's representation $\\tilde{x}$.\nStep-4 is the only purely technical step where we identified better technical defaults for the new architecture.\nSecond\n, how we improved the communication and made the available content more visible in the new PDF:\nIn Section A.2.1, we added extended motivation behind the design of the value module of TabR.\nIn the \"Step-2\" and \"Step-3\" paragraphs, we added references to the extended motivation and analysis available in the appendix.\nIn the \"Step-3\" paragraph, we improved the wording to highlight the transition of our focus from the similarity module to the value module."}, {"Heading": "Rebuttal (part 2/2)", "Subheading": "Official CommentbyAuthors18 Nov 2023, 11:42Everyone", "Content": "Comment:\nPaper describes training times in A.4, but I believe a summary of this is important enough to warrant inclusion in the main paper. Something like a mention of the geometric mean (over the datasets) of the ratio between TabR's training time to a gradient boosted methods, described in the conclusion, would be sufficient.\nWe are excited to discuss the aspect of efficiency in more detail!\nRegarding the suggestion to announce Table 10 (training times), currently, we achieve that and prepare readers for Table 10 by:\nrepeatedly mentioning two things throughout the paper:\nTabR is more efficient than prior retrieval-based tabular DL.\nTabR is less efficient than simple models and may require special considerations on larger datasets.\ndirectly mentioning Section A.4 and Section A.4.1 multiple times.\nImportantly\n, we believe that Table 10 (training times) needs a full-fledged presentation and nuanced discussion, and it may suffer from simplifications. In particular, announcing Table 10 with a single number can be misleading: for example, the suggested geometric mean of the ratios will completely hide absolute training times, which is what actually matters in practice. To avoid such effects, we report specific numbers for both \"positive\" (e.g. order(s) of magnitude improvements over prior work) and \"negative\" efficiency-related stories only in Table 10.\nit is better to acknowledge this weakness than to hide it.\nBased on the content from the original submission, let us illustrate that, in fact,\nwe care deeply about efficiency, invest heavily in this aspect of our work, and strive for complete transparency in this matter.\nWe truly consider the storyline about efficiency an important part of our paper. In particular, throughout the work, we repeatedly mention two things:\nTabR is more efficient than prior retrieval-based tabular DL.\nTabR is less efficient than simple models and may require special considerations on larger datasets.\n(The end of Introduction)\n\"Tree-based models, in turn, remain a cheaper solution\"\nThe whole Section 5.1 is dedicated to improving training times of TabR\n.\nEven the seemingly unrelated Section 5.2 mentions that the continual updates can be used to make TabR train faster:\n\"Additionally, this approach can be used to scale TabR to large datasets by training the model on a subset of data and retrieving from the full data.\"\n(Conclusion)\n\"An important direction for future work is improving the efficiency of retrieval-augmented models to make them faster in general and in particular applicable to tens and hundreds of millions of data points\"\n(Limitations on Page 6 refer to Section B, where we write:)\n\"the retrieval module R still causes overhead compared to purely parametric models, so TabR may not scale to truly large datasets as-is\"\nAlso, what is $I_{cand}$? Is it all rows of the table that labels have been provided for?\nIn most of the experiments, yes\n: the neighbors are retrieved from all training objects, which formally means $I_{cand} = I_{train}$.\nStrictly speaking, we implicitly rely on the fact that  $I_{cand}$ can be different from $I_{train}$ in the following places:\nIn Section 5.1, for\ntraining\nobjects,  $I_{cand} = I_{train}$ is true only until the context freeze, after which the retrieval for\ntraining\nobjects is not performed (instead, the same neighbors are reused until the end of training).\nIn Section 5.2, for\ntest\nobjects, $I_{cand} = I_{train}$ is true only until new labeled objects are added to the set of candidates.\nThe\n\"Second, depending on an application ...\"\nparagraph of Section B.\nIt's mentioned in page 3 that \"we use the same set of candidates for all input objects\"\nIn the new PDF, we improved the wording, now it says:\n\"In this work, unless otherwise noted, we use the same set of candidates for all input objects and set $I_{cand} = I_{train}$ (which means retrieving from all training objects)\"\n.\nReferences\n[1]\n\"Why do tree-based models still outperform deep learning on tabular data?\", NeurIPS 2022 Datasets and Benchmarks, Grinsztajn et al.\n[2]\n\"On Embeddings for Numerical Features in Tabular Deep Learning\", NeurIPS 2022, Gorishniy et al."}]}, {"Heading": "Official Review of Submission9502 by Reviewer Ly5o", "Subheading": "Official ReviewbyReviewer Ly5o22 Oct 2023, 16:51 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper introduces TabR, a retrieval-augmented tabular deep learning model that outperforms gradient-boosted decision trees (GBDT) on various datasets. TabR incorporates a novel retrieval module that is similar to the attention mechanism, which helps the model achieve the best average performance among tabular deep learning models and is more efficient compared to prior retrieval-based models.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nTabR demonstrates superior performance compared to GBDT and other retrieval-based tabular deep learning models on multiple datasets.\nThe new similarity module in TabR has a reasonable intuitive motivation, allowing it to find and exploit natural hints in the data for better predictions.\nWeaknesses:\nSome aspects are not clear, see the questions section.\nQuestions:\nWhat's the reason for choosing m to be 96? How does m affect the performance of TabR?\nWhat's the inference efficiency of TabR and how does it compare with other baselines (e.g., GBDT)?\nIs TabR applicable to categorical features? It seems like the paper only considers continuous features.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Rebuttal (part 1/2)", "Subheading": "Official CommentbyAuthors18 Nov 2023, 11:45Everyone", "Content": "Comment:\nWe thank the reviewer for the feedback!\nWhat's the reason for choosing m to be 96? How does m affect the performance of TabR?\nIn the new PDF, this is addressed in Section A.6.\nBelow, we provide a summary for convenience and an explanation for the obtained results.\nFirst,\nto evaluate how performance depends on $m$, we consider TabR-S with default hyperparameters and, for each value of $m$, we report the average rank of TabR-S on the default benchmark (Table 1 of the main text). For each\n(dataset, m)\npair, the performance is computed as the average over five random seeds.\nThe choice of $m$  must be made based on the\nvalidation\nmetrics (not on the\ntest\nmetrics), so we provide the ranks on the\nvalidation\nsets first:\nm=1\nm=2\nm=4\nm=8\nm=16\nm=32\nm=64\nm=96\nm=128\nm=256\navg\n5\n4.5\n4\n3.25\n2.75\n2.12\n2.12\n1.88\n1.88\n1.88\nstd\n2.45\n1.87\n1.73\n1.71\n1.56\n1.45\n1.45\n0.93\n1.05\n1.36\nIt seems that the m=96 turns out to be a reasonable default choice.\nAdditionally, here are the ranks on the\ntest\nsets:\nm=1\nm=2\nm=4\nm=8\nm=16\nm=32\nm=64\nm=96\nm=128\nm=256\navg\n4.12\n3.62\n3.5\n3.12\n2.38\n2.12\n2\n1.62\n1.75\n1.75\nstd\n2.2\n1.8\n1.5\n1.05\n1.41\n1.27\n1.41\n0.99\n0.97\n1.09\nSecond\n, we suggest an explanation for the obtained results. The presence of the softmax function in the retrieval module of TabR gives a hope that the only requirement for $m$ is to be large enough, and softmax will \"automatically\" choose the optimal value for each sample. That said, in this work, we don't analyse extreme cases like $m = |I_{train}|$ and recommend values like 96 as a starting point.\nWhat's the inference efficiency of TabR and how does it compare with other baselines (e.g., GBDT)?\nIn the new PDF, this is addressed in Section A.4.2.\nFor convenience, here, we provide a summary.\nBelow, we report the inference throughput of TabR and XGBoost.\n(The technical setup:\nXGBoost and TabR-S with tuned hyperparameters as in Table 4 of the main text\nComputation is performed on NVIDIA 2080 Ti.\nFor both models, objects are passed by batches of 4096 objects.)\nThe key observations:\nOn the considered datasets, the throughputs of TabR and XGBoost are mostly comparable.\nImportant\n: our implementation of TabR is naive and lacks even basic optimizations.\nCH\nCA\nHO\nAD\nDI\nOT\nHI\nBL\nWE\nCO\nMI\n#trainingObjects\n6400\n13209\n14581\n26048\n34521\n39601\n62751\n106764\n296554\n371847\n723412\n#features\n11\n8\n16\n14\n9\n93\n28\n9\n119\n54\n136\nn_estimators\nin XGBoost\n121\n3997\n1328\n988\n802\n524\n1040\n1751\n3999\n1258\n3814\nmax_depth\nin XGBoost\n5\n9\n7\n10\n13\n13\n11\n8\n13\n12\n12\nXGBoost throughput (obj./sec.)\n2197k\n33k\n179k\n131k\n417k\n19k\n72k\n84k\n15k\n10k\n14k\nTabR-S throughput (obj./sec.)\n35k\n35k\n55k\n33k\n43k\n40k\n37k\n27k\n34k\n23k\n11k\nOverhead\n62.3\n0.9\n3.3\n3.9\n9.6\n0.5\n1.9\n3.1\n0.5\n0.4\n1.2\nIn more detail:\nOn \"non-simple\" tasks (CA, OT, WE, CO), TabR is faster (\"non-simple\" = XGBoost needs many trees AND/OR XGBoost needs high depth AND/OR a dataset has more features).\nOn \"simple\" tasks, XGBoost is faster (\"simple\" = XGBoost is shallow AND/OR dataset has few features).\nWith the growth of training size (MI), TabR may become slower because of the retrieval, however, there is\na lot\nof room for optimizations (caching candidate representations instead of recomputing them on each forward pass; using float16 instead of the current float32; doing approximate search instead of the current brute force; using only a subset of the training data as candidates, etc.)\n(P.S. in the paper, we also analyze\ntraining\nefficiency in Table 10)"}, {"Heading": "Rebuttal (part 2/2)", "Subheading": "Official CommentbyAuthors18 Nov 2023, 11:46Everyone", "Content": "Comment:\nIt seems like the paper only considers continuous features\nIn fact, the used datasets include both continuous and categorical features\n:\nFor the default benchmark, Table 1 in the main text provides information on how many features of each type is presented in each dataset.\nThe benchmark from\n[1]\nalso includes many datasets with categorical features, please, see the original paper for details.\nIs TabR applicable to categorical features?\nYes, TabR is applicable to all kinds of features:\nits input encoder consists of conventional common modules, as explained in the caption of Figure 3. In particular, categorical features are encoded with the one-hot encoding, and, if needed, one can use other encoding/embedding schemes.\nReferences\n[1]\n\"Why do tree-based models still outperform deep learning on tabular data?\", NeurIPS 2022 Datasets and Benchmarks, Grinsztajn et al."}, {"Heading": "Inference efficiency: extended analysis", "Subheading": "Official CommentbyAuthors22 Nov 2023, 19:28Everyone", "Content": "Comment:\nDear reviewer,\nIn our \"Rebuttal (part 1/2)\" reply, we provided some analysis on the inference efficiency.\nNow, we are excited to share an\nextended study on the inference throughput covering SEVEN models in TWO modes.\nWe did not update the PDF document with the new content, but we are committed to do that based on feedback.\nThe main observations\n:\nTabR is faster than prior retrieval-based tabular DL (SAINT).\nTabR is slower than simple retrieval-free models.\nNotably, TabR is only moderately slower than FT-Transformer (a non-retrieval model).\n(not reflected in the tables below)\nOn large dataset, where the retrieval becomes the bottleneck, TabR can be made\nsignificantly\nfaster (e.g. by order of magnitude) by using approximate nearest neighbor search instead of the current brute force.\nTechnical details:\nWe did\nnot\nuse DL-specific optimizations like\ntorch.compile\n, mixed precision computation, pruning and other techniques.\nModels with tuned hyperparameters are used (that is, from Table 3 and Table 4). For FT-Transformer, the tuned hyperparameters were taken from\n[1]\nand\n[2]\nfor most datasets, and the default hyperparemeters were used on two datasets.\nThe first mode: \"Online predictions\"\nDevice: CPU Intel Core i7-7800X 3.50GHz\nThread count: 1\nBatch size: 1\nNotation\nValues are aggregated over the 11 datasets from Table 1 (the \"default\" benchmark)\n(A) ~ Absolute throughput (objects per second)\n(R) ~ Relative throughput w.r.t. TabR-S\n[CPU & Batch size = 1]\n(A) Min\n(A) Median\n(A) Max\n(R) Min\n(R) Median\n(R) Max\nTabR-S\n31\n470\n2.0K\n1\n1\n1\nTabR\n26\n236\n1.2K\n0.2\n0.5\n1.3\nSAINT\n< 1\n15\n59\n< 0.01\n0.02\n0.1\nXGBoost\n270\n2.2K\n10K\n0.6\n5.5\n17.5\nMLP\n2.6K\n4.8K\n15K\n4.0\n13.9\n387.1\nMLP-PLR\n532\n1.6K\n5.1K\n0.9\n5.2\n47.2\nFT-Transformer\n219\n694\n1.7K\n0.4\n1.6\n11.6\nThe second mode: \"Offline batch processing\"\nDevice: GPU NVIDIA 2080Ti 12GB\nBatch size: the largest possible batch size for a given model (but no larger then\n2 ** 17 ~= 128_000\n)\nIt turns out that XGBoost can achieve better numbers in this mode than with the fixed batch size 4096.\nNotation\nValues are aggregated over the 11 datasets from Table 1 (the \"default\" benchmark)\n(A) ~ Absolute throughput (objects per second)\n(R) ~ Relative throughput w.r.t. TabR-S\n[CUDA & Batch size = max]\n(A) Min\n(A) Median\n(A) Max\n(R) Min\n(R) Median\n(R) Max\nTabR-S\n24K\n123K\n319K\n1\n1\n1\nTabR\n28K\n91K\n308K\n0.4\n0.8\n2.5\nSAINT\n2.1K\n32K\n133K\n0.05\n0.3\n0.8\nXGBoost\n63K\n1.4M\n16.6M\n0.9\n11.4\n126.8\nMLP\n3.2M\n7.7M\n47.8M\n23.9\n67.4\n512\nMLP-PLR\n63K\n1.1M\n8.5M\n0.5\n6.7\n101.7\nFT-Transformer\n21K\n198K\n752K\n0.2\n2\n4.5\nReferences\n[1]\n\"Revisiting Deep Learning Models for Tabular Data\", Gorishniy et al.\n[2]\n\"On Embeddings for Numerical Features in Tabular Deep Learning\", Gorishniy et al."}]}, {"Heading": "Official Review of Submission9502 by Reviewer EnVq", "Subheading": "Official ReviewbyReviewer EnVq16 Oct 2023, 02:24 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe authors meticulously designed a supervised deep learning model for tabular data prediction, which operates in a retrieval-like manner. It outperformed tree-based models on middle-scale datasets, as well as other retrieval-based deep learning tabular learning models. To achieve this, they introduced a k-Nearest-Neighbors-like idea in model design.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nAs emphasized by the authors, their method has managed to outperform tree based models like xgboost on middle-scale datasets.\nOverall, the presentation is clear, and the experiments are comprehensive. The details are clear and the model is highly reproducible.\nThis model is the best-performing retrieval based model.\nWeaknesses:\nThe motivations behind the module designs are not entirely clear. It appears that the authors made meticulous module (equation) optimization based on its performance on some datasets empirically. Then:\n(1) Why does employing the L2 distance, instand of the dot product, lead to improved performance (as shown in Eq. 3)?\n(2) Why is the T function required to use LinearWithoutBias?\n(3) We are uncertain about the robustness of the designed modules. If the dataset characteristics are changed, is it likely that the performance rankings will change significantly? The performances only on middle-sized datasets cannot show the robustness.\n...\nI suggest that providing a theoretical analysis or intuitive motivation would enhance the reader's understanding of those details.\nSome sota DL approaches are not compared, such as T2G-Former (an improved version of FTT)[1], TabPFN [2], and TANGOS [3]. Especially, TabFPN is relatively similar to TabR. These papers are current SOTA, and may outperforms tree based models.\n[1] T2G-Former: Organizing tabular features into relation graphs promotes heterogeneous feature interaction\n[2] TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second\n[3] TANGOS: Regularizing Tabular Neural Networks through Gradient Orthogonalization and Specialization\nThe major comparison lies among middle-scale datasets, accompanied with some results on few other datasets shown in Table 3. In scenarios involving sparse, medium, and dense data-distributed datasets (which typically occur in small, medium-sized, and large-sized datasets, respectively), I suppose that there exists a variance in the nearest neighbor retrieval pattern. Hence, conducting tests solely on medium-sized datasets may not suffice. Furthermore, the issue of inefficiency when dealing with large-scaled datasets appears to have hindered the authors from proving the method's effectiveness in large-scaled datasets.\nThe method proposed by the authors appears to have achieved slight performance advantages on certain datasets (although some SOTA are not compared). However, due to the lacks of explanation for the model details that are designed empirically, it seems unnecessary and risky to apply this method in real-world scenarios (for example, it's unclear whether L2 distance may fail when uninformative features are present; or, for instance, when a table has a feature with values [f_1, f_2, f_3, ..., f_n], and we take the logarithm of these values [log f_1, log f_2, log f_3, ..., log f_n] or their reciprocals, the method may perform poorly in such cases).\nQuestions:\nIn Section 3.1, you mentioned \"continuous (i.e., continuous) features.\" Could this be a typographical error?\nI am curious if the L2 design is sensitive to uninformative features? You can offer some analysis or conduct experiments by adding some gaussian noise columns (uninformative features are commonly seen in tabular datasets) and observe the change of performances. Some transformation like logarithm may impact the results.\nSome questions in weakness.\nFlag For Ethics Review:\nNo ethics review needed.\nDetails Of Ethics Concerns:\nN/A\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Rebuttal (part 1/N)", "Subheading": "Official CommentbyAuthors18 Nov 2023, 11:50Everyone", "Content": "Comment:\nWe thank the reviewer for the detailed review. Below, we address all the raised concerns and questions.\nSome sota DL approaches are not compared ... T2G-Former ... TabPFN ... TANGOS ... These papers are current SOTA, and may outperforms tree based models.\nOur reply to this comment consists of three paragraphs.\nFirst\n, using the benchmark from\n[1]\n(43 classification and regression tasks; used in Figure 1 and Table 5 in our submission) we show that:\nOur model TabR significantly outperforms all the suggested baselines\n.\nThe baselines used in our paper are representative\n.\nThe results:\nModel\n[citation]\nAverage rank (+- std)\nWins/Ties/Losses vs TabR\nNot applicable\nUsed in our paper:\nTabR\n[ours]\n1.36 (\u00b1 0.65)\n0/43/0\n0\nXGBoost\n[11]\n1.93 (\u00b1 0.94)\n7/13/23\n0\nMLP-PLR\n[2]\n1.95 (\u00b1 0.92)\n5/19/19\n0\nFT-Transformer\n[4]\n2.45 (\u00b1 1.20)\n2/14/27\n0\nMLP\n3.07 (\u00b1 1.55)\n2/12/29\n0\nAdditional:\nT2G-Former\n[10]\n2.76 (\u00b1 1.56)\n3/14/23\n3\nMLP + TANGOS\n[9]\n3.29 (\u00b1 1.69)\n2/13/28\n0\nTabPFN\n[8]\n4.38 (\u00b1 1.98)\n0/4/5\n34\nTechnical details:\n\"Not applicable\" (the righmost column) means that the method is fundamentally not applicable to a given task or it does not fit into A100-80GB or (the case of T2G-Former) the hyperparameter tuning did not finish on time.\nIf a method is not applicable to a task, it is assigned the largest rank.\nSecond\n, a closer look at the suggested baselines reveals things that are not consistent with being SOTA\nin the scope and niche of our paper\n:\nTANGOS\nDifferent niche:\nTANGOS is not a model, it is a regularization method, which is completely orthogonal to our niche (model).\nDifferent scope\n: the original work mostly focuses on small data with only two datasets having more than 1000 objects.\nSignificant limitations:\nthe method seems to make training order(s) of magnitude slower (40x times slower on average on our datasets).\nTabPFN\nDifferent scope:\nthe work fully focuses on small (<1000 objects) classification tasks.\nSignificant limitations:\nnot applicable to regression tasks, not applicable to more than 10 classes and more than 100 features, high memory consumption.\nT2G-Former\nAlready tested\nin its paper on many of the datasets used in our paper and in\n[2]\n(where embeddings for continuous features and, in particular, MLP-PLR were introduced). The results from\n[10]\nand\n[2]\nindicate that models from\n[2]\nperform at least on par (and often better) while being more efficient. And, the above table confirms that MLP-PLR is indeed a good baseline, while T2G-Former did not generalize well to the new benchmark.\nThird\n, in the new PDF, in Section 2 Paragraph \"Tabular deep learning\", we improved the communication of the niche of our work and, in particular, cited TANGOS and TabPFN."}, {"Heading": "Rebuttal (part 2/N)", "Subheading": "Official CommentbyAuthors18 Nov 2023, 11:50Everyone", "Content": "Comment:\n[REGARDING THE BENCHMARK AND DATASET SIZES]\nIf the dataset characteristics are changed, is it likely that the performance rankings will change significantly?\n...\nThe performances only on middle-sized datasets cannot show the robustness\n...\nconducting tests solely on medium-sized datasets may not suffice\n...\nFEW\nother datasets\nOur understanding is that the above quoted comments express uncertainty about our benchmark and whether it is comprehensive enough to support our story.\nWe did our best to build a\ncomprehensive, challenging and representative benchmark\nto support our claims. Our reply consists of several points.\nFirst\n, let us show that\nour experimental setup is competitive\nby comparing it to other works on tabular DL:\nModel\n[citation]\nThe number of datasets used in the paper\nMax. train set size\nTabR\n[ours]\n~50 datasets (roughly,\n[1]\n+ some datasets from\n[2]\nand\n[3]\n)\n2.9M\n(+ 10M in the rebuttal)\nNODE\n[7]\n6 datasets\n10M\nTabNet\n[5]\n<10 datasets\n10M\nFT-Transformer\n[4]\n11 datasets\n723K\nSAINT\n[6]\n30 datasets\n321K\n<survey>\n[12]\n5 datasets\n10M\nMLP-PLR\n[2]\n11 datasets\n723K\nTabPFN\n[8]\n67 small datasets (different scope)\n1K\n<benchmark>\n[1]\n<50 *unique* datasets (>50 with multiple versions of the same datasets)\n50K\nTANGOS\n[9]\n20 datasets\n<100K\nT2G-FORMER\n[10]\n12 datasets\n320K\nSecond,\nto illustrate that\nour study is based on many diverse datasets\n, we provide the structure of our benchmark:\nThe \"default\" benchmark based on prior work (mainly\n[2]\n):\n11 datasets (binary classification, multiclass classification and regression)\nfrom 6.4K to 723K training objects\nfrom 8 to 136 features\nThe widely known and cited benchmark from\n[1]\n:\n43 datasets (binary classification and regression)\nfrom 1.7K to 50K training objects\nfrom 3 to 613 features\nOne weather prediction regression task from\n[3]\n: 2.9M training objects and 119 features.\nThird, crucially\n, to make the benchmark challenging for TabR, we ensured that\nthere are many challenging tasks that favour GBDT over prior neural networks.\nIn particular:\nOn the benchmark from\n[1]\n, GBDT was shown to be superior to neural networks.\n[2]\nalso contains multiple GBDT-friendly tasks.\nThe big weather prediction dataset\n[3]\nfavors GBDT over simple neural networks.\nTo sum up\n, we truly hope that the above points demonstrate why we consider our benchmark to be of reasonable size, diversity and complexity to support the claims that we make in the abstract and in the list of contribution in Section 1."}, {"Heading": "Rebuttal (part 3/N)", "Subheading": "Official CommentbyAuthors18 Nov 2023, 11:52Everyone", "Content": "Comment:\nFurthermore, the issue of inefficiency when dealing with large-scaled datasets appears to have hindered the authors from proving the method's effectiveness in large-scaled datasets.\nHowever,\nin the paper, we used datasets with up to three million training objects.\nTo further illustrate the applicability of TabR to large datasets, below, we report the performance of TabR on the Higgs dataset with\nten million training objects:\nModel\nHyperparameter tuning time\nAccuracy\nXGBoost\n1.5 days\n0.783\nTabR\n0\n0.797\nTechnical details:\nWe used TabR with the default hyperparameters with the simplest\nLinear-ReLU\nembeddings for continuous features from\n[2]\nwithout any hyperparameter tuning (\nd_embedding = 32\n).\nWe applied the \"context freeze\" technique described in Section 5.1 of the paper for faster training.\nThe motivations behind the module designs are not entirely clear ... I suggest that providing a theoretical analysis or intuitive motivation would enhance the reader's understanding of those details.\nProviding clear motivation is of high importance to us. Our reply consists of two parts.\nFirst\n,\nbased on the content from the submission\n, let us show how\nthe design steps behind TabR (Section 3.2) are motivated formally, informally and by prior work:\nStep-0 (baseline) is fully motivated by related work.\n(Section 3.2, Paragraph \"Step-0\")\n\"the self-attention operation was often used in prior work ... Then, instantiating retrieval module R as the vanilla self-attention ... is a reasonable baseline\"\nStep-1 (using labels) is highly intuitive:\nlabels is an extremely valuable signal, so we try using them to improve the model.\n(Section 3.2, Paragraph \"Step-1\")\n\"A natural attempt to improve the Step-0 configuration is to utilize labels of the context objects\"\nStep-2 (modifying the similarity module $\\mathcal{S})$ has a whole range of motivations\nprovided in the main text (\"Step-1\" and \"Step-2\" paragraphs of Section 3.2) and appendix (Section A.1 referenced from Section 5.3, Section A.3 referenced from Section 3.2):\nIntuitively,\nit is motivated by the outcome of Step-1:  (Section 3.2, Paragraph \"Step-1\")\n\"Perhaps, the similarity module S taken from the vanilla attention does not allow benefiting from such a valuable signal as labels.\"\nSection A.1.1 provides\nformal motivation\nfor removing the query representations.\nSection A.1.1 provides\ninformal motivation\nfor using L2 instead of the dot product.\nEmpirically\n, it is motivated by the exhaustive (literally) ablation study in Section A.3, which is references from the main text.\nStep-3 (modifying the value module $\\mathcal{V})$\nFormal motivation:\nusing $\\tilde{x}$ in the value module makes it strictly more expressive: (Section 3.2 Paragraph \"Step-3\"):\n\"we make the value module V more expressive by taking the target object\u2019s representation $\\tilde{x}$ into account\"\nRelated work:\n(Section 3.2 Paragraph \"Step-3\")\n\"we take inspiration from DNNR -- the recently proposed generalization of the kNN algorithm\"\nIntuitive interpretation:\n(Section 3.2 Paragraph \"Step-3\"):\n\"<the decomposition into the \"raw\" and \"correction\" terms>\"\nQuantifying the intuitive interpretation:\nSection A.2 referenced from Section 5.3\nStep-4\nis the only purely technical step. However, it is not uncommon for a new architecture to require different technical defaults.\nSecond,\nwe summarize how we further improve the communication and make the available content more visible in the new PDF:\nIn Section 3.2 Paragraph \"Step-2\", we added a reference to Section A.1.\nIn Section 3.2 Paragraph \"Step-3\", we added a reference to Section A.2.\nIn Section A.2.1, we added extended comments on Section 3.2 Paragraph \"Step-3\".\nIn Section A.8, we added a minor technical note on Section 3.2 Paragraph \"Step-3\"."}, {"Heading": "Rebuttal (part 4/N)", "Subheading": "Official CommentbyAuthors18 Nov 2023, 11:52Everyone", "Content": "Comment:\nIt appears that the authors made meticulous module (equation) optimization based on its performance on some datasets empirically.\nWe consider the design-step-based storytelling to be a purely stylistic choice resembling an ablation study, and this is not uncommon in literature, for example, see Section 2 in (ConvNeXt) \"A ConvNet for the 2020s\", CVPR 2022, Liu et al. In our specific case, we find it to be a productive way to highlight the important insights behind TabR.\nWhy does employing the L2 distance, instand of the dot product, lead to improved performance (as shown in Eq. 3)?\nThe changes introduced in Equation 3 are discussed in the paper\nin Section 3.2 Paragraph \"Step-2\" and Section A.1 referenced from Section 5.3 (and, in the new PDF, from Section 3.2):\n(Section 3.2 Paragraph \"Step-2\") Using L2 alone is not enough:\n\"removing any of the three ingredients (context labels, key-only representation, L2 distance) results in a performance drop back to the level of MLP\"\n.\n(Section A.1.1) Because of specific details of TabR's encoder and the nature of tabular data, L2 is a more reasonable default choice for TabR.\n(Section A.1.2) Using L2 leads to more diverse neighbor patterns, which, without any additional assumptions, may be a better default behavior for retrieval-based models.\n(Section A.1.3) For several datasets, we empirically observed that TabR with L2 was capable of uncovering much better neighbors than with the dot product.\nAlso, in Section 3.2 Paragraph \"Step-2\", we add that L2 is just a reasonable default choice:\n\"While the L2 distance is unlikely to be the universally best choice for problems (even within the tabular domain), it seems to be a reasonable default choice for tabular data problems.\"\nWhy is the T function required to use LinearWithoutBias?\nIn the new PDF, we clarified this in Section A.8: using Linear instead of LinearWithoutBias would be redundant because of the term $W_Y$ that already implicitly contains the bias:\n$\\mathcal{V}(\\tilde{x}, \\tilde{x}_i, y_i) = W_Y(y_i) + T(W_K(\\tilde{x}) - W_K(\\tilde{x}_i))$\nAnd, just in case, we avoid this redundancy."}, {"Heading": "Rebuttal (part 5/5, N=5)", "Subheading": "Official CommentbyAuthors18 Nov 2023, 11:53Everyone", "Content": "Comment:\n[REGARDING ROBUSTNESS OF L2 TO UNINFORMATIVE AND IRREGULARLY DISTRIBUTED FEATURES]\nit's unclear whether L2 distance may fail when uninformative features are present ... You can offer some analysis or conduct experiments by adding some gaussian noise columns\n...\nor, for instance, when a table has a feature ... and we take the logarithm of these values ...\nSome transformation like logarithm may impact the results\nBelow, we provide results indicating that\nthe presense of L2 does not make TabR any worse than alternatives\n, (dot-product-based TabR or simple MLP) in the presence of uninformative features or after the logarithmic transformation.\nMoreover, the alternatives continue lagging significantly behind TabR.\nTechnical setup:\nDatasets (see Table 1 in the paper): CA, HO, AD, DI, HI.\nWe use TabR with the default hyperparameters.\nThe uniformative (noisy) features are added as new features.\nFor the logarithmic transformation, original features are replaced with the transformed ones.\nThe table below reports the average (over datasets) difference in\n%\ncompared to the metric of the corresponding model trained on the original unmodified datasets.\nRatio of new uninformative features:\n0.1\n0.25\n0.5\nTabR\n-0.6%\n-1.9%\n-2.87%\nTabR (dot product)\n-1.27%\n-2.26%\n-3.77%\nMLP\n-1.32%\n-2.63%\n-4.37%\nRatio of log-transformed features\n0.1\n0.25\n0.5\nTabR\n-0.15%\n-0.77%\n-1.07%\nTabR (dot product)\n-0.23%\n-0.18%\n-0.39%\nMLP\n-0.05%\n-0.55%\n-0.45%\nImportantly\n, in both of the above regimes, TabR continues to\nsignificantly\noutperform alternatives.\nFor example, this is how the second of the above tables would look like if we reported the differences relative to the\nsame\nbaseline, namely, to TabR trained on unmodified original data:\nRatio of log-transformed features\n0.1\n0.25\n0.5\nTabR\n-0.15%\n-0.77%\n-1.07%\nTabR (dot product)\n-6.95%\n-6.95%\n-7.17%\nMLP\n-6.65%\n-7.23%\n-7.15%\nNote that, currently, TabR uses a simple MLP-like encoder, and naturally inherits all properties related to robustness to various data challenges  such as uniformative features, irregularly distributed features, etc.\n[1]\n,\n[13]\n,\n[14]\n. In particular:\nThese properties explain why, in the above table, there is a performance loss for all DL models regardless of the usage of L2.\nIt is also known how to avoid/alleviate these problems: by using transformer-like encoders\n[1]\n,\n[13]\nor by using embeddings for continuous features\n[2]\n.\nReferences\n[1]\n\"Why do tree-based models still outperform deep learning on tabular data?\", NeurIPS 2022 Datasets and Benchmarks, Grinsztajn et al.\n[2]\n\"On Embeddings for Numerical Features in Tabular Deep Learning\", NeurIPS 2022, Gorishniy et al.\n[3]\n\"Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks\", NeurIPS 2021 Datasets and Benchmarks , Malinin et al.\n[4]\n\"Revisiting Deep Learning Models for Tabular Data\", NeurIPS 2021, Gorishniy et al.\n[5]\n\"TabNet: Attentive Interpretable Tabular Learning\", AAAI 2021, Arik et al.\n[6]\n\"SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training\", ICLR 2022 submission, Somepalli et al.\n[7]\n\"Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data\", ICLR 2020, Popov et al.\n[8]\n\"TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second\", ICLR 2023, Hollmann et al.\n[9]\n\"TANGOS: Regularizing Tabular Neural Networks through Gradient Orthogonalization and Specialization\", ICLR 2023, Jeffares et al.\n[10]\n\"T2G-FORMER: Organizing Tabular Features into Relation Graphs Promotes Heterogeneous Feature Interaction\", AAAI 2023, Yan et al.\n[11]\n\"XGBoost: A Scalable Tree Boosting System\", KDD 2016, Chen et al.\n[12]\n\"Deep Neural Networks and Tabular Data: A Survey\"\n[13]\n\"A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning\", NeurIPS 2023 Datasets and Benchmarks, Cherepanova et al.\n[14]\n\"When Do Neural Nets Outperform Boosted Trees on Tabular Data?\", NeurIPS 2023 Datasets and Benchmarks, McElfresh et al."}, {"Heading": "Official Comment by Reviewer EnVq", "Subheading": "Official CommentbyReviewer EnVq05 Dec 2023, 04:35 (modified: 15 Mar 2024, 00:25)EveryoneRevisions", "Content": "Comment:\nThank you for your responses, which have partially addressed my concerns.\nHowever, I find it puzzling why the hyperparameter tuning for T2G-Former did not complete on time, and other approaches were able to finish. The architecture of T2G-Former is very similar to FTT, and their training schemes are also similar. Nevertheless, this seems to be a minor issue, and I trust the author can provide a satisfactory clarification in their final version. Even in such a situation, deeming a model as the worst in performance solely because hyperparameter tuning was not completed is unfair. Even without completing the scheduled tuning iterations, a model may still achieve good performances, not necessarily the worst. Therefore, comparing with the state-of-the-art in this way is unreasonable and the results are not convincing.\nThis method exhibits inefficiency in training. While the author attempts to justify its use without the need of hyperparameter tuning, comparing the time cost of TabR (without hyperparameter tuning) to other works undergoing hyperparameter tuning seems unfair. There are several competitive deep learning models that do not require hyperparameter tuning and perform well (the comparison with sota has not been executed successfully). Moreover, CatBoost often achieves good performance with fewer iterations compared to XGBoost. I still feel hesitant about using this approach in practice.\nI would like to keep my scores."}]}]}, "kKRbAY4CXv": {"paper_info": {"Primary Area": "applications to physical sciences (physics, chemistry, biology, etc.)", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Numerical PDE, structure preserving neural network, operator learning, boundary integral", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "Numerical solution of partial differential equations (PDEs) plays a vital role in various fields of science and engineering. In recent years, deep neural networks (DNNs) have emerged as a powerful tool for solving PDEs. DNN-based methods exploit the approximation capabilities of neural networks to obtain solutions to PDEs in general domains or high-dimensional spaces. However, many of these methods lack the use of mathematical prior knowledge, and DNN-based methods usually require a large number of sample points and parameters, making them computationally expensive and challenging to train. This paper aims to introduce a novel method named the Neural Evolutionary Kernel Method (NEKM) for solving a class of evolutionary PDEs through DNNs based kernels. By using operator splitting and boundary integral techniques, we propose particular neural network architectures which approximate evolutionary kernels of solutions and preserve structures of time-dependent PDEs. Mathematical prior knowledge are naturally built into these DNNs based kernels through convolutional representation with pre-trained Green functions, leading to serious reduction in the number of parameters in the NEKM and very efficient training processes. Experimental results demonstrate the efficiency and accuracy of the NEKM in solving heat equations and Allen-Cahn equations in complex domains and on manifolds, showcasing its promising potential for applications in data driven scientific computing.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9498", "PDF Url": "https://openreview.net/pdf?id=kKRbAY4CXv"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9498 by Area Chair QEoz", "Subheading": "Meta ReviewbyArea Chair QEoz10 Dec 2023, 17:32 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThe paper proposes a method for solving time-dependent PDEs by utilizing the knowledge of Green functions.\nThe approach potentially is applicable to interesting PDEs.\nWeaknesses:\nThe paper is not well-written. It is very difficult to decipher the actual algorithm from the paper. What is trained? On what data? With what loss function? What is the complexity? How many parameters are trained? This is completely unreproducible, unclear and is just not ready for publication in any form.\nOnce the details are recollected from different parts of the paper (namely, the BiNet approach that learns the density) it becomes clear that the paper uses the operator splitting plus BiNet to learn the neural network approximation to boundary potentials. In this case, the number of baselines for the comparison should be much larger.\nJustification For Why Not Higher Score:\nThe paper is not ready, it is not clearly written and does not contain minimal number of baselines.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "Official Review of Submission9498 by Reviewer 71KZ", "Subheading": "Official ReviewbyReviewer 71KZ04 Nov 2023, 16:17 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper introduces a novel approach called Neural Evolutionary Kernel Method (NEKM) for solving time-dependent semi-linear Partial Differential Equations (PDEs). The authors leverage a combination of operator splitting, boundary integral techniques, and Deep Neural Networks (DNNs) to construct evolutionary blocks that approximate solution operators. NEKM incorporates mathematical prior knowledge into each block, utilizing convolution operations and nonlinear activations tailored to the specific PDEs under consideration. This approach offers several noteworthy contributions:\nEfficiency and Generalizability\n: The use of boundary integral techniques is a standout feature of NEKM, allowing for a reduced requirement of network parameters and sampling points. This not only improves training efficiency but also relaxes the regularity assumptions on solutions. The capacity to apply NEKM to problems in complex domains and on manifolds showcases its versatility and potential real-world applicability.\nCompatibility with Time Discretization Schemes\n: NEKM can be effectively combined with time discretization schemes that possess structure-preserving properties, such as energy stability. This demonstrates the adaptability of the method to diverse mathematical contexts.\nTreatment of Singular Boundary Integrals\n: The paper introduces a method for computing singular boundary integrals that arise from fundamental solutions. This addition contributes to the overall training efficiency and robustness of NEKM.\nThe empirical validation of NEKM is conducted through testing on heat equations and Allen-Cahn equations in complex domains and on manifolds. The results demonstrate the method's high accuracy and its capacity to generalize across various domains.\nIn summary, the paper presents an innovative and promising approach, NEKM, which addresses the solution of time-dependent semi-linear PDEs. The combination of mathematical prior knowledge, boundary integral techniques, and DNNs provides a compelling method that improves training efficiency, generalizability, and adaptability to different mathematical scenarios. The successful testing on various equations and domains underscores the method's potential significance in the field of mathematical modeling and scientific computing.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nThe strengths of the paper \"Neural Evolutionary Kernel Method (NEKM) for Solving Time-Dependent Semi-Linear PDEs\" include:\nInnovative Approach\n: The paper introduces a novel approach, NEKM, which combines operator splitting, boundary integral techniques, and Deep Neural Networks (DNNs) to address the solution of time-dependent semi-linear Partial Differential Equations (PDEs). This innovation offers a fresh perspective on tackling complex mathematical problems.\nEfficiency Improvement\n: NEKM leverages boundary integral techniques to reduce the need for extensive network parameters and sampling points. This not only enhances the efficiency of training but also relaxes regularity assumptions on solutions. This efficiency improvement is a significant advantage in solving real-world problems.\nGeneralizability\n: The paper demonstrates that NEKM can be applied to problems in complex domains and on manifolds, showcasing its generalizability across different mathematical contexts. This broad applicability enhances its potential usefulness in a wide range of scientific and engineering applications.\nCompatibility with Time Discretization Schemes\n: NEKM's compatibility with time discretization schemes that possess structure-preserving properties, such as energy stability, is a valuable feature. This adaptability makes it easier to integrate NEKM into existing mathematical frameworks.\nTreatment of Singular Boundary Integrals\n: The paper provides a method for computing singular boundary integrals that arise from fundamental solutions. This contribution adds to the method's efficiency and robustness, making it more practical for real-world applications.\nEmpirical Validation\n: The authors validate the NEKM approach through rigorous testing on heat equations and Allen-Cahn equations in complex domains and on manifolds. The high accuracy demonstrated in these tests underscores the practical utility of NEKM.\nMathematical Rigor\n: NEKM incorporates mathematical prior knowledge into its framework through convolution operations and nonlinear activations. This mathematical rigor ensures that the method is well-founded and theoretically sound.\nInterdisciplinary Relevance\n: The paper's focus on solving complex mathematical problems with machine learning techniques has broad interdisciplinary relevance, as it can find applications in various fields, including physics, engineering, and computational science.\nOverall, the strengths of the paper lie in its innovative approach, efficiency improvements, generalizability, compatibility with existing mathematical schemes, and the rigorous empirical validation of the proposed method. These qualities make NEKM a promising addition to the field of mathematical modeling and scientific computing.\nWeaknesses:\nWhile the paper on \"Neural Evolutionary Kernel Method (NEKM) for Solving Time-Dependent Semi-Linear PDEs\" offers several strengths, there are also some potential weaknesses to consider:\nComplexity\n: The proposed NEKM method, while innovative, is complex in its approach, involving the integration of operator splitting, boundary integral techniques, and Deep Neural Networks. This complexity might make it challenging for practitioners who are not well-versed in all of these areas to implement and understand.\nComputational Resources\n: The paper does not extensively discuss the computational resources required for training and applying the NEKM method. Deep learning methods often demand significant computational power, which could be a limitation for some users, particularly those without access to high-performance computing resources.\nLimited Real-World Use Cases\n: While the paper demonstrates NEKM's effectiveness in solving specific mathematical problems, it remains largely theoretical. More real-world use cases and practical applications in various domains would strengthen the paper's relevance and utility.\nInterpretability\n: The paper discusses the use of neural networks, which are often seen as \"black-box\" models. While the paper addresses some interpretability challenges, it might not provide a complete solution to the interpretability issues associated with deep learning approaches.\nAlgorithm Complexity\n: The proposed method involves a combination of different techniques, such as boundary integral representation and neural networks. This may make the implementation and understanding of NEKM challenging for some users, potentially limiting its widespread adoption.\nEmpirical Validation Scope\n: While the paper includes empirical validation on heat and Allen-Cahn equations, the scope of the empirical validation might be limited. A more extensive range of test cases across different scientific and engineering domains would strengthen the method's generalizability.\nScalability\n: The paper does not explicitly address the scalability of the NEKM method. As the complexity of problems increases, it remains to be seen whether NEKM can efficiently scale to handle more complex and larger-scale scenarios.\nComparison to Existing Methods\n: The paper lacks a comprehensive comparison of the NEKM method with existing approaches for solving similar problems. Such comparisons would help to better assess the relative strengths and weaknesses of NEKM.\nIn conclusion, while the NEKM method offers several promising advantages, such as efficiency improvements and generalizability, it also has some potential limitations, including complexity, computational resource requirements, and the need for more extensive real-world applications and validation. These weaknesses should be considered when evaluating the method's suitability for specific applications.\nQuestions:\nCan you provide more insight into the computational resources required for training and applying the NEKM method? What kind of hardware and software infrastructure is necessary for its practical implementation?\nThe NEKM method is quite complex, involving a combination of operator splitting, boundary integral techniques, and neural networks. How user-friendly and accessible is the implementation for researchers and practitioners who may not be experts in all these areas?\nThe paper mentions empirical validation on heat and Allen-Cahn equations. Are there plans to expand the empirical validation to a broader range of mathematical problems or real-world applications to further assess the generalizability of NEKM?\nHow does NEKM address the interpretability challenge often associated with deep learning methods? Can you provide more details on how NEKM helps users understand and trust its results, especially in cases where interpretability is critical?\nThe paper mentions combining NEKM with time discretization schemes that possess structure-preserving properties. Could you elaborate on specific scenarios or use cases where this combination has proven to be advantageous?\nNEKM proposes the treatment of singular boundary integrals arising from fundamental solutions. Can you discuss the impact of this addition on the overall efficiency and robustness of the method in practical applications?\nIn the real world, problems often scale in complexity. How does NEKM address the scalability challenge, especially when dealing with larger and more complex scenarios beyond the examples provided in the paper?\nThe paper does not include a comprehensive comparison of NEKM with existing methods for solving similar problems. Could you share insights into how NEKM performs in comparison to other approaches, and in what scenarios it may have a comparative advantage?\nAre there any specific plans or ongoing research aimed at addressing some of the potential weaknesses or limitations identified in the paper, such as making the method more accessible or broadening the scope of empirical validation?\nHow do you envision the practical adoption of NEKM in various scientific and engineering domains? Are there specific industries or areas where NEKM is expected to have a significant impact, and if so, what are the next steps for its real-world application?\nThese questions aim to seek further clarification and insights from the authors regarding the NEKM method and its potential applications and improvements.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:34Everyone", "Content": "Comment:\nWe thank the reviewer very much for his/her careful reading and helpful feedback; it will be instrumental in our improvement! We also appreciate his/her positive acknowledgment of our work.\nResponses to Weaknesses:\nThe technique we employed in NEKM is quite standard in the community of mathematics. Though challenging for practitioners, the process is formally interpreted using machine learning terms. The solution process for semi-linear PDEs involves two stages: 1) training\u2014pretraining Green functions and neural network representations of evolutionary kernels; 2) inference\u2014evolving PDE dynamics using the trained kernel. This decomposition results from observing the PDEs' mathematical structure. Linear parts have a convolutional representation through BINet, allowing time-independent training in advance. Nonlinear parts resemble CNN activation processes. A well-designed time discretization integrates structures into a NeuralNet architecture, resembling a CNN mechanism. Green's function training is a one-time effort for inference, evolution, and solving similar equations. Energy-stable property and a designed time discretization maintain energy, ensuring physical reliability in the proposed NeuralNet framework.\nSpecifically, in the first step, we train the kernel function and the Green's function (if necessary), and in subsequent steps, all that is required is network inference and numerical integration calculations. The training of our Green's function is a one-time effort and can be used for the inference, evolution, and even the solution of other similar equations after the fact.\nApplying our method to more complex domains is theoretically feasible, but in practice, it may pose many challenges. This would be a long-term aspect of our work that needs further exploration.\nRegarding the concern about interpretability, we integrated mathematical knowledge into our method precisely because of the lack of interpretability in some neural network methods, aiming to enhance the interpretability of the equation solutions. Nevertheless, achieving complete interpretability in deep learning is a challenging area for us, as well.\nYes, our method is more difficult than some other methods such as PINNs in coding, but our advantages has shown in 1.\nPlease see 3.\nThis may require extensive study including both sufficient computational experiments and deep theoretical thinking; this is indeed one of the long-term tasks in our future planning.\nIn Appendix D.1, we compared the accuracy of the NEKM method with the PINN method; in Appendix D.2, we explained situations where the PINN method is challenging to train, while our method remains effective.\nResponses to Questions:\nTraining the neural network in Equation 4 was very fast, sometimes taking less than a minute. Training the Green's function could take longer. This can be further improved if more sophisticated machine could be used (our computation is based on TITAN XP GPU). After training, calculations can be performed on a CPU, which usually does not take much time.\nThe proposed method mainly rely on numerical integration, which we believe is not quite sophisticated for practitioners. In fact, given the advantages we elaborate in our response to Weakness 1, the CNN-like structure of our method makes it easily generalized.\nPlease see 3 in \u2018Weakness\u2019 part.\nPlease see 4 in \u2018Weakness\u2019 part.\nEnergy-stable properties are derived from the physical features or inferred from the equation itself. Taking the Allen-Cahn equation as an example, it describes phase transitions in materials. The solution u represents the material's phase field, and the equation's evolution reflects changes in the phase field. In NEKM, the network not only outputs the solution's evolution but also ensures a certain energy decay.\nWhen handling singular boundary integrals, it is true that our method introduces some complexity at the code level. But it contributes to improved accuracy.\nYes, it is a long-term challenge, and we need to invest more effort to address it.\nPlease see 8 in \u2018Weakness\u2019 part.\nWe believe that the tasks we can prioritize mainly involve solving problems in more complex domains, including how to improve the accuracy of the method and reduce computation time. In fact, we already have collaborators who have proposed using neural networks to approximate G_0, which will reduce the time required for directly computing G_0 using explicit expressions.\nIt's evident that the reviewer values the practical utility of a method and its potential integration with engineering problems. This is an aspect we will pay attention to, and some of our collaborators are from commercial companies, so we will pay attention to their needs and continuously modify and improve our method.\nOnce again, we thank the reviewer for his/her helpful suggestions and questions; his/her positive recognition will be crucial to our continued advancement!"}]}, {"Heading": "Official Review of Submission9498 by Reviewer sX1E", "Subheading": "Official ReviewbyReviewer sX1E29 Oct 2023, 15:41 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper aims to tackle solving partial differential equations (PDEs) traditionally solved by numerical methods with deep neural networks (DNNs). The authors address the challenges of solving PDEs with DNNs that a majority of these methods do not use any mathematical or physical parameters and require a large amount of parameters to tune. The authors propose the Neural Evolutionary Kernel Method (NEKM) to solve a type of evolutionary PDEs with DNN based kernels. The core idea is to incorporate pre-trained Green's functions. NEKM is an alternating two-step procedure that first analytically or numerically solves a nonlinear ODE to obtain a flow map and then numerically integrate the related linear PDE with a convolutional kernel.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nNice abstract that motivates the need for PDEs in science and engineering problems and use of numerical methods to solve them.\nThe paper and abstract are well-written.\nIncorporating ideas from numerical methods, e.g., Green's function, boundary conditions and energy stability is very nice. In particular, I like to the discussion in subsection 2.2 on energy conservation and would like more details in the Appendix.\nThe generalization and use of the pre-trained Green's function is nice.\nThe computational savings of defining the Green's function on the boundary rather than the interior domain is nice. For other boundary integral representations for conservation laws, see Hansen, et. al, \"Learning physical models that can respect conservation laws\", ICML 2023 (\nhttps://arxiv.org/abs/2302.11002\n).\nNice high dimensional simulations in Figures 6-7.\nGeneralizability to different manifolds and boundary conditions.\nWeaknesses:\nThe authors should define earlier what they mean by evolutionary PDEs.\nConnection to other kernel operator methods such as the Fourier Neural Operator (FNO) should be considered. It is only briefly discussed in one sentence of related work with a majority on the PINNs literature. In particular, in the related works, the authors discuss in detail how boundary conditions are incorporated into Physics-Informed Neural Networks (PINNs). The related in Neural Operator community should be discussed, such as how to incorporated boundary conditions into Neural Operators in Saad et. al, \"Guiding continuous operator learning through Physics-based boundary constraints\", ICLR 2023.\nThe method only works on semi-linear PDEs. This is actually a very strong assumption and limitation. The authors should discuss the extension to nonlinear PDEs.\nEvaluation: the method is only tested on the simple linear heat/diffusion equation and Allen-Cahn equations. The heat equation is smooth and parabolic and very easy for numerical methods to solve. It would be nice to test hyperbolic problems with shocks, e.g., in the GPME benchmarking framework in Hansen, et. al, \"Learning physical models that can respect conservation laws\", ICML 2023 (\nhttps://arxiv.org/abs/2302.11002\n).\nThe method seems to have strong limitations if the first step requires an analytical or numerical solution to the ODE.\nIn particular, the authors should clarify this in the last paragraph of the introduction. I don't understand where the nonlinear ODE is coming from in step 1 and then how there is \"numerically integration\" for the related linear PDE. Typically, in numerical methods a (non)linear PDE is first discretized in space and then the resulting semi-discrete form of the ODE is discretized in time. The authors should clarify what they mean here.\nI think some of the equation details of BINet in the related work should be moved to an appendix or background section.\nCare should be taken with the discretization because this adds a first order error into the scheme. For example, the first equation should not be discretized with the 1st order accurate Forward Euler without even citing the method. This is an explicit method and there are necessary bounds on $\\Delta t$/$\\tau$ to ensure numerical stability.  See Krishnapriyan et. al, \"Learning continuous models for continuous physics\", 2023 (\nhttps://arxiv.org/pdf/2202.08494.pdf\n) on how the time discretization matters in NeuralODE and the 4th order RK4 is advantageous but even that scheme without being careful about the numerics can lead to convergence issues.\nIdeally the method and presentation wouldn't need to be separated into separate cases for linear equations or not.\nIt seems like the method depends too strongly on the BINet method and the authors should better differentiate the novelty between the two.\nThe exposition of the method in Section 2 isn't too clear and some of the details can be moved to an appendix.\nThe unique features of the NEKM subsection seems like it could be incorporated with the contributions subsection in the intro.\nLabel x and y axis in Figure 3.\nAnother major weakness in the evaluation is just comparing to the exact solution and no other baseline methods, especially to related neural operator based methods.\nMinor\nFirst paragraph of related works can be longer and combined with parts of the longer second paragraph.\nheat equation shouldn't be plural in the last bullet point of the contributions.\ncomma after \"In this section\" at the beginning of Section 2 Method\nI would name Section 2 with the specific method name Neural Evolutionary Kernel Method (NEKM) rather than the generic Method.\nCould use standard notation from numerical methods $\\Delta t$ instead $\\tau$\nComma missing after Equation 7.\nLarger title lave on Figure 6.\nQuestions:\nDoes the method only work on semi-linear PDEs? If so, this is a bit limiting and the authors should discuss the extension to nonlinear PDEs.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:23 (modified: 23 Nov 2023, 06:24)EveryoneRevisions", "Content": "Comment:\nWe thank the reviewer very much for his/her reading and evaluation; this will be very helpful for the improvement of our work!\nResponses to Weaknesses:\nThank the reviewer for his/her suggestion, but we proposed it at the beginning of the method section, which should be considered a relatively conventional approach.\nAs suggested, we incorporated content related to FNO in the related work section in the revised paper.\nWe focus on semi-linear PDEs because they allow us to derive the evolutionary pattern of the desired ideal solution, something that is not achievable with nonlinear PDEs (which may not even have a corresponding fundamental solution for the operator). In fact, we claim to be able to solve semi-linear PDEs, covering a broad range of time-evolving PDEs. We only require the spatial part of the differential operator to be linear, meaning terms like (u_x)^2 are excluded. We allow terms such as f(u) to appear, encompassing a significant class of PDE equations.\nYes, the heat equation is a fundamental example, and we have added examples of the Allen-Cahn equation. Traditional numerical methods for solving the Allen-Cahn equation are not straightforward, often requiring considerations of numerical stability and knowledge related to energy stability. Additionally, our goal is to solve the equation in a general domain, posing new challenges for traditional methods. Examples of other equations will be explored in future numerical experiments.\nRegarding the concerns about the difficulty of analytically solving ODEs, there is no need to worry. The ODEs we solve have the form u_t = f(u), excluding spatially related differential operators, making them amenable to analytical solutions through integration. In Appendix D.1, we provide a specific example of implementation.\nNonlinear ODEs are given by operator splitting, as detailed in case 2 of Section 2.1. In essence, our method alternates between solving the nonlinear ODE part, solving the linear PDE, and solving the ODE\u2014a standard Strang splitting process. For a better understanding, refer to the detailed examples in case 2 of Section 2.1 and Appendix D.1. Due to space limitations, we were unable to showcase the specific implementation of the method in the numerical experiments section but included it in the appendix.\nAs suggested, we have reduced the introduction section's discussion of BINet.\nThe reviewer pointed out the discrete formulation in our article: our time discretization uses a first-order implicit scheme. However, in principle, we can adopt higher-order time discretization schemes without increasing difficulty. Traditional numerical methods need to balance spatial and temporal step sizes to ensure stability, but our method, not relying on spatial discretization, eliminates stability concerns. If one is concerned about energy stability and other properties, our method can still be combined with the corresponding time discretization formats to achieve the desired results.\nOur method indeed relies on BINet, but our primary contribution lies in proposing an evolution law for time-dependent equations using time discretization, operator splitting, and boundary integral equations. It can be combined with the appropriate time discretization format to maintain the desired properties. Additionally, we provided a detailed description of how to handle singularities introduced by the fundamental solution in boundary integrals.\nThis is corrected.\nIn Appendix D.1, we compared the accuracy of the NEKM method with the PINN method; in Appendix D.2, we explained situations where the PINN method is challenging to train, while our method remains effective. We apologize for any confusion caused.\nMinor points: We followed the reviewer\u2019s instructions to revise the paper and correct those items.\nResponses to Question:\nThe technique we employed in NEKM is quite standard in the community of mathematics. Though challenging for practitioners, the process is formally interpreted using machine learning terms. The solution process for semi-linear PDEs involves two stages: 1) training\u2014pretraining Green functions and neural network representations of evolutionary kernels; 2) inference\u2014evolving PDE dynamics using the trained kernel. This decomposition results from observing the PDEs' mathematical structure. Linear parts have a convolutional representation through BINet, allowing time-independent training in advance. Nonlinear parts resemble CNN activation processes. A well-designed time discretization integrates structures into a NeuralNet architecture, resembling a CNN mechanism. Green's function training is a one-time effort for inference, evolution, and solving similar equations. Energy-stable property and a designed time discretization maintain energy, ensuring physical reliability in the proposed NeuralNet framework.\nOnce again, we thank the reviewer for his/her comments and suggestions!"}]}, {"Heading": "Official Review of Submission9498 by Reviewer Ljfw", "Subheading": "Official ReviewbyReviewer Ljfw24 Oct 2023, 01:16 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper presents the Neural Evolutionary Kernel Method (NEKM) for solving semi-linear time-dependent PDEs. NEKM distinguishes itself by utilizing operator splitting and boundary integration, enabling efficient network architectures. The method is demonstrated to be effective and stable in solving classic PDEs, such as the heat equation and the Allen-Cahn equation.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nNEKM can be combined with time discretization schemes that preserve energy stability, which is crucial for modeling physical systems.\nThe method incorporates an evolutionary kernel, which inherently preserves the structure of the problem.\nThe method incorporates an evolutionary kernel, which inherently preserves the structure of the problem.\nWeaknesses:\nWhile NEKM is claimed to work in complex domains, the paper primarily provides examples in small and relatively simple domains. It would be beneficial to demonstrate its performance in more complex and realistic domains, similar to the level in the referenced paper (\nhttps://arxiv.org/pdf/2309.00583\n), including real-world scientific and engineering geometries.\nThe paper lacks references to related work that adopts neural networks only at the spatial level while using time discretizations to evolve spatial fields over time. Including references to papers like \"Evolutional deep neural network (Physical Review E 2021),\" \"Implicit Neural Spatial Representations for Time-dependent PDEs (ICML 2023),\" and \"Neural Galerkin Scheme with Active Learning for High-Dimensional Evolution Equations\" could help provide context and comparisons.\nThe paper does not provide information about the computational cost and scalability of NEKM compared to classical numerical methods, especially for larger 3D problems. It would be valuable to include performance comparisons in terms of computational efficiency.\nQuestions:\nMy biggest confusion and concern is the relationship between this paper (Lin et al., 2023a) as well as (Lin et al., 2023b). Those paper also use a convolution representation of the solutions using Green's functions. What exactly is the author's contribution except working with time-dependent problems?\nThe paper focuses on semi-linear PDEs, but it would be interesting to know if NEKM can be extended to handle nonlinear PDEs. Clarification on the limitations and potential extensions of the method for nonlinear problems would be beneficial.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:18 (modified: 23 Nov 2023, 06:25)EveryoneRevisions", "Content": "Comment:\nWe appreciate the reviewer\u2019s careful reading and serious evaluation! First of all we would like to reiterate our key contribution, and highlights. The technique we employed in NEKM is quite standard in the community of mathematics. While this may be challenging for practitioners to understand, we can interpret the process formally using the standard terminology of machine learning. Basically, we decompose the solution process of semi-linear PDEs into two stages: 1) training stage: this involves the pretraining of Green functions, the training of neural network representation of evolutionary kernels based on boundary integral formulations; 2) inference stage: this involves the time evolution of the dynamics of the PDEs using the well-trained kernel. This decomposition is a result of insightful observation of the mathematical structure of the underlying PDEs. The linear parts of the PDEs admit convolutional representation, which takes into account the geometry information, the linear operator information, and the boundary conditions. BINet is naturally employed to give a NeuralNet approximation of this linear evolutionary operator. Moreover, due to the semi-group property of the underlying PDEs, the training for this convolutional representation is time independent and thus can be done in advance. The nonlinear parts of the PDEs admit nonlinear transformation of the underlying nonlinear flows that can be solved exactly, making them look like the activation process of CNNs. A carefully designed time discretization method integrates these two key structures into a whole NeuralNet architechture, resulting in a CNN-like mechanism. Specifically, in the first step, we train the kernel function (also called density function) and the Green's function (if necessary), and in subsequent steps, all that is required is the network inference and numerical integrations. The training of our Green's function is a one-time effort and can be used for the inference, evolution, and even the solution of other similar equations after the fact. Moreover, due to the energy stable property of the underlying PDEs, a particularly designed time discretization (energy stable scheme) can maintain the energy so that the NeuralNet structure carries the energy stable flows as its natural property. This is consistent with the physical principle, i.e., the 2nd law of thermodynamics, making the proposed NeuralNet framework physically reliable.\nResponses to Weaknesses:\nApplying our method to more complex domains is theoretically feasible, but in practice, it may pose many challenges. For instance, the accuracy of Green's function training may decrease due to the complexity of the domain. This would be a long-term aspect of our work that needs further exploration.\nWe believe that our approach differs from the methods for solving time-dependent PDEs mentioned in the literature, although both involve time discretization. We emphasize an evolutionary pattern that allows us to minimize reliance on neural network training in the steps following the initial stage. The papers referred by the reviewer are all based on the L^2 residual loss minimization, which is essentially different from ours. As explained in the opening paragraph above, we aim to develop an inference-type NeuralNet architecture which preserves the physical principle such as energy stability. This is the novel point of our work.\nDue to its inference nature, the computational efficiency can be good even in comparison with classical numerical methods, as the proposed method avoids the solution of linear solver but instead saves the solution operator via a well-trained NeuralNet. The comparison of computational efficiency with PINNs was also provided in the appendix.\nResponses to Questions:\nOur main contribution is proposing an evolution law for time-dependent equations using a time discretization scheme, operator splitting, and boundary integral equations. In the solving process, we utilized the knowledge from the papers (Lin et al., 2023a and Lin et al., 2023b). As stated in our article, BINet and BI-GreenNet are the two essential tools we require. Additionally, we provided a detailed description of how to handle singularities introduced by the fundamental solution in boundary integrals.\nWe focus on semi-linear PDEs because they allow us to derive the evolutionary pattern of the desired ideal solution, something that is not achievable with nonlinear PDEs (which may not even have a corresponding fundamental solution for the operator). This is discussed in the opening paragraph above. In fact, we claim to be able to solve semi-linear PDEs, which already covers a broad range of time-evolving PDEs. We only require the spatial part of the differential operator to be linear, meaning terms like (u_x)^2 are excluded. We allow terms such as f(u) to appear, encompassing a significant class of PDE equations."}]}, {"Heading": "Official Review of Submission9498 by Reviewer tJtQ", "Subheading": "Official ReviewbyReviewer tJtQ19 Oct 2023, 21:19 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposes a neural network-based algorithm, namely the Neural Evolutionary Kernal Method (NEKM), for solving evolutionary PDEs. The method involves the operator splitting technique and the idea of boundary integral network. Specifically, the method pre-trains a neural network representation of the Green function and then solves the evolutionary PDE by applying the Green function block and kernel function block alternatingly with an ODE solver. Experiments on the heat equation and Allen-Cahn equations are conducted to demonstrate the performance.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThe paper is well-written and easy-to-follow.\nThe proposed method is interesting and mathematically grounded.\nExperimental results seem strong.\nWeaknesses:\nIt seems the method heavily relies on the closed form formula of the fundamental solution $G_0$. The numerical error of the integration involving $G_0$ seems troublesome.\nThe experimental results of Allen-Cahn equation is not compared with the exact one or any other method.\nSome minor issues: Figure 12 is too small.\nQuestions:\nNow that the Green function $G$ is computed by pre-training a neural network, the error of this step may propagate to solving the time evolutionary PDE. Was this problem an issue in the experiments? How accurate should the numerically approximated Green function be so as not to affect the performance?\nAs mentioned in the paper, the possible singularity of $G_0$ may demand special handling. But the form of $G$ is generally unknown. How can the singularity appearing in $G$ be dealt with?\nEnergy stsability is claimed as one of the contributions. Is this only empirically observed or grounded with some particular design?\nFlag For Ethics Review:\nNo ethics review needed.\nDetails Of Ethics Concerns:\nN/A\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:15 (modified: 23 Nov 2023, 06:24)EveryoneRevisions", "Content": "Comment:\nWe appreciate some of the reviewer\u2019s comments while cannot agree with the others. Perhaps the reviewer does not carefully read and correctly understand our paper, his/her comment could be subjective. First of all we would like to reiterate our key contribution, and highlights, although it has already been discussed at the end of Section 2.1 in the main text. The technique we employed in NEKM is quite standard in the community of mathematics. While this may be challenging for practitioners to understand, we can interpret the process formally using the standard terminology of machine learning. Basically, we decompose the solution process of semi-linear PDEs into two stages: 1) training stage: this involves the pretraining of Green functions, the training of neural network representation of evolutionary kernels based on boundary integral formulations; 2) inference stage: this involves the time evolution of the dynamics of the PDEs using the well-trained kernel. This decomposition is a result of insightful observation of the mathematical structure of the underlying PDEs. The linear parts of the PDEs admit convolutional representation, which takes into account the geometry information, the linear operator information, and the boundary conditions. BINet is naturally employed to give a NeuralNet approximation of this linear evolutionary operator. Moreover, due to the semi-group property of the underlying PDEs, the training for this convolutional representation is time independent and thus can be done in advance. The nonlinear parts of the PDEs admit nonlinear transformation of the underlying nonlinear flows that can be solved exactly, making them look like the activation process of CNNs. A carefully designed time discretization method integrates these two key structures into a whole NeuralNet architechture, resulting in a CNN-like mechanism. Specifically, in the first step, we train the kernel function (also called density function) and the Green's function (if necessary), and in subsequent steps, all that is required is the network inference and numerical integrations. The training of our Green's function is a one-time effort and can be used for the inference, evolution, and even the solution of other similar equations after the fact. Moreover, due to the energy stable property of the underlying PDEs, a particularly designed time discretization (energy stable scheme) can maintain the energy so that the NeuralNet structure carries the energy stable flows as its natural property. This is consistent with the physical principle, i.e., the 2nd law of thermodynamics, making the proposed NeuralNet framework physically reliable.\nResponses to Weaknesses:\nFor a linear partial differential operator, its fundamental solution exists, and the expressions for many fundamental solutions are known. Even if the fundamental solution is currently unknown, NeuralNet approximations of the fundamental solutions are still available (through radial basis function representations) and under our investigation. Regarding the accurate numerical integration, we have already employed asymptotic expansions and integration by parts, as mentioned in the text, to handle singular integrals. In mathematical terms, this does not introduce errors, so there is no need to worry about significant errors arising from the integration.\nWe have already compared the examples of the Allen-Cahn equation we worked on in the rectangular region (Section 3.2) and on the hemisphere (Section 3.3) with the solutions of classical numerical methods or the true solutions. In Appendix D.2, we explain the feasibility of the NEKM method compared to PINNs in this case. We invite the reviewer to go through these details.\nWe appreciate the reviewer\u2019s suggestion. The size of Figure 12 has been adjusted.\nResponses to Questions:\nAs introduced in the first paragraph, the training of the Green's function is a one-time effort, allowing us to take sufficient sampling points and computational efforts to approximate it as accurately as possible. This is definitely not a problem in the numerical experiments as can be observed in the presented results. By the numerical stability of the scheme we applied in time discretization, the errors cannot be amplified during the evolution. Nevertheless, one can apply the feedback control by sampling more data from the true trajectory of the PDE to reduce the approximation and enhance training and inference of the solution process in every time marching block.\nIn Appendix A.2, we elaborate on the acquisition of the Green's function G, whose singularity essentially arises from the fundamental solution G_0, and therefore, it can be estimated.\nEnergy stability is achieved through the time discretization of numerical schemes, and this can be rigorously proven mathematically, rather than being a baseless claim or anecdotal evidence. Please refer to Reference given in section 2.2 for more details."}]}]}, "ApjY32f3Xr": {"paper_info": {"Primary Area": "datasets and benchmarks", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "PINN, machine learning, physics-informed machine learning", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "While significant progress has been made on Physics-Informed Neural Networks (PINNs), a comprehensive comparison of these methods across a wide range of Partial Differential Equations (PDEs) is still lacking. This study introduces PINNacle, a benchmarking tool designed to fill this gap. PINNacle provides a diverse dataset, comprising over 20 distinct PDEs from various domains including heat conduction, fluid dynamics, biology, and electromagnetics. These PDEs encapsulate key challenges inherent to real-world problems, such as complex geometry, multi-scale phenomena, nonlinearity, and high dimensionality. PINNacle also offers a user-friendly toolbox, incorporating about 10 state-of-the-art PINN methods for systematic evaluation and comparison. We have conducted extensive experiments with these methods, offering insights into their strengths and weaknesses. In addition to providing a standardized means of assessing performance, PINNacle also offers an in-depth analysis to guide future research, particularly in areas such as domain decomposition methods and loss reweighting for handling multi-scale problems and complex geometry. While PINNacle does not guarantee success in all real-world scenarios, it represents a significant contribution to the field by offering a robust, diverse, and comprehensive benchmark suite that will undoubtedly foster further research and development in PINNs.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "Supplementary Material": "pdf", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9493", "PDF Url": "https://openreview.net/pdf?id=ApjY32f3Xr"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9493 by Area Chair f6oV", "Subheading": "Meta ReviewbyArea Chair f6oV10 Dec 2023, 10:58 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThere was one very critical review, in which it was criticised that there is no clear methodological contribution in this paper. Other reviewers had a slightly more positive impression of this paper, putting more emphasis on the experimental and benchmark character of this work. After the rebuttal and discussion phase, however, I still think that the lacking novelty on the conceptual side is indeed a severe weakness of this paper, which could not be compensated by the experimental studies: In my opinion, the conclusions drawn from the benchmark experiments seem to be somewhat limited regarding truly novel insights into PINNs. Therefore I recommend rejection of this paper.\nJustification For Why Not Higher Score:\nThis is mainly an experimental/benchmarking paper, but the experimental results did not lead to fundamentally new insights.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "Official Review of Submission9493 by Reviewer Ytqc", "Subheading": "Official ReviewbyReviewer Ytqc07 Nov 2023, 00:08 (modified: 04 Dec 2023, 19:30)EveryoneRevisions", "Content": "Summary:\nThis paper provides both a collection of benchmark datasets as well as a standardized suite of PINN-type neural network PDE solution approximators arranged as a python package.\nIt further shows benchmark numbers of the different PINN methods on the benchmark datasets.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nProviding any meaningful benchmark to the community is a valuable service.\nIn addition to creating the benchmark data sets, the authors have made a big effort in collecting and unifying PINN methods into a unified framework.\nThe paper appendix contains detailed specifications about the particular setup for the data benchmark.\nWeaknesses:\nWhile providing a benchmark data set to the community is a valuable service, several aspects could be improved.\nMinor: \n-It would be great to have a table or list (in the appendix) detailing a comparison of the provided data sets to those in PDEarena (and PDEbench).\nMajor:\nThe relative error values in the results tables are for the most part shockingly bad and simply not useful for many numerical analysis contexts. Given that PINNs seem to be mostly providing different function spaces for PDE solutions, one original base PINN should be included in the benchmark, which is to give each hat function on a finite element mesh one parameter, and hence include finite element methods. Because some of the data sets were created using FEM, the original mesh would yield 0 error, but different meshes may not, and in particular coarser meshes would accumulate error. Analyzing a curve of remeshing from same resolution to coarse would provide a baseline for the performances of the other PINNs.\nIn the above sense, it also becomes important to quantify flop counts. It appears that most PINNs need to be fitted for each PDE solution, incurring the typically high flop count of solving an optimization problem (compared to one forward pass), and only some of them can learn solutions conditional on hyperparameters given as input and require only forward passes to solve e.g. from different inital conditions.\nFor all cases, there should be 3 different flop counts provided: 1) The number of flops required to create the training set 2) The number of flops required for any general training of the method  3) the number of flops required to evaluate/fit the method on a particular example. Many PINNs, and the FEM baseline would only have nonzero counts in point 3, and it would be good to compare them.\nHaving flop counts or even wall time counts would allow answering questions like \"at equal error rate, does fitting a PINN or fitting FEM cost more computational power?\" and \"At equal computing power, can FEM beat the error rates of the listed PINNs?\"\ncontinuing the discussion about flop counts, methods learning from multiple data sets/examples should be included in order to compare flop counts and provide additional reference error values. In particular for the time propagating PDEs, solutions using U-nets or FNOs from e.g. PDE bench should be included as reference values, in terms of performance, flops required for training, flops required to generate the required training data, and flops required to run a forward pass to obtain a solution. Then one can assess how many PINNs or FEM solutions one can compute for the same budget as a certain number of forward passes of the propagator network. The should be a break-even point at some number of forward passes justifying the training effort.\nWithout these points, the benchmark is unfortunately sitting just beyond actual widespread utility. I would highly encourage the authors to add these baselines to make the benchmark useful. Despite my positive bias towards benchmarking efforts I cannot recommend acceptance of this paper in its current state.\nQuestions:\nWould it be possible to address the major issues listed above among weaknesses?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Thank you for your valuable review", "Subheading": "Official CommentbyAuthors17 Nov 2023, 04:26Everyone", "Content": "Comment:\nDear Reviewer Ytqc,\nThank you for your valuable feedback and suggestions on improving our paper. Here we address responses to your questions.\nQ1.\nThe relative error values in the results tables are for the most part shockingly bad and simply not useful for many numerical analysis contexts.\nA1.\nIn the experiments presented in this paper, a variety of metrics such as L2RE, L1RE, MSE, and Fourier error at different frequencies were employed to demonstrate the discrepancies between the predicted solutions and the true solutions. Importantly, the poor performance of L2RE in certain tasks is attributed to the deliberate selection of PDEs with varying degrees of difficulty. For instance, NS2d-LT and Heat2d-LT highlight the current challenges in mitigating error accumulation over long durations in PINNs, thereby drawing attention to this issue. Moreover, most PINN-related studies compare L2RE in their results; hence, our utilization of L2RE as a primary criterion facilitates a more effective comparison with other research findings.\nQ2.\nAnalyzing a curve of remeshing from the same resolution to coarse.\nA2.\nFirstly, PINNs are not purely data-driven machine learning methods; they do not require a training set. Thus, even using original FEM grid points does not reduce the error to zero at these points. Secondly, we present the trend of L2RE variation with increasing grid points (corresponding to batch size) in\nTable 15\nand\nFigure 2\n, accompanied by an analysis in the main text.\nQ3.\nOn using PINNs to predict FEM's hat function.\nA3.\nWe believe that combining the strengths of PINNs and FEM in this manner could be an interesting approach worthy of in-depth investigation. However, as a benchmark, comparing this currently nascent method falls outside the scope of this paper. We anticipate considering its inclusion in our comparisons once more detailed related research is published in the future.\nQ4.\nCalculation of FLOPS.\nA4.\nWe have listed the training and inference FLOPs of different methods across various PDEs in the\nTable 13, Table 16, Appendix E.1\n. As for the flops for data generation in main experiments, we only generate FEM solution for evaluating different approaches on a single instance per PDE. Thus the cost is negligible as we do not need parametric data like PDEBench.\nQ5.\nComparison with methods like U-Net, FNO, etc.\nA5.\nIn all our experimental settings for positive PDE problems, no data was provided; we sought to solve using PINNs solely based on the PDE itself. In contrast, neural operator methods such as FNO and U-Net are data-driven and, thus, are not applicable to our problem settings.\nQ6.\nComparing datasets and PDEs with those used in PDEBench/PDEArena.\nA6.\nThe PDEs we selected are not significantly related to those chosen in PDEBench or PDEArena. PDEBench and PDEArena primarily focus on time-dependent PDEs like compressible Naiver-Stokes equations, Diffusion Reaction equations, etc. Moreover, their complexity is such that PINNs often fail to solve them correctly or yield substantial errors. Our selection of PDEs, derived from various PINN publications, encompasses a diverse range of types and complexities. For common PDEs, we selected the incompressible Naiver-Stokes equation and the Poisson equation (Darcy flow) due to their representativeness and widespread applicability in numerous fields."}, {"Heading": "Sincerely looking forward to the further discussions", "Subheading": "Official CommentbyAuthors22 Nov 2023, 03:35Everyone", "Content": "Comment:\nDear reviewer Ytqc,\nWe are wondering if our response and revision have resolved your concerns. In the revised version and rebuttal responses, we have conducted experiments like flops computation and revised our manuscript. If our response has addressed your concerns, we would highly appreciate it if you could re-evaluate our work and consider raising the score.\nIf you have any additional questions or suggestions, we would be happy to have further discussions.\nBest regards,\nauthors"}, {"Heading": "Official Comment by Reviewer Ytqc", "Subheading": "Official CommentbyReviewer Ytqc04 Dec 2023, 19:30 (modified: 15 Mar 2024, 00:27)EveryoneRevisions", "Content": "Comment:\nA6. The PDEs we selected are not significantly related to those chosen in PDEBench or PDEArena.\nThere is overlap (e.g. Burgers' equation), so this statement is False, unless Burgers' equation is not significant. My suggestion was to add a table in the appendix to highlight the differences between the data sets. This was in an effort to help the authors point out the gap that they are filling by introducing this data set ...\nQ2. Analyzing a curve of remeshing from the same resolution to coarse.\nA2. Firstly, PINNs are not purely data-driven machine learning methods; they do not require a training set. Thus, even using original FEM grid points does not reduce the error to zero at these points.\nI do not understand this response. The point here was to have a baseline that shows how FEM using coarser and coarser grids fares and can lead to a plot of compute power vs error that should be at minimum attained by PINN methods before they can be considered useful\nA3. We believe that combining the strengths of PINNs and FEM in this manner could be an interesting approach worthy of in-depth investigation.\nAs above, the idea was to include FEM as a baseline. By \"base PINN\" I mean one where each FEM hat function receives a parameter and the solution is obtained using a sparse matrix solve, i.e. the FEM method.\nI feel uneasy about this contribution because it artificially creates boundaries to other existing methods thereby avoiding comparison. If you propose a data set, why restrict it to PINNs? I will increase my score, because having more data sets available is a good thing, but am generally unhappy with the way it is presented."}]}, {"Heading": "Official Review of Submission9493 by Reviewer RGvf", "Subheading": "Official ReviewbyReviewer RGvf31 Oct 2023, 23:56 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper provides a comprehensive comparison of PINN training methods, problems, and data. The paper visits common problems with training PINNs, namely the complex geometry, the multi-scale phenomena, nonlinearity of some PDE ofrs, and the high dimensional PDE problems. They also provide various training mechanisms such as domain decomposition and loss reweighting methods.\nSoundness:\n4 excellent\nPresentation:\n4 excellent\nContribution:\n4 excellent\nStrengths:\nThe paper is well-written; it seems obvious this work has gone through a few rounds of polishing and review.\nThe literature review is detailed and comprehensive.\nThe challenging aspects of training PINNs are decomposed and categorized well.\nThe appendix section of the paper is thorough and contains quality information.\nThe suite of experiments is admittedly comprehensive; there are more than 20 PDE forms, 10 methods considered and compartmentalized well in this paper.\nThe scale of the experiments and the analyses of the hyper-parameters is certainly admirable.\nWeaknesses:\nI'm saying out of respect to the author's work, but this paper may be more suited for a journal format. In particular, the page limit constraint is hitting the work hard in my opinion.\nBy the time the authors present the data and experiments, there is less than half a page left to interpret the results and provide discussions and conclusions.\nMany key discussions, at different points in the main text, were deferred to the appendix. While they do exist in the appendix and carry out important information, they carry more scientific content than the existing paper's text.\nTo be clear, the paper's topic is certainly relevant to ICLR and could benefit the ICLR community. However, the conference format may not be the most suitable to present the work as best as it could have been.\nThe work utilizes 10 different methods for training PINNs, but a brief description of these methods in a single mathematical framework is missing. Adding such a description and correlating the numerical findings to the theoretical properties of each method is probably the most important, yet under-performed, part of the work in my opinion.\nTo be clear, I understand the paper's space constraints, but this is very important in my opinion. The least the authors could do is to add such a section, however briefly, to the appendix.\nQuestions:\nSee the weaknesses section.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Thank you for your valuable review", "Subheading": "Official CommentbyAuthors17 Nov 2023, 04:32Everyone", "Content": "Comment:\nDear Reviewer RGvf,\nThank you for your valuable feedback and suggestions on improving our paper. Here we address responses to your questions.\nQ1.\nThe conference format may not be the most suitable to present the work.\nA1.\nWe acknowledge your concern regarding the format constraints. Due to space limitations, a considerable portion of our results and discussions had to be relegated to the appendix, which might inconvenience readers. However, it's worth noting that at the ICLR conference, many benchmark articles are accepted annually [1, 2, 3, 4]. Furthermore, the main conclusions of our article are presented in the main text, with the appendix serving primarily to supplement and corroborate these findings.\nQ2.\nA brief description of these methods in a single mathematical framework is missing.\nA2.\nThank you for your valuable suggestion. It is important to note that different methods contribute to the improvement of PINNs in varied ways, hence we categorized these methods and described each category within a unified framework. Due to constraints in the main text's length, we have included these descriptions in an additional section denoted as\nAppendix B.4\n.\nReferences\nEAGLE: Large-scale Learning of Turbulent Fluid Dynamics with Mesh Transformers (\nhttps://openreview.net/forum?id=mfIX4QpsARJ\n), ICLR 2023,\nLearned Coarse Models for Efficient Turbulence Simulation (\nhttps://openreview.net/forum?id=msRBojTz-Nh\n), ICLR 2022,\nSoftZoo: A Soft Robot Co-design Benchmark For Locomotion In Diverse Environments (\nhttps://openreview.net/forum?id=Xyme9p1rpZw\n), ICLR 2023,\nGeneDisco: A Benchmark for Experimental Design in Drug Discovery (\nhttps://openreview.net/forum?id=-w2oomO6qgc\n), ICLR 2022."}]}, {"Heading": "Official Review of Submission9493 by Reviewer oeuE", "Subheading": "Official ReviewbyReviewer oeuE31 Oct 2023, 01:35 (modified: 22 Nov 2023, 13:26)EveryoneRevisions", "Content": "Summary:\nThe paper provides a benchmarking tool called PINNacle which was lacking in the domain of PINNs. The tool provides a diverse set of 20 different PDEs spanning over various application domains. The tool also provides implementations of 10 state-of-the-art techniques in PINNs and shows extensive experiments to show the strengths and weaknesses of each method.\nSoundness:\n2 fair\nPresentation:\n4 excellent\nContribution:\n1 poor\nStrengths:\nThe paper is overall well written and easy to follow.\nThe paper provides an extensive comparison of the different SOTA methods for different PDEs.\nWeaknesses:\nThe paper lacks technical novelty to be considered for the main track. In my opinion, the paper is more suitable for an application/dataset track, for e.g., NeurIPS Dataset/Benchmark Track.\nThe insights provided in the paper are not novel and are also well-known in the PINN literature which the paper cites as well.\nQuestions:\nTable 3 shows that all of the selected SOTA methods fail on the KS Equation. However, some PINN methods can solve KS Equations such as Causal PINNs [1].\nWhen comparing the effect of the parametric PDEs on different PINN variants (shown in Table 4), using the Average L2RE is not a good choice. It would be more informative to show the mean and the standard deviations for the different parameter choices. The average L2RE can be skewed if one (or few) of the parameter settings fails (i.e., have L2RE of 100%) while others have very low errors (such as ~1e-4).\n[1] Wang, S., Sankaran, S., & Perdikaris, P. (2022). Respecting causality is all you need for training physics-informed neural networks. arXiv preprint arXiv:2203.07404.\nFlag For Ethics Review:\nNo ethics review needed.\nDetails Of Ethics Concerns:\nNone\nRating:\n3: reject, not good enough\nConfidence:\n5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Thank you for your valuable review", "Subheading": "Official CommentbyAuthors21 Nov 2023, 03:14Everyone", "Content": "Comment:\nDear Reviewer oeuE,\nThank you for your valuable feedback and suggestions on improving our paper. Here we address responses to your questions.\nQ1.\nThe paper is more suitable for an application/dataset track, for e.g., NeurIPS Dataset/Benchmark Track.\nA1.\nWe appreciate your perspective, yet it's pertinent to note that at the ICLR Conference, numerous benchmark and dataset articles are accepted each year, as exemplified by references [1,2,3,4]. While benchmark articles may not introduce new techniques, they play a crucial role in highlighting existing issues in the field and guiding future research directions. Therefore, we respectfully request that you reconsider the contribution of our paper in this context.\nQ2.\nAll of the selected SOTA methods fail on the KS Equation but some PINNs methods could solve it.\nA2.\nCausualPINNs represent a combination of various methods and techniques, and the training parameters and iteration numbers in their original work significantly differ from those in our study. Although their results suggest the resolution of the KS equation, it does not entirely apply under our problem settings. Given the plethora of improvements in PINN methods, we promise to gradually include more widely recognized, published, and effective works in our future research.\nQ3.\nUsing averaged L2RE in parametric PDEs experiments is not a good indicator.\nA3.\nDue to the limited space in the main text, we only presented averaged L2RE, but comprehensive statistics, including mean and standard deviation, are provided in the appendix's\nTable 27\nfor reference.\nReferences\nEAGLE: Large-scale Learning of Turbulent Fluid Dynamics with Mesh Transformers (\nhttps://openreview.net/forum?id=mfIX4QpsARJ\n), ICLR 2023,\nLearned Coarse Models for Efficient Turbulence Simulation (\nhttps://openreview.net/forum?id=msRBojTz-Nh\n), ICLR 2022,\nSoftZoo: A Soft Robot Co-design Benchmark For Locomotion In Diverse Environments (\nhttps://openreview.net/forum?id=Xyme9p1rpZw\n), ICLR 2023,\nGeneDisco: A Benchmark for Experimental Design in Drug Discovery (\nhttps://openreview.net/forum?id=-w2oomO6qgc\n), ICLR 2022."}, {"Heading": "Response to Author Rebuttal", "Subheading": "Official CommentbyReviewer oeuE22 Nov 2023, 13:25Everyone", "Content": "Comment:\nI thank the authors for providing clarifications to my questions/comments. I have read the other reviews and the authors' comments and have updated my score accordingly. I have a few follow-up comments regarding the paper:\nI would like to thank the authors for pointing out that benchmark papers are accepted at ICLR, which I was not familiar with, and I completely understand the importance of benchmarking for the community. As a side note, two of the four example benchmark papers [1, 2] mentioned by the authors are not purely benchmarks and they have algorithmic innovations. However, I do appreciate the efforts that the authors have put into this paper for creating a benchmark with diverse PDEs and PINN variants.\nFrom reading the other reviews and discussions, I also share the same concern as other reviewers that a journal format might be more suitable for this work as most of the interesting discussion of the results are in the Appendix.\nAs a benchmark study, I feel it would further strengthen the paper if more detailed discussions about the results and the some deeper insights about them are included, as opposed to just reporting the results and suggesting suitable approaches. While I recognize the constraints of the ICLR format in terms of page limits, I believe that an expanded discussion could significantly enhance the paper's impact in the scientific ML community.\nMinor Comment:\nAlthough the major focus of this work has been on using PINNs for forward problems, the practical applications on PINNs are mostly for inverse problems which has not been extensively explored in this paper. If the authors are planning to extend the benchmark, it would be great to look into a more diverse set of inverse problems."}]}, {"Heading": "Official Review of Submission9493 by Reviewer xNZs", "Subheading": "Official ReviewbyReviewer xNZs30 Oct 2023, 18:36 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe article introduces \"PINNacle\", a robust benchmark suite tailored for Physics-Informed Neural Networks (PINNs). This suite boasts a rich assortment of over 20 intricate PDE challenges, complemented by a user-centric toolbox that houses over 10 of the latest PINN techniques. These techniques are segmented by the authors into categories: loss reweighting, advanced optimizers, unique loss functions, and groundbreaking architectures. An exhaustive analysis is then executed with this benchmark dataset to scrutinize these variations.\nMany of the challenges pinpointed in the dataset resonate with a multitude of real-world scenarios. Thus, the efficacy of a method in tackling these challenges becomes a credible measure of its real-world utility. To generate the data, the authors employ the FEM solver from COMSOL 6.0 for intricately geometric problems and the spectral method from Chebfun for the more chaotic issues. This dataset encompasses challenges like the heat equation, Poisson equation, Burgers' equation, Navier-Stokes equation, among others.\nThe paper outlines a uniform criteria to gauge the performance of varied PINN techniques across all challenges, promoting a methodical comparison of different tactics. Performance assessment is conducted using various metrics, such as accuracy, convergence rate, and computational prowess. Moreover, the authors shed light on the advantages and limitations of these methods, providing direction for subsequent studies, especially in fields like domain decomposition and loss reweighting.\nIn essence, the article's merits lie in its crafting of a dataset that mirrors significant challenges confronted by PINNs, establishing a uniform assessment criteria for different PINN approaches, and giving valuable insights on the strengths and pitfalls of these methods. This work undeniably propels the growth of PINNs, igniting further creativity and advancements in this burgeoning domain.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThis paper stands out with several merits, accentuating its importance in the realm of Physics-Informed Neural Networks (PINNs).\nTo begin with, it offers an all-encompassing benchmark suite for PINNs, showcasing a varied dataset containing over 20 intricate PDE challenges, supplemented by an accessible toolbox with more than 10 leading PINN techniques. This suite facilitates an organized comparison of multiple approaches and delivers a uniform metric to evaluate the efficacy of various PINN methodologies across tasks.\nNext, the authors embark on an in-depth evaluation using the benchmark dataset to appraise these variations. They measure the performance through multiple indicators such as accuracy, convergence speed, and computational prowess. Their findings elucidate the advantages and pitfalls of these methods, charting a course for prospective studies, especially in areas like domain decomposition and loss reweighting.\nMoreover, the challenges pinpointed in the dataset find parallels in many real-world scenarios. Hence, how a method navigates these challenges becomes a tangible testament to its applicability in practical contexts. This tangible applicability amplifies the relevance of both the benchmark suite and the research's findings to field professionals and researchers.\nIn conclusion, this work marks a significant leap in the trajectory of PINNs, fueling further innovation and exploration in this riveting domain. The paper's offerings, spanning from the benchmark suite to the critical insights, are poised to galvanize more in-depth investigations and advancements in PINNs, ushering in enhanced solutions for real-world quandaries.\nWeaknesses:\nThe paper has some areas it could improve on.\nFirst, the authors only discuss PINN methods. They didn't look at other common methods. It would be good to see how PINN methods compare to these.\nSecond, they didn't give much detail on what computer stuff is needed for PINN methods. They did say if the methods work fast or slow. But, it would be helpful to know what computer tools or power is needed. People who want to use these methods would find that information useful.\nLast, the authors worked with a set of 20 PDE problems. But they might have missed some other important problems. In future studies, it would be good to add more problems to their list. This way, we can learn even more.\nQuestions:\nHow did you ensure that the PINN methods you evaluated were able to handle the diverse range of PDEs in your dataset, and what challenges did you encounter in this process?\nCan you describe the process of training the neural networks for each PDE, and how you optimized the hyperparameters for each method?\nHow did you handle issues such as boundary conditions and initial conditions in your experiments, and what strategies did you use to ensure that these conditions were satisfied?\nCan you discuss the limitations of your benchmarking tool, and how future research could address these limitations to further advance the field of PINNs?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Thank you for your valuable review", "Subheading": "Official CommentbyAuthors17 Nov 2023, 04:55Everyone", "Content": "Comment:\nDear Reviewer xNZs,\nThank you for your valuable feedback and suggestions on improving our paper. Here we address responses to your questions.\nQ1.\nThis work only compares PINN variants but did not look at other methods.\nA1.\nPINNs represent a highly significant and promising approach, with numerous studies dedicated to their improvement. Given the diverse range of enhancements applicable to PINNs, conducting a systematic comparison within this domain alone is a substantial undertaking that can guide the development of future methodologies.\nQ2.\nDetails of the computer stuff needed for PINNs.\nA2.\nAll our experiments were conducted using a Linux server with 20 Intel(R) Xeon(R) Silver 4210 CPUs @ 2.20GHz and eight NVIDIA GeForce RTX 2080 Ti each with 12 GB GPU memory. We have included this information in\nAppendix E.1\n.\nQ3.\nHow did you ensure that the PINN methods you evaluated were able to handle the diverse range of PDEs in your dataset, and what challenges did you encounter in this process?\nA3.\nThe selection of PDEs for our benchmark was derived from a range of representative literature, encompassing typical equations categorized by their mathematical properties. Specifically, we chose 7 major categories of PDEs, relevant to various fields like fluid dynamics and electromagnetism. The primary challenge lay in the multitude of PDE types and the complexity of categorization. Our solution involved classifying PDEs by mathematical form, then grouping practical problems under these different types, and designing varying levels of difficulty.\nQ4.\nCan you describe the process of training the neural networks for each PDE, and how you optimized the hyperparameters for each method?\nA4.\nIf I understand your question correctly, in our main experiments, we fixed certain hyperparameters like 1e-3 for learning rate, 8192/32768 for batch size, 20000 for epochs, etc., and adopted an approach of running three repetitions to ensure reproducibility. For hyperparameter selection, we initially studied the impact of some shared hyperparameters in\nFigure 2, Figure 3, Appendix E.2\nand chose a balanced parameter in terms of effectiveness and computational cost. For method-specific hyperparameters, detailed studies are provided in the appendix\nAppendix E.2, Table 19~Table 26\n.\nQ5.\nHow did you handle issues such as boundary conditions and initial conditions in your experiments, and what strategies did you use to ensure that these conditions were satisfied?\nA5.\nDifferent types of PINN variants have their own approaches to handling boundary conditions. For instance, vanilla PINNs directly incorporate boundary conditions into the loss function combined with PDE loss, so the results may not strictly satisfy boundary conditions. Some methods, like FBPINNs, employ strategies such as hard constraints to restrict the solution space to a certain type of boundary conditions.\nQ6.\nCan you discuss the limitations of your benchmarking tool, and how future research could address these limitations to further advance the field of PINNs?\nA6.\nAs a benchmark, its main limitation is the difficulty in continuously following new work and fairly, comprehensively testing its strengths and weaknesses. For future work, if researchers correctly use our benchmark for comparison, we can establish a leaderboard. This approach will make researchers aware of the current issues in PINNs and continue to drive progress in the field."}]}]}, "eUgS9Ig8JG": {"paper_info": {"Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Graph Neural Networks, Higher-order Representation Learning, Simplicial Complexes, Simplicial Neural Networks, Weisfeiler-Lehman Isomorphism Test", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "Simplicial neural networks (SNNs) are deep models for higher-order graph representation learning. SNNs learn low-dimensional embeddings of simplices in a simplicial complex by aggregating features of their respective upper, lower, boundary, and coboundary adjacent simplices. The aggregation in SNNs is carried out during training. Since the number of simplices of various orders in a simplicial complex is significantly large, the memory and training-time requirement in SNNs is enormous. In this work, we propose a scalable simplicial-aware neural network (SaNN) model with a constant run-time and memory requirements independent of the size of the simplicial complex and the density of interactions in it. SaNN is based on pre-aggregated simplicial-aware features as inputs to a neural network, so it has a strong simplicial-structural inductive bias. We provide theoretical conditions under which SaNN is provably more powerful than the Weisfeiler-Lehman (WL) graph isomorphism test and as powerful as the simplicial Weisfeiler-Lehman (SWL) test. We also show that SaNN is permutation and orientation equivariant and satisfies simplicial-awareness of the highest order in a simplicial complex. We demonstrate via numerical experiments that despite being computationally economical, the proposed model achieves state-of-the-art performance in predicting trajectories,  simplicial closures, and classifying graphs.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "Supplementary Material": "zip", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Primary Area": "learning on graphs and other geometries & topologies", "Submission Number": "9491", "PDF Url": "https://openreview.net/pdf?id=eUgS9Ig8JG"}, "review_info": [{"Heading": "Request full source code", "Subheading": "Public CommentbyRongqin Chen04 Nov 2024, 03:09Everyone", "Content": "Comment:\nDear Authors,\nThank you for your excellent paper. I am very interested in the proposed method and would like to explore the implementation details further. However, I noticed that the source code provided in the supplementary material appears to be incomplete.\nCould you please provide the complete source code? This would greatly assist me in understanding and replicating your results.\nThank you for your time and assistance.\nBest regards."}, {"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:53 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nAccept (spotlight)"}, {"Heading": "Meta Review of Submission9491 by Area Chair Aj5i", "Subheading": "Meta ReviewbyArea Chair Aj5i10 Dec 2023, 05:05 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThe paper presents an approach for more efficient simplicial complex learning by trading the high computational complexity of high order graph learning (due to the high number of simplicies) with cheaper, precomputed features (computed one-time). The paper provides theory justifying the expressive power achieved when incorporating these features within the WL theoretical framework. Experiments support the efficiency claims compared to existing simplicial complex learning baselines. While reviewers appreciate the method and the computational saving the fact that GNN works have followed this path of adding features and proving WL expressive power reduces a bit the novelty in this work.\nJustification For Why Not Higher Score:\nAs mentioned above, while this is a well executed work, the conceptual novelty is somewhat limited.\nJustification For Why Not Lower Score:\nThe work does state a useful goal of making higher order graph learning more scalable, which is a worthy and important goal. The paper provides algorithm and theoretical analysis to achieve this goal. There is also an experimental evidence supporting the paper's claim."}, {"Heading": "Revised Manuscript Incorporating Reviewers' Feedback Uploaded", "Subheading": "Official CommentbyAuthors20 Nov 2023, 12:46Everyone", "Content": "Comment:\nDear Reviewers,\nThank you for your time and the positive evaluation of our manuscript. We have now uploaded the revised version, in which we believe we addressed all your concerns. In particular, in the revised version, we (a) defined oriented incidence matrices and clarified when we use oriented and unoriented simplices to improve the exposition; (b) reported precomputation times; (c) introduced the notation of the order of simplicial complexes as $K$ and corrected the upper limit of $k$ where necessary. Please refer to individual comments for responses to your questions and weaknesses.\nBest regards,\nAuthors"}, {"Heading": "Official Review of Submission9491 by Reviewer jAzK", "Subheading": "Official ReviewbyReviewer jAzK08 Nov 2023, 15:13 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper describes an efficient, and effective approach for learning representations for simplices in a simplicial complex. The central idea is that of using injective functions for aggregating simplicial features, as it ensures that the embeddings are unique. The simplicial features are aggregated over upper, lower, boundary and co-boundary adjacencies. The paper provides precise definitions and theorems and statements on the properties of the networks. The proofs are summarized in the main body and provided in full detail in the appendices. The method is further experimentally validated and shows that the proposed model (SaNN) is both efficients (significantly faster than any of the other baselines) and effective (performance within the uncertainty intervals on accurcies, or above the baselines).\nSoundness:\n4 excellent\nPresentation:\n3 good\nContribution:\n4 excellent\nStrengths:\nI am impressed by the clarity of presentation in the paper. I find talking and reading about simplicial complex often a messy business given all the types of simplices and adjacencies, and the abstract notion in the first place. It is clear that the authors though well about how to present the math. This includes proper use of figures.\nThe goal of the paper itself -efficiency whilst not compromising on expressivity- is relevant and important, and it is great to see the authors succeeding in reaching this goal.\nI appreciate the summary of the proofs after the formal statements.\nNext to a sound theoretical exposition, the experiments are thorough as well and include many ablation studies that are used to distill insightful take home messages.\nWeaknesses:\nI only have 1 important concern:\nAlthough the main principles are clear, I am still confused about the actual architecture/predictive models. In the end we have equation 8, but it describes a representation for each of the $N$ sets of $k$-simplices, each consisting of the $N_k$ simplices. It is unclear how to distill a global prediction out of all these representations, as would be needed for e.g. the classification tasks. Details on how the architectural design for each of the benchmarks is missing.\nQuestions:\nCould you respond to the above concern, and additionally address the following questions/comments?\nOn several occasions the notion of \"non-deep baselines\" is used. What is meant by this. Could you clarify what non-deep means here, which methods are these?\nIn section 2 when presenting the symbols it is mentioned that $k=1,2,\\dots,N+1$. Does $k$ always run up all the way to $N+1$?\nIn section 4. The sentence that starts with \"The theorem implies that any arbitrary ...\" is extremely long and hard to comprehend. I suggest to split it 2 or 3 sentence to improve readability.\nJust above property 1 it is mentioned \"other commonly used sum, mean, or max read-out functions are not injective\" I am not fully sure I understand it correctly. The paragraph above explains that sum aggregation is the best injective aggregator, in contrast to mean aggregation. I think the statement that I just quoted is about aggregating over the different $\\mathbf{Y}$'s? Perhaps this can be clarified.\nIn the tables: since colors red and blue are used you might as well color the text in the caption as well. I.e. \"The {\\color{red}first} and {\\color{blue}second} best performances ...\"\nThe insights section says \"The deep models are observed to perform exceptionally better than logistic regression\", where do I see this? Logistic regression taking what as input? Could this be clarified.\nThank you for considering my comments and questions.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n8: accept, good paper\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors20 Nov 2023, 01:14Everyone", "Content": "Comment:\nWe deeply appreciate your kind words regarding the clarity of our presentation and the balance between efficiency and expressivity in our work. Thank you for acknowledging the thoroughness of our theoretical and experimental sections."}, {"Heading": "Clarification on Architectures/Predictive Models", "Subheading": "Official CommentbyAuthors20 Nov 2023, 01:18Everyone", "Content": "Comment:\nThe predictive models, which are used to learn task-specific embeddings from the embeddings of all simplices (as calculated using Equation 8), are detailed in Appendix H.\nTo use the edge embeddings learnt using SaNN to perform trajectory prediction, we map the embeddings from the vector space of edge embeddings to the vector space of node embeddings by multiplying them by the incidence matrix $\\mathbf{B}_1^T$. We consider the node embeddings of only those nodes that have a possibility of being the next in the trajectory, which are the neighboring nodes of the last node in a trajectory. For a trajectory t, which ends at node i, we use a masking function to extract the embeddings corresponding to the neighboring nodes of the node i. We do this for node embeddings from different depths and concatenate them. We then give the combined node embeddings as input to a decoder MLP which maps the node embeddings to a vector space of dimension same as the maximum number of neighboring nodes of the last node in any trajectory in the dataset. The last layer of the decoder is a softmax activation function, the output of which for a trajectory is a vector containing the probabilities with which the neighboring nodes of the trajectory\u2019s last node become the successor node of that trajectory. For all the datasets, we train SaNN in a supervised manner using the cross-entropy loss function.\nTo perform simplicial closure prediction for triangles, we concatenate the embeddings of the constituent nodes and edges in the open triangles in the training set. The concatenated embeddings are given as input to a decoder MLP with a sigmoid activation function in the last layer, the output of which is the probability of the open triangles forming a simplex. For all the datasets, we train SaNN in a supervised manner using the binary cross-entropy loss function. Since the datasets are heavily skewed with many negative examples (open triangles that never get closed) than positive examples (open triangles that get closed), we perform oversampling of the positive examples in each training batch and use a weight for positive examples in the binary cross-entropy loss function.\nTo perform graph classification using the embeddings learnt using SaNN, we combine embeddings of the constituent simplices of all orders in the clique-lifted graphs. Since the number of k-simplices in a training example (a clique-lifted graph) is large concatenating embeddings of all the k-simplices in a training example will result in very high-dimensional embeddings. Therefore, we use summation as the readout function at $k$-simplices level. We, finally, use concatenation as the global-level readout function. Specifically, we take a summation of all the node embeddings, edge embeddings, and triangle embeddings in a graph and concatenate the nodelevel, edge-level, and triangle-level embeddings of the simplicial complex to get one global representation for the simplicial complex. The global embeddings of graphs are given as input to a decoder MLP with either a sigmoid activation function or a softmax activation function as the final layer depending on whether the dataset has two or more classes. For all the datasets, we train SaNN in a supervised manner using the cross-entropy loss function, which takes the output of the decoder and the true graph labels as the input.\nThe hyperparameters used in SaNN to implement the different tasks are summarized in Tables 5, 7, and 9. We have maintained consistency in our experiments by using the same hyperparameters, such as depth ($T$), order ($K$), and hidden dimensions, across all baseline SNN models. This was done to ensure a fair computational comparison. The predictive models for the three tasks are also the same for all the neural network-based models. Due to space constraints, we provided all these details in the Appendix.\nTo provide further clarity about the hyperparameters used for benchmarks, we have added the following line to Appendix H: \n\"For a fair computational comparison, the same predictive models, and hyperparameters, namely depth ($T$), order ($K$), and hidden dimensions, were utilized for all the baseline SNN models."}, {"Heading": "Clarification Regarding 'Non-Deep Baselines'", "Subheading": "Official CommentbyAuthors20 Nov 2023, 01:21Everyone", "Content": "Comment:\nIn our paper, the term \"non-deep\" refers to models that do not utilize deep learning methodologies. Examples include techniques like Projection for trajectory prediction and Logistic Regression for simplicial closure prediction.\nThe projection based method is used as a baseline for trajectory prediction in Roddenberry et al. (2021). In the projection-based method for trajectory prediction, a mapping that projects the input edge signal corresponding to a trajectory onto the kernel of the Hodge Laplacian is considered. The output is passed through a softmax layer to pick a successor node in the trajectory.\nFor simplicial closure prediction, we use logistic regression as a non-deep baseline, following the approach proposed by Benson et al. (2018). In this context, logistic regression takes as its input the harmonic, geometric, and arithmetic means of the three edge weights in the open triangle. This is a method suggested by Benson et al. (2018) in their work."}, {"Heading": "Clarification Regarding the Order of Simplicial Complexes", "Subheading": "Official CommentbyAuthors20 Nov 2023, 01:26Everyone", "Content": "Comment:\nThank you for bringing this to our attention. You are correct; the variable $k$ does not necessarily run up all the way to $N+1$. This was an oversight on our part. In the revised manuscript, we introduced the notation of the order of the simplicial complex as $K$ and corrected the upper limit of $k$ wherever applicable."}, {"Heading": "Revising the Lengthy Sentence", "Subheading": "Official CommentbyAuthors20 Nov 2023, 01:29Everyone", "Content": "Comment:\nThank you for your feedback. You are correct that the sentence in Section 4 was quite lengthy. To improve clarity, we haven broken it down as follows and have made changes in the manuscript:\n\u201cThe theorem we present implies that any arbitrary extension of GAMLPs (Chen et al., 2020), SPIN (Doshi & Chepuri, 2022), or SIGN (Rossi et al., 2020) to higher-order simplices does not result in the node-embeddings from SaNN having a superior expressive power GNNs. In one possible extension, we could replace the integer powers of adjacency matrices in these graph models with those of the Hodge Laplacian matrices. These matrices generalize the graph Laplacian to simplicial complexes and are defined as the sum of upper and lower Laplacians. However, even with this modification, the expressive power of these extended models does not surpass that of GNNs.\u201d"}, {"Heading": "Clarification Regarding Injectivity of Sum Aggregator and Readout Functions", "Subheading": "Official CommentbyAuthors20 Nov 2023, 01:35Everyone", "Content": "Comment:\nWhen we discuss the injectivity of an aggregation function, we are referring to its ability to distinguish between non-isomorphic simplicial complex structures with identical initial features. In this context, the sum aggregation function is effective at producing distinct outputs for distinct structures after aggregation.\nThe sentence above Property 1, however, is discussing the read-out function which combines features of simplices aggregated from different types of neighborhoods. Here, the sum function may not be injective, as combining different sets of embeddings through summation could result in identical outputs."}, {"Heading": "Color Coding in Captions", "Subheading": "Official CommentbyAuthors20 Nov 2023, 01:38Everyone", "Content": "Comment:\nThank you for your suggestion regarding the color coding in the captions of tables. We have incorporated this change in our revised manuscript."}, {"Heading": "Clarification Regarding Logistic Regression Baseline", "Subheading": "Official CommentbyAuthors20 Nov 2023, 01:40Everyone", "Content": "Comment:\nIn numerical experiments in the paper, we use logistic regression as a non-deep baseline for simplicial closure prediction, following the approach proposed by Benson et al. (2018). In this context, logistic regression takes as its input the harmonic, geometric, and arithmetic means of the three edge weights in the open triangle. This is a method suggested by Benson et al. (2018) in their work."}, {"Heading": "Thank you for the clarification", "Subheading": "Official CommentbyReviewer jAzK20 Nov 2023, 05:08Everyone", "Content": "Comment:\nI checked appendix H which indeed gives sufficient details on the experiments. I also appreciate the given responses to my questions. Although the mentioned minor changes to the paper should be easy to incorporate,  I would appreciate seeing an updated pdf on openreview. Is it possible to do this?\nThank you!"}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors20 Nov 2023, 22:16Everyone", "Content": "Comment:\nThank you for your feedback and for acknowledging the details provided in Appendix H. We are glad to hear that the responses to your questions were satisfactory.\nWe have uploaded the revised manuscript. The revised version incorporates all the changes that we discussed.\nWe appreciate your time and constructive comments throughout this process. Please feel free to review the updated manuscript and share any further thoughts or questions you might have."}, {"Heading": "Official Comment by Reviewer jAzK", "Subheading": "Official CommentbyReviewer jAzK22 Nov 2023, 10:41Everyone", "Content": "Comment:\nThank you, much appreciated. I maintain my score."}]}, {"Heading": "Official Review of Submission9491 by Reviewer 7LnN", "Subheading": "Official ReviewbyReviewer 7LnN06 Nov 2023, 02:31 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe authors propose a class of simple simplicial neural network models, referred to as simplicial-aware neural\nnetworks (SaNNs), which leverage precomputation of simplicial features. The authors theoretically demonstrate that under certain conditions, SaNNs are better discriminators of non-isomorphic graphs than the WL and SWL test. Empirically, SaNNs are shown to perform competitively against other SNNs and GNNs on tasks such as trajectory prediction, simplicial closure prediction, and several graph classification tasks over various datasets.\nSoundness:\n3 good\nPresentation:\n4 excellent\nContribution:\n3 good\nStrengths:\nThe theoretical results are intriguing. Indeed, a competitor to the WL and SWL tests would be a valuable contribution to the graph ML community.\nA wide variety of benchmarks over several tasks and datasets are conducted to demonstrate the efficacy and efficiency of SaNNs.\nSaNNs inherit several valuable invariance properties of other SNNs including permutation invariance, orientation invariance, and simplicial-awareness.\nCompared to MPSNs, consideration of higher-order simplices does not blow up computation complexity.\nWeaknesses:\nIt is unclear for a research with limited expertise in this rather niche area to conclude the strength of the conditions prescribed in Theorems 4.1 and 4.2. (See questions.)\nThere do not appear to be any results describing the pre-computation time which should be included in any run-time comparisons which I imagine should scale near-exponentially with graph size and order of simplices considered.\nSaNNs are often not outright the winner in terms of prediction accuracies for the tasks displayed in Tables 1 and 3. For example, in Table 1, the SaNN is outcompeted by Projection and Scone on 3/4 of the datasets and the run-time savings of SaNN are not significant enough to justify usage of the SaNN. In Table 3, SaNN is not the leader in 4/5 of the datasets and it is not even the fastest. On the other hand, the time savings against MPSN are quite significant, but since many practitioners of graph learning expect training to take significant amounts of time, accuracy is the topmost priority, so there wouldn't be a strong enough justification to go with a SaNN.\nQuestions:\nIs assuming the learnable transformation functions $g_k^{(t)}\\cdot)$ are injective too strong? Although the MLPs will be injective, appealing to the Universal Approximation Theorem to declare that $g_k$ can be injectively-approximated is probably not practical.\nI may have missed this but are pre-computation times explicitly indicated in the results?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n8: accept, good paper\nConfidence:\n2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors20 Nov 2023, 01:41Everyone", "Content": "Comment:\nWe sincerely appreciate your positive feedback and recognition of the potential impact of our work in the graph ML community. Your acknowledgment of the strengths of our SaNN model, its invariance properties, and its computational efficiency is highly encouraging. Thank you."}, {"Heading": "Clarification Regarding the Usage of the Universal Approximation Theorem", "Subheading": "Official CommentbyAuthors20 Nov 2023, 01:44Everyone", "Content": "Comment:\nWe agree that citing the Universal Approximation Theorem to propose that $g_k$ can be injectively approximated may seem like a strong assumption. This assumption is primarily used for theoretical justification, and we understand its potential limitations in practical applications.\nHowever, it is important to note that we have only used multilayer perceptrons (MLPs) as one possible example of the many functions that $g_k$ could be. It is possible to choose any suitable learnable injective function. Our choice of MLPs is simply an example, and it has indeed performed well in our numerical experiments. Therefore, while the injectivity assumption offers theoretical support, it does not diminish the practical effectiveness of our proposed method."}, {"Heading": "Regarding Inclusion of Precomputation Times", "Subheading": "Official CommentbyAuthors20 Nov 2023, 02:14 (modified: 20 Nov 2023, 03:28)EveryoneRevisions", "Content": "Comment:\nWe appreciate your feedback regarding the inclusion of precomputation times in our overall computation assessment. While we agree that precomputation is indeed part of the overall process, it generally constitutes a relatively insignificant portion compared to the cumulative time spent on training and testing. \nNonetheless, in response to your suggestion, we incorporated precomputation times into our analysis to provide a more comprehensive view of our method's performance. We present here the precomputation and per epoch training time values of SaNN and the per epoch training time values of baseline SNN modesl for the three tasks at hand. The first terms in the runtime values of SaNN correspond to the precomputation times and the second terms to the per epoch training times.\nTrajectory prediction:\nDataset\nOcean\nSynthetic\nPlanar\nMesh\nScoNe\n0.4\n3.2\n30.2\n18.9\nSCNN\n1.9\n4.2\n36.3\n29.8\nSaNN\n0.01,0.1\n0.06,2.5\n1.22,26.8\n0.39,8.1\nSimplicial closure prediction:\nDataset\nEnron\nHigh-school\nPrimary-school\nNDC-classes\nMath-sx\nMPSN\n255\n413\n3499\n-\n-\nSCNN\n17\n401\n1891\n-\n-\nSaNN\n0.01, 3\n0.05, 112\n0.76, 916\n0.26, 13\n95.91, 52883\nGraph classification:\nDataset\nProteins\nNCI1\nIMDB-B\nReddit-B\nReddit-M\nMPSN\n33\n292\n46\n242\n1119\nSaNN\n3.6, 0.4\n6, 58\n4, 8\n6, 45\n34, 104\nThe values provided represent the training time per epoch. Out-of-memory results are indicated by \u2212. The total training time for SaNN is calculated as follows:\nTotal training time = Precomputation time + (Number of epochs * Training time per epoch). This total is substantially less than the precomputation times."}, {"Heading": "Addressing the Comparative Performance of SaNN", "Subheading": "Official CommentbyAuthors20 Nov 2023, 02:19Everyone", "Content": "Comment:\nWe agree that SaNN does not consistently outperform other models in terms of prediction accuracy for certain datasets. However, the true strength of SaNN lies in its substantial runtime improvements and theoretical characterization while maintaining competitive accuracy. This computational efficiency is crucial for practical applications.\nThe reduced training time that SaNN offers enables more frequent model retraining, which is particularly beneficial for real-time applications. It also ensures scalability for larger datasets. As shown in Table 2, traditional SNN models struggle with large datasets, while SaNN excels, especially with datasets in simplicial closure prediction where existing SNNs face memory constraints.\nIn Table 3, although SaNN is not the leader in average accuracies for most datasets, the high standard deviations suggest significant overlap in accuracy results, rendering the differences between the top-performing models statistically insignificant. While SPIN is the fastest for graph classification, it is important to note that it is not applicable to applications involving simplicial complexes such as simplicial closure prediction and trajectory prediction.\nWhile accuracy is indeed a priority in many applications, the balance between accuracy and computational efficiency is an equally critical factor in real-world scenarios. SaNN offers a valuable trade-off in this context, making it a viable option, particularly for large-scale applications."}, {"Heading": "Thanks for the response", "Subheading": "Official CommentbyReviewer 7LnN21 Nov 2023, 00:07Everyone", "Content": "Comment:\nI'd like to thank the authors for addressing my questions and providing pre-computation times which are indeed not significant. I am maintaining my score."}]}, {"Heading": "Official Review of Submission9491 by Reviewer LALK", "Subheading": "Official ReviewbyReviewer LALK30 Oct 2023, 15:08 (modified: 23 Nov 2023, 11:15)EveryoneRevisions", "Content": "Summary:\nThe authors present a Simplicial Graph Neural Network, which considers higher-order structures in the input graphs. In comparison to previous work, the features from k-simplices are precomputed without trainable parameters and only then fed into a GNN. This leads to lower runtime during training since features can be reused in each epoch, which is validated by the authors theoretically and empirically.\nThe authors prove that their method is more powerful than the WL test and as powerful as the Simplicial WL (SWL) test, when it comes to distinguishing non-isomorphic subgraphs. Further, they prove permutation equivariance, orientation equivariance, and simplicial-awareness.\nThe method is evaluated on trajectory prediction, simplicial closure prediction, and graph classification, where it is on par/slightly outperforms previous works with better training runtimes.\nSoundness:\n4 excellent\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThe goal of the work, achieving better scalability of expressive networks by using non-parametric simplicial encoders makes sense.\nThe authors thoroughly analyze their method theoretically and provide proofs for all relevant properties.\nThe presented method seems to find a good trade-off between expressiveness, runtime and empirical quality.\nThere is theoretical value in the non-parametric encoder for simplices that keeps equivariant properties and simplicial-awareness\nThe paper is mostly well written\nWeaknesses:\nRuntime and asymptotic comparisons in this work are done by excluding the precomputation of features. I think this is misleading, since in practice, the precomputation is certainly part of the computation, especially during inference. Thus, the presented gains seem to be only valid during training, when the features need to be computed only once for many iterations of training.\nAt the same time, the method only performs on par with previous work, with small gains on some datasets.\nThe method requires many hyper parameter choices like hops, T, k, which seem to have different optimal settings on different datasets. The result quality differs substantially depending on the configuration.\nI am skeptical regarding the practical relevance of the presented method due to above reasons.\nThe method lacks conceptual novelty. The main idea of precomputing features by non-learnable functions has been seen in other areas, e.g. non-parametric GNNs. The general structure of the work follows a long line of work about GNN expressiveness (higher order and WL-level) without presenting many novel insights.\nQuestions:\nI wonder how the method compares to previous methods in inference runtime, when feature precomputation needs to be included.\nI thank the authors for proving the precomputation times explicitly and for replying to other concerns I have - this is certainly helpful to evaluate the differences to previous work.\nIn general, I am still on the fence and still doubt the significance of the contribution. However, I acknowledge that other reviewers find value in it and, thus, slightly raise my score. I am not opposing this paper to be accepted, as I think it is a thorough and well executed work.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors20 Nov 2023, 02:20Everyone", "Content": "Comment:\nWe sincerely appreciate your positive feedback on our work's objectives, theoretical analysis, trade-off balance, the value of our non-parametric encoder, and the overall quality of our writing."}, {"Heading": "Addressing the Inclusion of Precomputation Times in Runtime Comparisons", "Subheading": "Official CommentbyAuthors20 Nov 2023, 02:29 (modified: 20 Nov 2023, 03:28)EveryoneRevisions", "Content": "Comment:\nWe appreciate your feedback regarding the inclusion of precomputation times in our overall computation assessment. While we agree that precomputation is indeed part of the overall process, it generally constitutes a relatively insignificant portion compared to the cumulative time spent on training and testing.\nIt is important to note that our method requires precomputation of features just once before the training begins. On the other hand, conventional SNNs require this computation for every layer, where the number of layers correspond to the neighborhood depth from which features are aggregated, and for each epoch and across all orders of simplices. In Appendix B, we do provide a theoretical comparison of overall computational complexities of SaNN and the existing SNN models. The overall time complexity of the existing SNN models is about $\\mathcal{O}\\left(T\\left(\\left(2 N_k^2 D_k+N_k N_{k-1} D_{k-1}+N_k N_{k+1} D_{k+1}\\right)+\\left(3 N_k D_k^2+N_k D_{k-1}^2+N_k D_{k+1}^2\\right)\\right)\\right)$, which is dominated by the first term containing $2 N_k^2 D_k, N_k N_{k-1} D_{k-1}$, and $N_k N_{k+1} D_{k+1}$, corresponding to feature aggregation in every layer. Since feature aggregation happens only once before training as a pecomputation step, an SaNN model capturing information from $0, \\ldots, T$-hop neighborhood of $k$-simplices has a time complexity of $\\mathcal{O}\\left(T\\left(3 N_k D_k^2+N_k D_{k-1}^2+N_k D_{k+1}^2\\right)\\right)$, which is negligibly small when compared to the computational complexity of the existing SNN models.\nWe acknowledge that the time savings during inference may not be as pronounced as those during training, but we assert that the substantial enhancements during the training phase are quite significant, given that training time is the primary contributor to overall computation time, which is the sum of precomputation time, training time and inference time. \nNonetheless, in response to your suggestion, we have incorporated precomputation times into our analysis   to provide a more comprehensive view of our method's performance. We present here the precomputation and per epoch training time values of SaNN and the per epoch training time values of baseline SNN models for the three tasks at hand. The first terms in the runtime values of SaNN correspond to the precomputation times and the second terms to the per epoch training times.\nTrajectory prediction:\nDataset\nOcean\nSynthetic\nPlanar\nMesh\nScoNe\n0.4\n3.2\n30.2\n18.9\nSCNN\n1.9\n4.2\n36.3\n29.8\nSaNN\n0.01,0.1\n0.06,2.5\n1.22,26.8\n0.39,8.1\nSimplicial closure prediction:\nDataset\nEnron\nHigh-school\nPrimary-school\nNDC-classes\nMath-sx\nMPSN\n255\n413\n3499\n-\n-\nSCNN\n17\n401\n1891\n-\n-\nSaNN\n0.01, 3\n0.05, 112\n0.76, 916\n0.26, 13\n95.91, 52883\nGraph classification:\nDataset\nProteins\nNCI1\nIMDB-B\nReddit-B\nReddit-M\nMPSN\n33\n292\n46\n242\n1119\nSaNN\n3.6, 0.4\n6, 58\n4, 8\n6, 45\n34, 104\nThe values provided represent the training time per epoch. Out-of-memory results are indicated by \u2212. The total training time for SaNN is calculated as follows:\nTotal training time = Precomputation time + (Number of epochs * Training time per epoch). This total is substantially less than the precomputation times. Furthermore, it is also worth noting that for any neural network model, the larger computational burden lies in the training phase rather than the inference phase. In our study, we have demonstrated both theoretically and experimentally that our training times are significantly reduced compared to other models. Therefore, the overall computational efficiency of our method remains superior."}, {"Heading": "Addressing the Comparative Performance of SaNN", "Subheading": "Official CommentbyAuthors20 Nov 2023, 02:32Everyone", "Content": "Comment:\nIt is indeed correct that SaNN demonstrates comparable, and occasionally superior, performance to the baselines on some datasets. However, the real strength of SaNN lies in its significant training time improvements. This means that while maintaining competitive accuracy, SaNN offers substantial computational gains, which is a crucial factor in practical applications. Table 2 illustrates that existing SNN models often encounter out of memory errors when handling very large datasets. In contrast, our proposed SaNN model not only significantly reduces computational time, but also delivers performance that is consistently competitive with, and at times surpasses, that of the existing SNN models."}, {"Heading": "Addressing Concerns on Hyperparameter Choices", "Subheading": "Official CommentbyAuthors20 Nov 2023, 02:36Everyone", "Content": "Comment:\nWe acknowledge your concern about the numerous hyperparameters in our method, such as depth $(T)$, order of simplicial complex $(K)$, and hops. It is true that optimal settings can vary across different datasets, potentially impacting the quality of results.\nHowever, it is important to note that hyperparameters like depth $T$ and order $K$ are not unique to our model, but are intrinsic to any model dealing with simplicial complexes. To understand the impact of information from different depths and simplices of varying orders, we conducted an ablation study. Our findings from this study are detailed in Appendix J. We have also summarized key insights from this analysis in Section 5 under \"Additional Insights.\" We believe these observations can guide the selection of hyperparameters for different applications."}, {"Heading": "Addressing Concerns on Practical Relevance", "Subheading": "Official CommentbyAuthors20 Nov 2023, 02:38Everyone", "Content": "Comment:\nRegarding the concern of practical relevance of our method, we want to highlight that one of the main goals of the paper is to have a simpler model of practical value that can be theoretically characterized. Reduced training time not only enables more frequent model retraining, catering to real-time applications, but also offers scalability when dealing with larger datasets. While inference time is indeed crucial, the ability to train models more efficiently is equally significant in real-world machine learning applications. As indicated in Table 2, existing SNN models struggle with training on very large datasets. However, our SaNN model, despite its significantly lower computational time, achieves performance that is competitive with, and occasionally even superior to, existing SNN models. This balance between computational efficiency and performance highlights the practical value of SaNN, particularly in large-scale applications.\nIn terms of practicality in hyperparameter selection, our ablation study provides valuable insights. We found that local neighborhood information is vital across all tasks, while information from larger hops seems less critical. For simplicial closure prediction, our study revealed that the constituent edges of open triangles carry the most crucial information about whether a simplex will be formed. This agrees with the observation made in Benson et al. (2018), which states that the tie strengths of the edges in an open triangle positively impact its simplicial closure probability. We believe insights such as these offer practical guidance for choosing hyperparameters in different applications."}, {"Heading": "Addressing Concerns about the Novelty of SaNN", "Subheading": "Official CommentbyAuthors20 Nov 2023, 02:41Everyone", "Content": "Comment:\nIndeed, our work draws inspiration from graph-based methods such as GAMLPs, SPIN, and SIGN, which utilize the idea of precomputing features using non-learnable functions. Nevertheless, the definition of a neighborhood in SaNN is fundamentally different as discussed in the paper.\nA straightforward extension of the precomputation step in GAMLPs, SPIN, or SIGN to simplicial complexes would involve precomputing the features of simplices using integer powers of the Hodge Laplacian matrix. However, this approach fails to account for the information embedded in the boundary and co-boundary adjacent simplices, which, as we demonstrate in our work, is crucial for proposing an efficient model that matches the power of the SWL test.\nOur work presents a novel recursive aggregation approach, which, subject to the selection of specific functions for generating embeddings, leads to node embeddings that are more expressive than those produced by traditional GNN models. Moreover, we offer theoretical evidence that SaNN, through our proposed aggregation and transformation methodology, adheres to orientation equivariance and simplicial awareness, which are characteristics specific to simplicial complex-based models. We believe these unique attributes highlight the novelty of our method when compared to conventional graph-based approaches."}]}, {"Heading": "Official Review of Submission9491 by Reviewer 4crX", "Subheading": "Official ReviewbyReviewer 4crX29 Oct 2023, 22:33 (modified: 21 Nov 2023, 19:27)EveryoneRevisions", "Content": "Summary:\nThis paper considers the design of neural networks for simplicial complexes, which are more general combinatorial structures than graphs, but less general than hypergraphs. The authors propose to use multihop aggregation schemes to build an architecture that is more expressive than the simplicial Weisfeiler-Lehman isomorphism test, while satisfying useful invariance, equivariance, and expressivity properties. They also demonstrate the efficientcy of their proposed method, and its performance for a few different tasks in simplicial data processing.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nFor the most part, this paper is well-written, and is easy to digest for someone who is familiar with graph neural networks. I don't think the intended audience of this paper includes someone not familiar with GNNs, but this is fine in my opinion.\nThe proposed method is demonstrated to be quite efficient in comparison to existing ones, with similar performance as well.\nWeaknesses:\nCertain definitions regarding the types of operators and features are not laid out clearly enough, which leads to ambiguity in the paper on a technical level. As noted in the list of questions and suggestions, the claimed properties of the proposed models are not clearly true, possibly due to this misunderstanding.\nQuestions:\nMy most important concern is summarized in point 1 -- in particular, the ambiguities around orientation equivariance and the use of oriented operators built from the incidence matrices are what cause me to suggest this paper be rejected. If the authors are to focus on either of the two points in order to change my mind on this paper, it should be the first one.\nThere are some details missing regarding the type of data being handled. In particular, the incidence matrices are not defined in a way sufficient for the discussion following in the paper. Normally, the incidence matrices have values of +-1 depending on a chosen reference orientation (usually given by some ordering of the nodes). Coupled to this, the signs of the features on the simplices are determined relative to the same reference orientation -- this gives meaning to the notion of orientation equivariance. Without discussing these things, orientation equivariance is not a meaningful concept within the context of the paper.\na. This calls into question the validity of the example in Section 4.1. You say that all simplices are given a feature value given by some scalar $a$ -- yet, the matrices acting on these feature vectors/matrices have an orientation associated to them. It seems as if you are using an\noriented operator\nto act on\nunoriented features\n. Property 1 in this example is thus difficult to claim, as the property of orientation equivariance is one describing the action of\noriented operators\nacting on\noriented features\n, and how the choice of orientation to begin with is irrelevant to the computation.\nb. Furthermore, this problem yields a comparison for isomorphism testing incorrect, as the erroneous imposition of differently-oriented features relative to the chosen orientations could be used by SaNN to yield a \"false negative,\" i.e., saying that two isomorphic complexes are different.\nc. A more minor comment in this direction comes from the\nInsights\nsection of Section 5.1. It is not correct to say that \"the superior performance of SaNN also proves the orientation equivariance of SaNN experimentally.\" Orientation equivariance is a simple mathematical property, and does not guarantee good performance, nor are all performant architectures on a given dataset orientation equivariant. These properties are possibly linked, but the claim that one proves the other in some way is not justified.\nd. Moreover, based on my reading of the appendix, many of their experimental setups for tasks other than trajectory prediction use \"unoriented data\" by simply assigning scalar values to high-order simplices, which is again incompatible with the use of oriented operators. Perhaps something in the implementation of SaNN in these examples does not use oriented operators such as the incidence matrices, but this is not clear to me.\nPlease either justify, clarify, or revise the paper's discussion regarding orientation equivariance.\nRelated to the above point, the claims in Section 4.2 seem reasonable at first glance, but are not explained well enough. Permutation equivariance is easily seen to hold, so is not much of a concern. Orientation equivariance is subject to the problems noted above, so more clarification on the type of simplicial features and relevant operators needs to be made. That is not to say that the result proved in the appendix is wrong, but it needs to be clarified in order to be understood in a way that acts on oriented features. Simplicial awareness is more subtle than the other two, based on the definition from (Roddenberry et. al., 2021). For instance, some of the existing convolutional-type SNNs in the literature fail to satisfy simplicial awareness if they are implemented without nonlinear activation functions, due to the fact that the square of the (co)boundary operator is the zero operator. Perhaps it is the case that the assumptions of Theorem 4.2 are sufficient to exclude such methods, but a clearer connection is needed. It would be very helpful for the authors to briefly survey some of the methods they compare to, and clarify whether Theorems 4.1 and 4.2 apply or don't apply to them.\nThank you for addressing my questions -- I have raised my suggested score.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors20 Nov 2023, 02:43Everyone", "Content": "Comment:\nWe sincerely appreciate your positive feedback and constructive remarks on our paper. We acknowledge that the unclear definitions may have led to some confusion regarding the properties of the proposed models. Upon reflection, we agree that the orientations of signals and simplices were not sufficiently defined, potentially leading to misunderstandings. We have made revisions to the paper, as detailed below, to enhance clarity and readability."}, {"Heading": "Clarifying the Orientations of Simplices and Features", "Subheading": "Official CommentbyAuthors20 Nov 2023, 02:51 (modified: 20 Nov 2023, 02:52)EveryoneRevisions", "Content": "Comment:\nWe appreciate your feedback and recognize that our initial description, which stated \"The $(i, j)$th entry of $\\mathbf{B}_k$ is non-zero if the $i$th $(k\u22121)$-simplex is a boundary simplex of the $j$th $k$-simplex\", could have been clearer.\nWe have clarified in the revised manuscript that the $(i,j)$th entry of $\\mathbf{B}_k$ can be either $+1$ or $-1$, depending on the relative orientations of the $i$th $(k\u22121)$-simplex and the $j$th $k$-simplex. Even though in the introduction we mentioned about orientations of higher-order simplices as \u201cHigher-order simplices are usually oriented, ensuring a consistent node arrangement within each simplex, facilitating tasks like determining information flow directions along the edges.\u201d, we did not formally define the orientations of features on the simplicial complex in relation to the reference node ordering. Therefore, we have now specified in the revised manuscript that signs of features on simplices are determined based on the reference node arrangement within each simplex. Additionally, when features are unoriented, we clarify that there is no need for a reference orientation of simplices. In such cases, we consider unoriented incidence matrices.\nTo this end, we have incorporated the following points in Section 2 (Background) at appropriate locations:\n\"Each simplex has an orientation defined by a standardized vertex order, typically ascending or descending, establishing a standard node arrangement within each simplex.\"\n\"The non-zero entries of an oriented incidence matrix $\\mathbf{B}_k$ can be either $+1$ or $-1$, reflecting the relative orientations of the $i$th $(k\u22121)$-simplex and the $j$th $k$-simplex.\"\n\"The sign of feature $\\mathbf{X}_k[\\sigma_k]$ is determined based on the reference orientation of $\\sigma_k$.\"\n\"For unoriented features, there is no need for a reference orientation of simplices. In such cases, we consider unoriented incidence matrices. Specifically, the $(i, j)$th entry of an unoriented incidence matrix  $\\mathbf{B}_k$ is $1$ if the $i$th $(k\u22121)$-simplex is a boundary simplex of the $j$th $k$-simplex, and $0$ otherwise.\"\nWe believe that these revisions will enhance the comprehension of the orientation equivariance concept within our paper."}, {"Heading": "On Validity of the Example in Section 4.1", "Subheading": "Official CommentbyAuthors20 Nov 2023, 02:58Everyone", "Content": "Comment:\nWe appreciate your feedback regarding Section 4.1. In Section 4.1, we consider unoriented simplices. The details are as follows.\nIn Section 4.1, we explore example functions that adhere to the conditions in Theorem 4.2, demonstrating that SaNN is as powerful as the SWL test. It is important to note that the SWL test differentiates non-isomorphic simplicial complexes based on structure, not the features they carry as in the commonly used WL test. Consequently, it assumes uniform initial features (colors) across all simplices, which are then updated iteratively [Please refer to Section 3 in Bodnar et al., 2021].\nOur goal in showing equivalence to the SWL test is to demonstrate that SaNN assigns distinct embeddings to two non-isomorphic simplicial complexes, irrespective of their features, if the SWL test assigns different colors to their structures. In this context, we consider unoriented uniform features across all simplices, thus dealing with unoriented simplicial complexes.\nHowever, SaNN is also capable of handling oriented features in practice. For instance, in trajectory prediction where flow has orientation, we consider oriented simplicial complexes. In such scenarios, we can also establish orientation equivariance as detailed in the paper, hence validating Property 1. In sum, the two settings are different as highlighted by the reviewer and we have clarified this in the paper. To enhance clarity, we have slightly revised the introductory text of Section 4.1 as follows:\n\"In this section, we discuss example functions that fulfill the conditions outlined in Theorem 4 and demonstrate that SaNN is as powerful as the SWL test. The SWL test [cf. Appendix C] distinguishes non-isomorphic simplicial complexes based on structure, assuming uniform initial features (colors) across all simplices. To establish equivalence with the SWL test, we consider simplicial complexes with a uniform scalar feature $a$ on all simplices, without attributing any orientation to the features or the simplices. However, it is worth noting that SaNN can also process oriented features in practice, and in such cases, we work with oriented simplicial complexes (or incidence matrices) as defined in Section 2.\""}, {"Heading": "Clarifying SaNN's Equivalence to the SWL Test", "Subheading": "Official CommentbyAuthors20 Nov 2023, 03:05Everyone", "Content": "Comment:\nIn response to your comment, we reemphasize that isomorphism testing primarily focuses on the structure of simplicial complexes, not the features they carry. The objective is that SaNN should assign distinct embeddings to two different structures that the SWL test identifies as non-isomorphic. When establishing equivalence to the SWL test, our reference to requiring injective functions for aggregation pertains to structure, not initial features. This means that we need distinct embeddings for two unique structures, assuming all features are identical. As we theoretically prove in the paper, under the conditions stated in Theorem 4.2, starting with uniform initial features (colors) across all simplices, SaNN is as powerful as the SWL test in distinguishing the structures of non-isomorphic simplicial complexes."}, {"Heading": "Correcting the Minor Error in the Insights Section", "Subheading": "Official CommentbyAuthors20 Nov 2023, 03:09Everyone", "Content": "Comment:\nThank you for pointing it out. Indeed, you are correct. Orientation equivariance does not automatically equate to superior performance. Similarly, good performance on a specific dataset does not necessarily imply orientation equivariance. Our intention was to highlight that SaNN's good performance signifies its effective use of the orientations of flows for trajectory prediction. Therefore, we will revise the sentence as follows:\n\"The good performance of SaNN also signifies its effective use of the orientations of flows for trajectory prediction.\""}, {"Heading": "Clarification on Orientations of Simplices in Experiments.", "Subheading": "Official CommentbyAuthors20 Nov 2023, 03:14 (modified: 20 Nov 2023, 03:16)EveryoneRevisions", "Content": "Comment:\nIn our implementation of SaNN, we use oriented incidence matrices for feature aggregation when the features are oriented. On the other hand, we use unoriented incidence matrices when the features are unoriented.\nIn the numerical experiments for trajectory prediction, the input features are oriented flows on simplicial complexes, prompting us to use oriented incidence matrices for aggregation. For other experiments such as graph classification and simplicial closure prediction, no input features were given on the simplicial complexes. We constructed initial features on simplices as the sum of upper and lower adjacent simplices. As these features are unoriented, we utilized unoriented incidence matrices for feature aggregation.\nTo further clarify, we have included the following text in the respective subsections of Section 5 in the revised manuscript:\nIn the Trajectory Prediction section: \"Trajectory prediction involves predicting the next node in a sequence formed by a series of nodes connected by edges, with oriented flows on the edges. As the features are oriented, we use oriented incidence matrices for aggregation.\"\nIn the Simplicial-Closure Prediction section: \"In all simplicial closure prediction experiments, we consider the initial features on simplices as the cumulative count of their lower and upper adjacent simplices. Given that these are unoriented, we use unoriented incidence matrices for aggregation.\"\nIn the Graph Classification section: \"For all graph classification experiments, we consider the initial features on simplices as the cumulative count of their lower and upper adjacent simplices.  As these are unoriented, we use unoriented incidence matrices for aggregation.\""}, {"Heading": "On the Expressiveness and Simplicial Awarenessof the existing SNNs", "Subheading": "Official CommentbyAuthors20 Nov 2023, 03:23Everyone", "Content": "Comment:\nWe appreciate your feedback and agree that the lack of formal definitions for oriented features and missing to mention the use of unoriented incidence matrices for unoriented features might have caused confusion. We have now clarified these points and expect this to improve the understanding of the theorems and properties related to orientations.\nRegarding simplicial awareness, this property requires a model's output to be dependent on all incidence operators. All baseline methods in our paper rely on the powers of lower-adjacent matrices, upper-adjacent matrices, or Hodge Laplacian matrices. As none of these powers approach zero, all baselines inherently exhibit simplicial awareness. However, excluding MPSN, the other baselines do not match the expressivity of the SWL test, as they do not account for feature aggregation from boundary and coboundary simplices, while the SWL test does. Furthermore, since the baselines other than MPSN do not consider aggregation from coboundary simplices, the node embeddings from these models cannot be shown to be more expressive than those from the WL test.\nWe will supplement Appendix A with the following text for further clarification:\n\"All baseline methods, including \\texttt{MPSN}, \\texttt{SCNN}, \\texttt{SCoNe}, and \\texttt{S2CNN}, exhibit simplicial awareness as they utilize the powers of lower-adjacent matrices, upper-adjacent matrices, or Hodge Laplacian matrices, none of which approach zero. However, except for \\texttt{MPSN}, these baselines do not match the SWL test in terms of expressivity, as they do not account for feature aggregation from boundary and coboundary simplices, which is considered by the SWL test. Furthermore, since the baselines other than MPSN do not consider aggregation from coboundary simplices, the node embeddings from these models cannot be shown to be more expressive than those from the WL test. We summarize the properties of \\texttt{SaNN} and the existing SNN models as follows:\nMethod\nAggregations\nMore expressive than WL\nAs expressive as SWL\nSimplicial-awareness\nSaNN\nPrecomputed\nYes\nYes\nYes\nMPSN\nSequential\nYes\nYes\nYes\nSCoNe\nSequential\nNo\nNo\nYes\nSCNN\nSequential\nNo\nNo\nYes"}, {"Heading": "Addressing Feedback and Revisions Made", "Subheading": "Official CommentbyAuthors20 Nov 2023, 03:25Everyone", "Content": "Comment:\nWe appreciate your insightful feedback regarding the clarity of orientations in our work. In our revised manuscript, we have provided comprehensive clarifications on this aspect. We believe that these modifications, along with the substantial results presented in the paper, strengthen the overall quality of the manuscript. We hope you find these revisions address your concerns."}]}]}, "mnyXZBa5dP": {"paper_info": {"Primary Area": "applications to neuroscience & cognitive science", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Image Manipulation Detection, Cascade Networks, Eye-tracking  Data, Model Stability", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "TL;DR": "Optimization of a classifier based on human eye tracking to determine whether an image has been manipulated.", "Abstract": "In the digital age, determining the authenticity of images has become \nincreasingly crucial. This study aims to explore the capability of machine \nlearning models in identifying manipulated images using eye movement data and \ncompares this with human judgment. We collected a series of both manipulated \nand unaltered images and conducted eye-tracking experiments on a set of \nparticipants. After data preprocessing, various machine learning models were \ntrained and validated, including a simple classifier, cascade-optimized classifier, \nand models integrating attention mechanisms with ResNet architectures. Results \nindicate that all models outperformed the baseline set by human judgment. \nSpecifically, the Attention-ResNet model achieved the highest accuracy at 0.685, \nmaking it the top-performing model. Our analysis delves further into the stability, \ngeneralization capabilities, and practical value of these models. Ultimately, this \nresearch underscores the immense potential of deep learning strategies in \nverifying image authenticity, providing valuable insights for future research and \napplications.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9489", "PDF Url": "https://openreview.net/pdf?id=mnyXZBa5dP"}, "review_info": []}, "fMX07g3prp": {"paper_info": {"Keywords": "Neural Architecture Search, Performance Predictor, Graph Neural Network", "TL;DR": "We introduce a novel GNN predictor for NAS that efficiently combines conventional and inverse graph representations, demonstrating improved accuracy up to 16% over leading predictors on benchmark datasets.", "Abstract": "Neural Architecture Search (NAS) has risen to prominence as a pivotal tool for identifying optimal configurations for deep neural networks suited to particular tasks. However, the process of training and assessing numerous architectures introduces considerable computational overhead. One approach to mitigate this is through performance predictors, which offer a means to estimate an architecture's potential without exhaustive training. Given that neural architectures fundamentally resemble directed acyclic graphs (DAGs), graph neural networks (GNNs) become an apparent choice for such predictive tasks. Nevertheless, the scarcity of training data can impact the precision of GNN-based predictors.\nTo address this, we introduce a novel GNN predictor for NAS. This predictor renders neural architectures into vector representations by combining both the conventional and inverse graph views. Additionally, we incorporate a tailored feature loss within the GNN predictor to ensure efficient utilization of both types of representations.\nWe subsequently assess our method's efficacy through experiments on benchmark datasets including NASBench-101, NASBench-201, and the DARTS search space, with a training data range of 50 to 400 samples. The results demonstrated a notable performance improvement, achieving an enhancement of 3%-16% in terms of prediction accuracy when compared to state-of-the-art GNN predictors across the board.\nThe source code will be made publicly available.", "Primary Area": "general machine learning (i.e., none of the above)", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9483", "PDF Url": "https://openreview.net/pdf?id=fMX07g3prp"}, "review_info": [{"Heading": "Official Review of Submission9483 by Reviewer wK9q", "Subheading": "Official ReviewbyReviewer wK9q10 Nov 2023, 03:03 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposes a new GNN performance predictor for NAS that considers the forward and reverse computational graph of architectures. Furthermore, the authors also propose a loss function that minimizes the variance between the dual encodings of the forward and backward pass. Experiments in standard tabular and surrogate benchmarks show improvements NPNAS and NPENAS.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\n- The paper presents a simple and effective way to improve the predictive performance of GNNs for NAS. The empirical evaluation demonstrates that the performance increases with the number of datapoints, which is nice to see.\n\n- Easy to read and clearly written.\n\n- Compared to NPNAS and NPENAS, the proposed algorithm shows significant improvements.\nWeaknesses:\n- The proposed method to encode both the forward and backwards encoding is well-known in literature (see section 3.4 in [1] for instance) as well as in NAS [2]. The linear scalarization of the prediction loss with the loss term that minimizes the variance between the two encoders is trivial.\n\n- The authors evaluate their method on 3 tabular/surrogate benchmarks. I think this is not enough considering the diversity of available NAS benchmarks out there. There are more interesting NAS benchmarks (see NAS-Bench-Suite [3]) that also have evaluated NPENAS, and therefore makes the comparison to the proposed method possible.\n\n- No code available at submission time.\n\n\n**References**\n\n[1] https://arxiv.org/pdf/1904.11088.pdf\n\n[2] https://arxiv.org/pdf/2010.04683.pdf\n\n[3] https://arxiv.org/pdf/2201.13396.pdf\nQuestions:\n- Can the authors evaluate their method on the same framework as used in [4]? It would be great to see how FR-NAS performs under the same settings as those methods are evaluated.\n\n- What is the performance of the predictor inside a NAS algorithm? Can you evaluate FR-NAS as done in NAS-Bench-Suite (see Table 2)?\n\n**References**\n\n[4] https://arxiv.org/pdf/2104.01177.pdf\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9483 by Reviewer 6p2a", "Subheading": "Official ReviewbyReviewer 6p2a31 Oct 2023, 01:49 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposes FR-NAS, a neural architecture performance predictors that estimates performance using both the forward-pass and backwards-pass representation of a NAS architecture. FR-NAS uses an Instance Relation Graph (IRG) loss to train the dual encoder. The author's evaluate the method on three NAS-Benchmarks and compare to known predictors NPENAS and NPNAS, outperforming both.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n1 poor\nStrengths:\nThere is some novelty to considering the backwards pass representation of a NAS architecture when making a prediction. \nThe evaluation shows that FR-NAS conclusively defeats NPNAS and NPENAS on NAS-Bench-{101, 201, 301} at every training dataset size.\nThere are additional ablation studies for some components.\nWeaknesses:\nThe novelty of this work is somewhat limited as its really only using a dual encoder with adjacency matrix transpose, while components like the IGR loss are heavily borrowed from different work.\n\nThis work only considers experiments on cell-based NAS Benchmarks but not on real NAS problems, which are outperformed by macro-based NAS structures like Once-for-All/MobileNets/EfficientNets. Also, no search is applied and no found architectures are evaluated. \n\nThere are probably simpler ways to consider the backwards-pass representation of a NAS DAG which this paper does not consider. The trivial method would be to simply cast the DAG as a fully-directed graph (for every edge (i, j), add edge (j, i)) and still use a simpler encoder. Another way, also simpler than this would be to consider weighted edges, e.g., forward-pass edges have weight '1', backwards-pass have weight '-1', and you use torch.nn.GINEConv instead of torch.nn.GINConv. \n\nThe IGR loss in this paper is somewhat counter-intuitive. The intuition behind considering the transposed adjacency matrix is that you are providing the predictor with new information not found in the forward-pass adj. matrix. This information should allow the predictor to learn a better understanding of architecture performance, which would help performance. Under this assumption, you would expect the encodings of the forward encode and backwards encoder to probably be different as they should be learning on distinct information, and that the concatenation of that information (graph embeddings from each encoder) benefits predictor performance. Instead, the IGR loss is counter to this as it forces both the forward/backwards encoders to 'learn the same thing' using different views of the same data. In other words, in Figs 2-3, showing how the IRG matrix values goes down with the addition of the loss and more samples seems counter-intuitive.\n\nAnalysis in Figures 2 and 3 is missing NAS-Bench-201 using the IGR loss, NAS-Bench-301 400 samples with the loss, even though the manuscript is not even 9 pages.\n\nAuthor's mention Graph Attention Networks (GAT) a few times in the paper, but do not use them to perform any analysis on their encoders, e.g., highlighting the nodes/edges assigned high attention scores. This would be a good way to highlight how their method learns and the benefits of their design, and rebut the hypothesis that FR-NAS outperforms NPNAS and NPENAS simply because the predictor has more parameters.\n\nThere are a lot of missing entries in the related work section. Some of which should be added, and compared to, e.g.,\n- TNASP [1] and PINAT [2] deal with special encodings for the adjacency matrix, like this paper.\n- CDP [3] is a cross-domain predictor which cases 201 and 301 to be like 101, to deal with limited target data like this paper.\n- GENNAPE [4] also deal with limited target data like CDP, but they also utilize a robust form of Computational Graph that covers the entire architecture, not just the NAS cell design.\n- Multi-Predict [5] show how to leverage other information like Zero-Cost Proxies [6] and device latency/FLOPs to aid prediction - both of which this paper does not acknowledge, yet it is a critical concern of NAS.\n\nFor the above reasons I would recommend rejection of this manuscript. \n\nReferences:\n\n[1] Lu et al., \"TNASP: A Transformer-based NAS Predictor with a Self-Evolution Framework\", in NeurIPS 2021.\n\n[2] Lu et al., \"PINAT: A Permutation INvariance Augmented Transformer for NAS Predictor\", in AAAI-23.\n\n[3] Liu et al., \"Bridge the Gap Between Architecture Spaces via a Cross-Domain Predictor\", in NeurIPS 2022.\n\n[4] Mills et al., \"GENNAPE: Towards Generalized Neural Architecture Performance Estimators\", in AAAI-23.\n\n[5] Akhauri and Abdelfattah, \"Multi-Predict: Few Shot Predictors For Efficient Neural Architecture Search\", in AutoML Conf 2023.\n\n[6] Abdelfattah et al., \"Zero-Cost Proxies for Lightweight NAS\", in ICLR 2021.\nQuestions:\nNot a question but minor nitpick: Eq. 2 should be Enc(A_{T}, O; W_2)\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9483 by Reviewer NESX", "Subheading": "Official ReviewbyReviewer NESX30 Oct 2023, 11:04 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe FR-NAS paper devised a new graph neural network (GNN) based surrogate model for neural architecture search. The adjacency matrix is passed to a GNN which encodes the forward propagation of the neural network. Its transpose is passed to another GNN which encodes the backward propagation. These encodings are passed to their respective predictors ($p_{f}$ and $p_{r}$) and the final predictor is an average of these two. $p_{f}$ and $p_{r}$ are trained using mean squared error loss between the predicted and the true accuracies of the networks. To ensure that the forward and backward embeddings are consistent with each other, they used an additional loss to enforce that the relative distance between two architectures in the forward embedding space and the backward embedding space is similar.\n\n They report the results on NasBench-101, NasBench-201 and Darts search space. In their ablation studies, they bolster the case for using both the forward and the backward pass encodings.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\n1. Using two encoders to capture the forward and backward propagation encodings and using the IRG loss to synchronize them is novel.\n2. Their algorithm outperforms the other 2 baselines on NASBench-101, NASBench-201 and the DARTS search space.\nWeaknesses:\n1. Please compare against [1], [2] which are also GCN based predictors. \n2. It is also important to demonstrate that the surrogate model is competitive to other baselines such as those included in Neural architecture optimization (NAO) [3], BANANAS [4] and other predictors in  [5]\n3. For all the baselines, please report the time taken to train the predictors and to compute the correlations on all the 3 benchmarks.\n\n[1] BRP-NAS: Prediction-based NAS using GCNs,  Dudziak et al.\n[2] Bridging the gap between sample-based and one-shot neural architecture search with bonas, Shi et al.\n[3] Neural Architecture Optimization, Luo et al.\n[4] BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search, White et al.\n[5] How Powerful are Performance Predictors in Neural Architecture Search? White et al.\nQuestions:\n1. Can you please tabulate figure 5? Given that the NPENAS-FR and FR-NAS plots are very close to each other as the training data increases, it would be good to see the actual correlation values.\n2.Did you consider other alternatives to the feature loss?  Given that both $L_{pf}$ and $L_{pr}$ are predicting the accuracy of the same architecture, what would happen if you minimize the divergence between the outputs of $L_{pf}$ and $L_{pr}$ predictors?\n3. Given that the algorithm is trained to minimize the feature loss, it would have the least Diff(i,j) when compared to those that are trained without them. So is figure 3 a fair comparison?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9483 by Reviewer YbEc", "Subheading": "Official ReviewbyReviewer YbEc29 Oct 2023, 22:18 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposed a GNN-based performance predictor for NAS, the bidirectionality information is employed for performance improvement, and some experiments are conducted for verification.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nUsing the bidirectionality information to improve the performance of the predictor is quite interesting because almost all existing works did not recognize this point.\nWeaknesses:\nThe peer competitors used for comparison in this paper are not SOTA. This paper should not only compare the methods based on GNN but also SOTA performance predictors based on other techniques.\n\nThe experiments should also go to ImageNet, instead of only the measures for performance predictors, the final goal of which is for NAS.\nQuestions:\nSee Above\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}]}, "qBL04XXex6": {"paper_info": {"Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Large Language Models; Prompt Engineering; Boosting Mechanism;", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting, which involves providing a few chain of thought demonstrations as exemplars in prompts. Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in reasoning step selection for complex problem solving. In this paper, we present Boosting of Thoughts (BoT), an automated prompting framework for problem solving with LLMs by iteratively exploring and self-evaluating many trees of thoughts in order to acquire an ensemble of trial-and-error reasoning experiences, which will serve as a new form of prompting to solve the complex problem. Starting from a simple prompt without requiring examples, BoT iteratively explores and evaluates a large collection of reasoning steps, and more importantly, uses error analysis obtained from the LLM on them to explicitly revise prompting, which in turn enhances reasoning step generation, until a final answer is attained. Our experiments with GPT-4 and Llama2 across extensive complex mathematical problems demonstrate that BoT consistently achieves higher or comparable problem-solving rates than other advanced prompting approaches.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "Supplementary Material": "zip", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Primary Area": "generative models", "Submission Number": "9482", "PDF Url": "https://openreview.net/pdf?id=qBL04XXex6"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:53 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nAccept (poster)"}, {"Heading": "Meta Review of Submission9482 by Area Chair WrDu", "Subheading": "Meta ReviewbyArea Chair WrDu05 Dec 2023, 07:25 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThe paper introduces \"Boosting of Thoughts\" (BoT), a novel approach for problem-solving in Large Language Models (LLMs), marked by its conceptually clear framework that utilizes an iterative trial-and-error mechanism for prompt refinement. The methodology stands out for its originality, significantly advancing over existing methods like Chain of Thought (CoT) and Tree of Thoughts (ToT) in mathematical datasets. \nThe authors have addressed most of the reviewer's concerns during the rebuttal phase, enhancing the paper's clarity and depth. However, the paper does have limitations, including the lack of explicit supervised evaluation of the model-generated error assessments and scores. Additionally, the BoT methodology might incur high costs in API tokens due to its dependence on multiple iterations and frequent error analysis, an aspect that warrants further exploration to ensure fair comparison with CoT and ToT. Overall, considering the strengths and addressing the limitations, I recommend accepting this paper for ICLR.\nJustification For Why Not Higher Score:\nthe BoT methodology might incur high costs in API tokens due to its dependence on multiple iterations and frequent error analysis, an aspect that warrants further exploration to ensure fair comparison with CoT and ToT.\nJustification For Why Not Lower Score:\nThe approach looks novel and may inspire follow-up research on this."}, {"Heading": "Official Review of Submission9482 by Reviewer x4s4", "Subheading": "Official ReviewbyReviewer x4s407 Nov 2023, 03:44 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper proposes a new framework Boosting of Thoughts (BoT) with large language models (LLMs) for task-specific prompting. It provides how to construct prompts and use the trial-and-error reasoning approach to interact with the LLM to generate the final responses. The experiments show the effectiveness of the proposed method.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\n- Prompt engineering is a non-trivial task, and crafting effective prompts may require specialized training for human experts. The paper introduces an innovative framework for iterative prompting, leveraging LLM's feedback on its own reasoning, thereby reducing the need for human prompt engineering.\n\n- Addressing complex problems is crucial in LLM applications. This approach effectively demonstrates the power of prompt engineering and expands the capabilities of LLMs without the need for retraining or fine-tuning. Experiments conducted on multiple datasets show competitive performance compared to other prompting approaches.\nWeaknesses:\n- I agree that prompt engineering is crucial for LLM applications. However, it's worth noting that prompt engineering is often model-dependent, and the techniques may evolve as LLM capabilities improve. This may not offer long-term guidance for research unless it uncovers fundamental insights. This distinction is critical in differentiating academic research from practical production. Therefore, while the paper does offer valuable techniques for prompting the model and achieving good results on evaluation sets, it lacks in-depth discussion of the underlying reasons. This makes the paper better suited for application-oriented conferences rather than ICLR.\n\n- LLMs can be unstable and prone to hallucination, which could result in bad or incorrect feedback when using the Boosting of Thoughts (BoT) iterative prompting framework. Is there analysis on the impact of \"bad\" LLM feedback? Further, as the iterative produces are automatic, spurious feedback could get amplified over iterations. Some discuss may be necessary.\n\n-  Details are lacking on key components like aggregation strategies and generating edge weights for trees. More analysis or ablation studies are also helpful.\nQuestions:\n- In Section 3.2, it is not quite clear how to calculate the weights for the weighted binary tree.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Reply to Reviewer x4s4 - Part 1", "Subheading": "Official CommentbyAuthors22 Nov 2023, 10:28 (modified: 23 Nov 2023, 00:30)EveryoneRevisions", "Content": "Comment:\n__Response to W1__:  \n\nFirst, we cannot agree with the reviewer that this work does not offer long-term guidance for research and does not uncover fundamental insights. We do have offered fundamental insights for reasoning with LLMs. Traditionally, LLMs cannot solve complex mathematical problems sufficiently well. We show that there is a way to generate and select thoughts, and leverage the error analysis generated by LLM itself for these thoughts and feeding such error report back to prompt the LLM, we can progressively and accumulatively derive the right prompt required for LLM to generate the solution to complex mathematical problems. It shows the latent capability and potential of LLM can be unleashed through proper triggering from the error analysis and advice on the generated and selected reasoning steps. This process is fundamentally similar to humans who possess basic mathematical skills and are tasked to solve a math question. For solving a particular question, one sometimes must learn from not only demonstrations but also prior trials and errors to progressively discover the reasoning path. Intuitively speaking, Chain-of-Thoughts (CoT) [1] shows that demonstration via examples is conducive to problem solving. However, we point out that demonstration is not enough and also hard to obtain anyway. One must also practice and encounter errors and absorb such trial-and-errors into his/her knowledge base to solve the problem. So does an LLM. Thus, BoT offers fundamental insights and guidance on how to enable LLMs to generate effective reasoning steps for complex problem solving, essentially through retrospective error analysis, without requiring human demonstrations.\n\nWith these insights, the research of prompt engineering on inducing the reasoning capability of LLMs can focus on how to generate a compiled list of trial-and-error history (error analysis report from solving the problem) instead of introducing more human priors (examples) to the prompt like existing CoT, ToT methods would do, which we believe is both insufficient and hard to acquire. This, in turn, makes BoT an automated prompting framework because by iteratively collecting effective error analysis in the prompt without human annotations, LLMs can be guided to produce a correct reasoning chain toward problem solving. \n\nSecond, our insight is NOT model dependent---the same method can be successfully applied to several LLMs, such as gpt-3.5-turbo, gpt-4, and Llama2, according to our experiments. In particular, we utilize the gpt-3.5-turbo model to generate examples shown in the Appendix. The results still support our insights and conclusion. In Section B of the Appendix, we present more discussions on the fundamental insights of BoT. Furthermore, the base model in BoT's boosting mechanism does not have to be specific tree thought structures, such as ToT [2]. We chose ToT here for its effectiveness and simplicity. Therefore, the fundamental concept underpinning the proposed BoT's outstanding performance with an automated prompting framework lies in recognizing the fundamental cycle of thought generation, selection and error analysis to be fed back into LLM prompting as advice and guidance, which unlocks LLM's ability to solve complex problems. We propose BoT as a general trial-and-error prompting framework to enhance the reasoning capabilities of LLMs (which proves to be effective on multiple LLMs in general), rather than a model-dependent engineering trick that will fade away as models evolve.  \n\nThird, we are certainly not targeting applications or practical production related to LLMs. While most real-world LLM applications are regarding chatting with humans (i.e., ChatGPT), it is an emerging research question arising in the NeurIPS/ICLR/ICML/AAAI community to ask whether LLMs can solve complex mathematical questions (which is certainly not applications today, but simply a research question asked by the research community out of pure intellectual interests). For example, Chain of Thoughts (CoT) arises in NeurIPS 2022. To answer this fundamental question, we show the possibility of prompting LLM in a different way that no other work on Chain of Thoughts has tried out before, i.e., through a log of trial-and-error analysis, and show that this style of prompting (instead of just demonstrating human priors to LLMs) is key to unlocking LLMs' capabilities to solve complex mathematical problems. The underlying reason is that simply demonstrating to LLMs the steps to solve a math problem is not enough. We must also leverage a compiled list of trials and errors obtained from attempting to solve the problem so that the LLM progressively learns to avoid pitfalls, just like humans. Our experiments have substantiated our claim by showing that without human priors, BoT can achieve outstanding performance at solving various mathematical problems via the proposed thought generation, aggregation, and error analysis accumulated to be used for prompting LLMs."}, {"Heading": "Reply to Reviewer x4s4 - Part 2", "Subheading": "Official CommentbyAuthors22 Nov 2023, 10:29 (modified: 23 Nov 2023, 00:38)EveryoneRevisions", "Content": "Comment:\n__Response to W2__: \n\nIn Section E of the appendix, we have discussed that the spurious feedback may lead the LLMs to generate reasoning steps that are logically incorrect or do not adhere to any of the task rules. Table 7 of the Appendix presents the corresponding results of BoT when the spurious feedback is included as the experience in the prompt. However, we argue that spurious feedback will NOT be amplified over iterations; instead, thanks to the iterative mechanism of BoT, its negative impact on the generated reasoning steps can be mitigated or even entirely rectified in subsequent iterations. Specifically, as the wrong reasoning steps caused by spurious feedback contain obvious mistakes, it is likely that LLMs tend to generate correct error analysis by comparing to the final target at some later steps and provide effective suggestions for revisions. With this new experience included in the prompt, BoT is capable of generating correct thoughts (reasoning steps). This is further mitigated by the proposed thought generation process through an ensemble of binary trees and aggregating them to acquire the better chain of thoughts to be used in error analysis in each iteration.  \nAs demonstrated by the experience in Table 8, BoT produces detailed error reports and revision suggestions, resulting in a rational thought generation process illustrated in Table 7 of the Appendix. \n\nThe advantage of BoT, which leverages iterations to mitigate the detrimental effects of invalid or erroneous feedback (i.e., the LLMs progressively avoid pitfalls during the BoT process), is evident in Figure 4. Notably, the performance of BoT exhibits consistent enhancement as the number of iterations increases. This implies both the significance of accumulating trial-and-error experiences iteratively and the capacity of subsequent experiences to rectify (avoid) errors in earlier experiences.\n\n__Answer to W3 and Q1__: \n\nWe have added more details and discussions on edge weight computation for trees and the aggregation strategies in Sections C and D of the Appendix. In summary, to calculate the edge weight $V_{i-1, i}$ between two reasoning steps, represented as nodes in the tree and denoted by $z_{i-1}$ and $z_{i}$, LLMs are utilized to evaluate the entire reasoning chain $z_{1, ..., i-1, i}$, which forms a branch of the tree with $z_1$ acting as the root node. The corresponding prompt used by LLMs for this weight computation can be found in Section A of the Appendix and the source code _examples/BoostingOfThought/BoT\\_reasoner.py_.\n\n[1]. Wei, Jason, et al., Chain-of-thought prompting elicits reasoning in large language models, Advances in Neural Information Processing Systems 35 (2022): 24824-24837.\n\n[2]. Yao, Shunyu, et al., Tree of Thoughts: Deliberate Problem Solving with Large Language Models, Arxiv 2023.\n\n[3]. Besta, Maciej, et al., Graph of thoughts: Solving elaborate problems with large language models, Arxiv 2023.\n\nWe endeavour to address reviewer questions and concerns and improve the quality of this research. It is much appreciated if the reviewer could reassess the value of our contribution after reading the rebuttal."}, {"Heading": "Official Comment by Reviewer x4s4", "Subheading": "Official CommentbyReviewer x4s403 Dec 2023, 22:33 (modified: 15 Mar 2024, 00:25)EveryoneRevisions", "Content": "Comment:\nI am certain that prompt engineering is cruicial, particularly in practical applications. However, I still have some doubts about its fundamental importance for AGI. While human communication needs sophisticated and nuanced skills, we do not always employ complex skills to construct language when seeking straightforward answers, which is what we desire from LLMs. In the future, AGI should be able to respond to humans without the need for complicated prompt engineering. But, of course, this is my personal opinion, and I may not use it as the standard for evaluating the paper. Therefore, I am open to accepting the authors' response on this matter."}, {"Heading": "Official Comment by Reviewer x4s4", "Subheading": "Official CommentbyReviewer x4s403 Dec 2023, 23:04 (modified: 15 Mar 2024, 00:25)EveryoneRevisions", "Content": "Comment:\nFor responses of W2, it is questionable whether an LLM can consistantly identify its own errors during iterations. It seems likely that, due to their limited capabilities, LLMs might become entrapped in their own incorrect reasoning. For instance, when querying several LLMs, including ChatGPT and Claude, with a very simple question, \"If A>B and B<C, then what is the relationship between A and C?\", I found that except for GPT-4, all others struggled with this query. Also, I followed to the methodology described in this paper, but still found that LLMs consistently assigned a score of 1.0 to incorrect answers. However, if I modified the prompt to \"If A>B and C>B,...\", it leads most LLMs to provide correct answers without any prompt engineering. This observation suggests that LLMs may only learn language patterns rather than possessing true reasoning capabilities. Therefore, while I agree that BoT iterations can enhance performance, it is better for the authors to acknowledge that under certain conditions, the inherent limitations of LLMs might lead to failure.\n\nResponses of W3 are accepted."}]}, {"Heading": "Official Review of Submission9482 by Reviewer dFck", "Subheading": "Official ReviewbyReviewer dFck05 Nov 2023, 08:16 (modified: 04 Dec 2023, 10:00)EveryoneRevisions", "Content": "Summary:\nThe paper presents an extension of the Chain of Thought (CoT) and Tree of Thoughts (ToT) method, named Boosting of Thoughts (BoT). BoT refines the problem-solving process in large language models (LLMs). BoT harnesses error analysis to improve the LLM's problem-solving accuracy iteratively. The \"Boosting of Thoughts\" (BoT) procedure is a two-step process that first generates a diversity of reasoning paths from a Large Language Model (LLM) in the form of a weighted binary tree, enhancing problem-solving by creating a hierarchy of potential solutions. Then, it employs a novel aggregation strategy that iteratively refines and combines these paths. Through best-first and greedy aggregations, BoT selects and optimizes the most promising chain of thought, using iterative feedback to progressively improve the LLM's performance on complex problem-solving tasks. The paper reports improved performance on complex mathematical problems when tested with GPT-4 and LLAMA2, compared to CoT and ToT.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\n1. This is an innovative extension of the Chain-of-Thought (CoT) and Tree-of-Thought (ToT) methods. Compared to CoT and ToT, the author adopts the idea on leveraging error analysis to refine the LLM. This can be a limitation of CoT and ToT, as they do not conduct error analysis and more importantly, learn from errors. The motivation is intuitive and clear.\n\n2. Unlike ToT, which expands multiple reasoning tree branches, the BoT method iteratively refines a single line of thought. This focus on iteration rather than expansion allows for a more concentrated and efficient improvement of the reasoning path. The computation moves from exploring the tree into learning from erroneous trials. \n\n3. The Boosting of Thoughts (BoT) concept shows a clear advancement in problem-solving methodologies within large language models. It effectively combines generation and evaluation steps to progressively enhance reasoning, demonstrating a significant leap in the model's ability to handle complex tasks. \n\n4. The experiments are clear, the results are effective. And all experiments are classic experiments from CoT and ToT, so it is clear to compare BoT\u2019s performance over CoT and ToT.\nWeaknesses:\nThe mauscript need polished in their figures' presentation, e.g., the authors need give more detailed examples in Fig1.\nQuestions:\nQ1: In the prompt, I wonder whether the \u201cerror input\u201d are included, or only the \u201cexperience\u201d is included? From figure 1, I only see \u201cerror report\u201d like \u201cstep 1 is not closer to 24\u201d, no \u201cerror input\u201d like what is step 1, 2, 3. How the LLM know what step 1 mean, and how can LLM learn from error, if LLM does not know specific input?\nQ2: How about you consider the entire (input, error analysis) as an In-context Learning example? Then the entire method is similar to CoT, meaning that you can manually construct an exemplar consisting of (input, error analysis) pair. Then use the CoT idea to follow the strategy to generate analysis and think about the correct answer.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Reply to Reviewer dFck", "Subheading": "Official CommentbyAuthors22 Nov 2023, 10:32 (modified: 22 Nov 2023, 10:36)EveryoneRevisions", "Content": "Comment:\nThanks for your comments and we reply to them below.\n\n__Response to W1&Q1__: \n\nWe have revised and polished all Figures, especially Figure 1, in the submission. To provide further clarity, additional details about each component of BoT, as illustrated in Figure 1 and Figure 2, have been included in sections A, C, and D of the appendix. In particular, we have supplemented the appendix with additional experimental results, including the experience in Table 7 and Table 8, generated by BoT. Furthermore, we have enhanced the organization of the source code within the _examples/BoostingOfThought/_ directory to provide a more comprehensive overview of the BoT implementation. For instance, within the code file _BoT\\_commenter.py_, one can find the process for generating experience using LLMs, while _BoT\\_reasoner.py_ details how this experience is structured and incorporated into the prompt for subsequent iterations of reasoning.\n\nSpecifically, both the 'error input' and 'experience' are included in the prompt, but the 'error input' is embraced as the sub-block of 'experience'. In fact, the experience comprises input reasoning steps, the conclusion, error analysis, advice, and the confidence score, all of which are generated automatically by LLMs. As each iteration of BoT produces one such experience for the reasoning chain, LLMs can gradually generate the right answer due to the experience accumulation in the prompt over iterations. Tables 7 and 8 of the appendix provide a direct example of how experience is organized in the prompt. \n\nThe confusion arising from Figure 1 primarily stems from our intention to first emphasize the significance and effectiveness of an ensemble of trial-and-error reasoning experiences within the context of the proposed automated prompting framework BoT. To mitigate the confusion, we have incorporated additional explanations into Figure 1.\n\n__Response to Q2__:\n\nYour further suggestion to include manually constructed (input, error analysis) as an in-context learning example in the prompt, similar to the CoT approach, is highly insightful. We also contemplated this intriguing idea but had to abandon it due to three key concerns. First, relying on human priors to manually design error analysis and advice is time-consuming, inefficient, and hard to generalize well to different tasks. Second, BoT, an automated prompting framework with experience generated by LLMs, is able to achieve a competitive or even top problem-solving rate on multiple mathematical problems, as shown by experimental results. Third, embracing human knowledge in the prompt may have limited applications in real-world usage due to possible bias, misleading, and security issues brought by others."}]}, {"Heading": "Official Review of Submission9482 by Reviewer VMbH", "Subheading": "Official ReviewbyReviewer VMbH04 Nov 2023, 11:10 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper proposes a Boosting of Thoughts (BoT) framework, which aims to achieve the boosting mechanism that embraces aggregation and experience, thereby enabling the progressive refinement of unreliable reasoning steps (weak thoughts) by learning from errors to solve various problems, eventually.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThis paper reiterates their proposition that a simple prompt can be enhanced by gradually accumulating error analysis on its generated thoughts to address complex tasks.  The authors present a novel framework, the Boosting of Thoughts (BoT), to implement such progressive prompt enhancement for effective thought generation with an experience-driven iteration process. Iteratively exploring and self-evaluating the generated simplistic trees of thoughts enables a simple initial prompt to be gradually enhanced by an ensemble of trial-and-error reasoning experiences, resulting in accurate solutions. \nThis work seems like a quite comprehensive investigation with well-structured and easy to read sections.\nWeaknesses:\nThe paper is based on the motivation that starting with a simple prompt without human annotations for LLMs, BoT may get weak thoughts. However, with aggregation, BoT is capable of deriving a more logical and effective thought chain from them, thereby guiding the subsequent refinement.\n\nCould the authors expand on this statement \"Experience consistently leads to thought revision, but too much can have the opposite effect.\"? If one is looking to recreate the study, are there any guidelines or steps one could adopt to as where should be the stopping point?\n\nVery interesting findings, however Experimental results reported are limited. The authors evaluated LLM models only on a single testing procedure. However, the analysis doesn't seem concrete due to the smaller sample set considered and it would truly be insightful if the analysis was done on a larger dataset to infer results.\nFurther experiments should be performed using statistical metrics, and statistical distribution of the results should be extracted. These outcomes help better support the conclusions' claims.\nThe paper would be greatly strengthened if the proposed algorithm would outperform state-of-the-art methods\nQuestions:\nPlease review the weakness section\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Reply to Reviewer VMbH", "Subheading": "Official CommentbyAuthors22 Nov 2023, 23:09Everyone", "Content": "Comment:\nThanks for your comments and we reply to them below.\n\n__Response to Q1__: \n\nBoT is an automated prompting framework that automatically obtains and accumulates the trial-and-error reasoning experiences to enhance the prompt for LLMs toward solving complex math problems. Besides, BoT, in each iteration, adds one piece of experience to the prompt. Therefore, without being limited by how many experiences to collect and when to stop reasoning, one simply obtains the aggregated reasoning chain after the final iteration as the solution. To further clarify, we have prepared an algorithm Table showing this procedure in Section A of the Appendix.\n\nThe argument \"Experience consistently leads to thought revision, but too much can have the opposite effect\" appears in the Ablation Study part of the paper. And the main purpose is to show that 1) the effectiveness of BoT, an automated prompting framework, is attributed to the experience accumulation in the prompt; and 2) accumulating too many trial-and-error reasoning experiences while not being selective may hurt the performance of BoT. For example, the ablation study shows that without performing the thought structures aggregation but just adding error analysis on all generated reasoning chains to the prompt, i.e., BoT (No), in Table 4 of the paper, achieves the worst performance in all cases. The main reason is that the quality of experience matters to BoT as only the experience obtained from 'good' reasoning chain can benefit the reasoning generation in subsequent iterations. This means that the proposed thought structure generation and thought structure aggregation must be performed to fully explore the reasoning space and obtain a relatively good reasoning chain to be analyzed by LLMs in order to derive a better 'experience', which is then fed back into prompting toward problem solving. \n\n__Response to Q2__:\n\nOur experiments cover 4 hard benchmark math datasets, which are commonly used by existing literature. For example, the AQuA dataset consists of about $100000$ algebraic word problems with natural language rationales. Considering the hardness, diversity, and size of these included datasets, current experiments provide enough evidence to support the claim of BoT. Particularly, our work includes a more challenging 'Game of 24' task, which is not used by other related work because even the latest GPT-4 with CoT prompt obtains only $4\\%$ solving rate. In contrast, BoT with GPT-4 achieves $83.7\\%$ solving rate.\n\nTo further enhance the experiments for BoT, we have specifically added Section F in the appendix to present more detailed results of the MATH dataset, which contains 7 categories of problems and thus represents a significantly challenging benchmark for mathematical reasoning. By performing 9 methods with GPT-3.5-turbo and GPT4 on this dataset, we show that the statistical distribution of the results across 7 categories not only supports our claims in the main paper but leads to more insights. \n\nBesides, the evaluation metric of this submission focuses on the solving rate as this is the most common and even the only used core performance metric. But, as presented in the main paper and Section F of the appendix, more detailed performance analyses are also included to offer better insights. \n\nBoT does achieve the state-of-the-art in most datasets. In the mathematical reasoning datasets, as shown in Tables 1 and 2 of the main paper and Figure 5 of the appendix, BoT outperforms other state-of-the-art methods by a relatively large margin. BoT only lags behind the CSV [1] on the MATH dataset. Yet this is not a fair comparison, since CSV additionally heavily relies on the GPT-4 code interpreter, whereas BoT achieves a competitive performance as an automated prompting framework without relying on any tools such as the GPT-4 code interpreter. Under a fair comparison with other related works on the MATH dataset, as shown in Section F of the appendix, BoT consistently achieves the highest problem solving rate on different sub-categories. Furthermore, for the Game of 24, according to the results shown in Table 2, BoT is $9.7\\%$ higher than the current best method ToT [2]. Besides, what is more important is that BoT is an automated prompting framework that generates effective reasoning chains by collecting trial-and-error reasoning experiences without introducing human annotations. \n\n\n[1]. hou, Aojun, et al. Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification, Arxiv 2023.\n\n[2]. Yao, Shunyu, et al. Tree of Thoughts: Deliberate Problem Solving with Large Language Models, Arxiv 2023."}]}, {"Heading": "Official Review of Submission9482 by Reviewer pKnK", "Subheading": "Official ReviewbyReviewer pKnK31 Oct 2023, 07:39 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposes a new framework called Boosting of Thoughts (BoT) for complex problem solving with large language models. BoT aims to iteratively explore many possible trees of thoughts and learn from ineffective thoughts/errors to progressively refine the prompt and elicit effective reasoning from LLMs. It aggregates the best reasoning chains from the trees and analyzes them with the LLM to gain experience on errors and revisions. Experiments on mathematical reasoning show BoT matches or exceeds previous SOTA approaches without needing human annotations.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\n1. The paper proposes a novel framework, Boosting of Thoughts (BoT), that utilizes an iterative trial-and-error approach to refine prompting and elicit complex reasoning from LLMs. The key idea of learning from errors/ineffective thoughts is creative and mimics human problem-solving.\n2. Authors involve interesting techniques like weighted binary trees and heterogeneous growth strategies to generate diverse, shallow thought structures from a simple prompt.\n3. Evaluations on mathematical reasoning benchmarks demonstrate effectiveness of BoT. It matches or exceeds state-of-the-art methods without needing human annotated prompts. Also, the authors conduct ablation studies to further explain the mechanisms.\nWeaknesses:\n1. Although this article proposes several practical strategies, its reasoning framework remains inherently reliant on the Tree-of-thoughts model, thereby limiting its novelty. BoT's structure is restricted to binary trees. Expanding to more complex graph structures will further improve reasoning but is not explored.\n2. For analysis, the prompts used to seed BoT could introduce biases and variances. More evaluations on OOD data would be useful to assess the robustness and generalizability of the improvements.\n3. The evaluations are mainly limited to mathematical reasoning. For generality, testing BoT's performance on other domains like commonsense reasoning or symbolic reasoning is needed.\n4. In the 'Competitors' paragraph, authors mentioned incorporating CoT-SC and Complex CoT as  baselines, yet CoT-SC is not shown in the 'mathematical reasoning' part. I hold the view that comparing the proposed method with prevailing baselines like SC(5) or SC(10) will offer a more direct reflection of BoT's efficacy. If it can outperform Complexity-based SC with fewer resources, it would make the work more solid.\nQuestions:\n1. As for the statement on page 3, 'Our paper embraces ToT due to its high ability and leaves GoT and BoT for future work,' is 'BoT' a typo error here? Or you mean combining BoT method with GoT? Regardless, I believe that including GoT in the comparison would make this work more interesting and informative.\n2. In the 'Competitors' paragraph in experiments, could you clarify how many reasoning chains are sampled for Complex-CoT and PHP respectively?\n3. The study conducts experiments based on GPT-4, which can lead to substantially high experimentation costs. Have the authors considered or utilized more cost-effective options like GPT-3.5-turbo? I'm aware of the recent variability in performance of this model. However, if there are experimental results showing that GPT-3.5 combined with BoT can outperform GPT-4 with CoT/CoT-SC, it would render the study's findings more convincing.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Reply to Reviewer pKnK - Part 1", "Subheading": "Official CommentbyAuthors23 Nov 2023, 00:06Everyone", "Content": "Comment:\n__Response to W1 and Q1__: \n\nFirst, the goal of BoT is not to extend Tree-of-Thoughts reasoning. Nor does it rely on 'Tree of Thoughts' or binary trees. Rather, our contribution lies in a novel way of prompting LLMs by enlisting iteratively accumulated trial-and-error analysis, i.e., 'experiences', into the prompting for LLM in order for it to find the right solution to complex mathematical problems. Prior to this work, CoT or ToT [1] still heavily relied on manually designed examples as demonstrations or human priors as steps for reasoning. The fundamental insight of our paper is that without introducing human annotations or examples, a simple prompt can be refined over iterations by adding 'experiences', which contain the error analysis obtained from previous reasoning steps, to enhance thought generation until a final answer to the complex mathematical problem is attained. Using binary trees is just a vehicle for reasoning step (thought) exploration, and certainly, other CoT schemes can be employed instead to generate intermediate reasoning steps. Therefore, in the paper, our emphasis is on collecting an ensemble of trial-and-error reasoning experiences to be used in promoting LLMs, just like a human with a general math background, when approaching a tough math problem, still needs to tentatively tries to give some answers but learn from trial-and-errors to avoid the pitfalls until the right path to solving the problem is found. By doing so, we open up a new research direction to not extend the prompt with human priors like existing CoT work does but instead focus on how to generate effective error analysis to be fed back into LLMs so that it can find the right path to solve challenging mathematical problems. \n\nThe base thought structure of BoT is not limited to ToT, as we have emphasized in the revised related work and Section C of the Appendix. We choose ToT due to the availability of high-quality source code and its suitableness as a tool to explore the space of reasoning steps in mathematical problems. Although Graph of Thoughts (GoT) [2] can certainly be another natural choice for thought structure, we did not employ it partly because the source code of GoT became publicly available only recently. Another thing is that the GoT paper didn't report its performance on mathematical problems, while the ToT paper did report its performance on Game of 24. However, ToT itself can hardly be extended to other maths problems because of the hardness of coming up with human priors in prompts. In fact, in BoT, we do not recommend complicating the reasoning step generation through GoT or other more advanced thought structures, because we have shown through experiments that for the purpose of thought exploration, ToT, and binary trees in particular, is sufficient. The reason is that if shallow binary trees are used as the base structures, in each iteration, we can create an ensemble of simple and heterogeneous trees for thought structures to well explore the reasoning space.\n\n\n[1]. Yao, Shunyu, et al., Tree of Thoughts: Deliberate Problem Solving with Large Language Models, Arxiv 2023.\n\n[2]. Besta, Maciej, et al., Graph of thoughts: Solving elaborate problems with large language models, Arxiv 2023.\n\n__Response to W2__: \n\nBoT is an automated prompting framework, which, starting from a simple prompt (which may be invalid) with no manually-designed demonstrations, iteratively accumulates a history of trial-and-error reasoning experiences toward the right path to problem solving. That is, even if the prompts used to seed BoT could introduce biases or are simply wrong, the thought exploration (via ensembles of binary trees) and error analysis report generated by BoT will be iteratively fed back to prompt the LLM, such that collectively when trial-and-error history is used in prompting the LLM again, the LLM already learns to avoid those pitfalls (poor initial reasoning steps). Therefore, BoT is naturally robust to variation in the initial seed prompt, which does not matter to BoT. In contrast, CoT and ToT traditionally may be affected by human examples and demonstrations or human-suggested thought structures.   \n\nFor this reason, without assuming any prior knowledge of the prompt (without assuming any prior derived from any dataset), BoT is directly evaluated on the test set of each dataset. Therefore, there is no OOD issue because there was no training data; all data are assumed unseen and are test data to directly test the generalizability of BoT. OOD evaluation is also less of a concern in existing literature, e.g., ToT, PHP, Complex-CoT."}, {"Heading": "Reply to Reviewer pKnK - Part 2", "Subheading": "Official CommentbyAuthors23 Nov 2023, 00:07Everyone", "Content": "Comment:\n__Response to W3__:\n\nWe have explicitly mentioned that BoT is meant to solve complex mathematical problems with LLMs starting from the abstract. LLMs are generally believed to be able to chat and perform natural language tasks such as summarization. The specific research question asked by this work is--can LLMs solve complex mathematical problems especially without human annotations or demonstrations? This question is very challenging. \n\nTo review the experimental efforts, the experiments in our manuscript cover 4 commonly used benchmark math datasets and also introduce a more challenging task known as the 'Game of 24,' which even the latest GPT-4 struggles to solve. In the results on MATH shown in Section F of the Appendix, BoT achieves state-of-the-art performance on all categories of the test set.\n\nAll these experiments help to verify the capability of BoT to solve complex mathematical problems, which is challenging to achieve, while using BoT toward other reasoning tasks, such as symbolic reasoning, is an interesting direction for future investigation.\n\n__Response to W4, Q2__:\n\nBecause self-consistency (SC) is very resource-consuming as it generally requires sampling a large number of reasoning chains from the LLM, we only adopted CoT-SC as a baseline in the 'Game of 24'. On this task, CoT-SC (k=100), although using the majority output from the $k=100$ best samples, only achieves $9\\%$ solving rate. In contrast, BoT achieves $83.7\\%$ solving rate, which is a much higher solving rate, without even using SC. This significant gap shows that using SC is only marginally helpful for Game of 24, and the proposed BoT method without relying on SC-based sampling is sufficient to achieve state-of-the-art performance. Therefore, we did not include SC-related mechanisms in the 'mathematical reasoning' part due to its limited improvement in BoT and its huge resource consumption. Besides, existing results in Tables 1, 2, and Section F of the Appendix support our claim that BoT, an automated prompting framework without relying on human annotations, is able to outperform existing state-of-the-art methods, especially PHP+Complex-CoT, by a substantial margin in all problems. While BoT is already significantly better than the current best methods, performing resource-consuming SC-related experiments does not produce further evidence of the excellence of BoT, especially considering CoT-SC (k=100) lags behind BoT substantially in the 'Game of 24'. In fact, our proposed BoT methods offer different insights than SC. We show that there is a way to generate and select thoughts (as intermediate reasoning steps) and feed the analysis on these thoughts done by the LLM back into LLM prompting in order to find the right path to problem solving. \n\nIn the 'mathematical reasoning' task, no method, including Complex-CoT and PHP Complex-CoT, utilizes the SC ensemble mechanism. The prompt of these methods simply includes the reasoning examples derived from the Complex CoT. Especially, PHP utilizes the greedy decoding (i.e., temperature = 0). We have provided more details in Section F of the appendix. In the 'Game of 24' task, CoT-SC (k=100) takes the majority of output from $100$ reasoning chains."}, {"Heading": "Reply to Reviewer pKnK - Part 3", "Subheading": "Official CommentbyAuthors23 Nov 2023, 00:08 (modified: 23 Nov 2023, 00:45)EveryoneRevisions", "Content": "Comment:\n__Response to Q3__: \n\nFollowing the reviewer's suggestions, in Section F of the appendix, we have added more experiments on the MATH dataset during the rebuttal period. The Table below presents the solving rate ($\\%$) in all samples (Overall) and the category Precalculus for different model-method combinations. We refer the reviewer to Section F of the appendix for more details. \n\n| Methods             | Overall | Precalculus |\n|---------------------|---------|-------------|\n| GPT3.5 ComplexCoT   | 34.1    | 14.5        |\n| GPT3.5 BoT          | 40.61   | 15.5        |\n| GPT4 ComplexCoT     | 50.3    | 26.7        |\n| GPT4 PHP+ComplexCoT | 53.9    | 29.8        |\n| GPT3.5 BoT (GPT4)   | 55.8    | 27.9        |\n\n\"if there are experimental results showing that GPT-3.5 combined with BoT can outperform GPT-4 with CoT/CoT-SC, it would render the study's findings more convincing.\"\nThis is simply not possible and is not a fair comparison. On the MATH dataset, the solving rate of GPT-3.5-Turbo + BoT (GPT3.5 BoT) turns out to be not as good as GPT4 + ComplexCoT. This is not a fair comparison anyway, just because GPT4 is way more powerful than GPT3.5, which is evidenced by the fair comparison between GPT3.5 + ComplexCoT and GPT4 + ComplexCoT. This result is offered here just out of curiosity reasons. However, GPT3.5 + BoT is much better than GPT3.5 + ComplexCoT, showing BoT is a better method than ComplexCoT as a fair comparison.\n\nHowever, what is interesting is that if GPT4 is used to generate experience while GPT3.5 is used for thought generation, referred to as GPT3.5 BoT (GPT4), the solving rate for BoT increases to $55.8\\%$, which is not only $5.5\\%$ higher than GPT4 + ComplexCoT but also outperforms the current state-of-the-art GPT4 + PHP+ComplexCoT by $1.9\\%$. This is an important finding as it shows that the performance of BoT actually depends more on the quality of experiences, and even using the inferior GPT3.5 for thought generation, BoT is a powerful method and can still beat PHP+ComplexCoT using GPT4.\n\nThus, a viable resource-efficient way could be to use resource-friendly LLM, such as GPT-3.5-Turbo, for thought generation, while only using the more powerful GPT4 for error analysis to generate experiences as guidance. Such a combination can achieve a better tradeoff between performance and resource consumption, according to the Table.\n\nOverall, we are committed to addressing reviewer questions and concerns and improving the quality of this research. It is much appreciated if the reviewer could reassess the quality of this work based on the value of the research question it is addressing, its novelty and the outstanding performance in comparison to SOTA methods in the literature."}]}, {"Heading": "Official Review of Submission9482 by Reviewer kjxP", "Subheading": "Official ReviewbyReviewer kjxP29 Oct 2023, 19:29 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper looks at optimizing prompting for GPT-4 and Llama2 for solving mathematical problems. They provide an iterative strategy to prompting models for complex problems. The key challenges are SVAMP (1000 tasks), GSM8K (8500 tasks), AQUA (100 000 tasks), and MATH (12 500 tasks). The BoT, especially when enhanced with CoT, outperforms alternative methods.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nWhen we reach the limits by simply increasing language models, optimal interaction becomes increasingly interesting. Exploring new ways of pushing the models to do more complex tasks can get more value out of existing LLMs and is highly relevant. \n\nThe paper provides code that is easy-to-read (although a Readme would be a nice addition).\n\nThe method shows that BoT and CoT perform above the comparisons.\nWeaknesses:\nThe use of the term 'boosting' in the context of refining 'weak thoughts' introduces some ambiguity. In traditional machine learning, boosting involves the iterative enhancement of quantifiably weak learners. In the BoT framework, the concept of a 'weak thought' is more abstract, and its \"weakness\" is not as straightforward to measure. This led me to perceive the process more as a 'pruning of weak thoughts' rather than 'boosting' in the conventional sense. It would be beneficial for the paper to clarify how the model aggregates and refines these thoughts in the tree structure, and how the \"weakness\" of a thought is determined and improved upon.\n\nI think the paper comes across unnecessarily complicated, compared to the code the text is hard to fully grasp. The figures all depict Game of 24, adding examples from both a successful and a failed example of BoT for the other tasks would be beneficial.\n\nFor complete reproducibility and clarity, it would be beneficial to provide the full codebase, including modules like 'llmpebase\u2019s residual_tree_of_thoughts', which is referenced several times but not included.\n\nThe title suggests a general problem-solving approach using Large Language Models. However, the content is specifically focused on mathematical problems. It might be beneficial to make the domain-specific nature of the research clearer in the title or early in the abstract to set accurate expectations for readers.\nQuestions:\nDo I understand correctly that T 10 was maximum 10 prompts and M 15 consisted of 15 instances that generated binary trees that you then averaged over?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n8: accept, good paper\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Reply to Reviewer kjxP", "Subheading": "Official CommentbyAuthors22 Nov 2023, 10:34 (modified: 22 Nov 2023, 10:37)EveryoneRevisions", "Content": "Comment:\nThanks for your comments and we reply to them below. \n\n__Response to W1__: \n\nYour comments are pretty insightful because, during paper preparation, we also considered a similar concern on whether using the term 'refining weak thoughts' in the framework with the Boosting mechanism is precise. Thus, the current usage of words is supported by three reasons. First, the 'thought' is the output of LLMs, and without a proper prompt to LLMs, the generated 'thought' will be wrong in the logic and even not follow the rule of the task. Such 'weak thought' corresponding to the incorrect reasoning step will inevitably lead to wrong answers. Second, the core insight of BoT, an automated prompting framework, is that a strong prompt for LLMs toward problem solving can derive from gradually collecting an ensemble of trial-and-error reasoning experience. Thus, the iterative nature of boosting in BoT allows LLMs to learn from mistakes in the prompt, continually refining the 'weak thought' in the reasoning process. Third, as BoT starts from a simple prompt for LLMs, the generated 'thought' in the first iteration will be inherently 'weak', as measured by the low success rate in Figure 4 of the paper. Only by incorporating error analysis as part of the experience to enrich the prompt over iterations can the 'weak thought' be refined gradually to lead to the correct answer. \n\nYour suggestion on using 'pruning of weak thoughts' is also attractive. But, the terms with 'pruning', such as 'model pruning', may cause the reader to think that the 'weak thoughts' are powerful enough but only complex in structure, and thus, they should be pruned. This may not follow the core insight of BoT. \n\nTo further reduce ambiguity, we have included more detailed discussions on tree thought structures and the aggregation of thought structures in Sections C and D, accompanied by some examples from BoT in the appendix.\n\n__Response to the code and example concerns__: \n\nThe _llmpebase_ codebase is a unified platform we developed for performing prompt engineering on large language models (LLMs), with a focus on easy-to-use and fairness comparison guarantees. Thanks to the interest from reviewers, we have released the most code (still a part of our whole project, but it is enough to Run the BoT) with a clean and neat structure, which can be accessed in the 'code/' of the supplementary. To facilitate the quick experiment running with _llmpebase_, we have prepared a README\\.md file under the 'code/' to allow users to see the codebase structure and our proposed BoT (under examples/BoostingOfThought) within several minutes. \n\nApart from experiments on Game of 24, we also presented detailed results from other mathematical tasks in Table 1, Figure 3, and 4. We emphasized the performance on Game of 24 because BoT outperforms ChapGPT4 by a large margin on this challenging task. In response to your suggestion of including more success and failure cases from other tasks, we have expanded the content in the appendix. Given that BoT accumulates experience in the prompt over multiple iterations, this extension of content has resulted in a lengthier appendix.\n\n__Response to the concern on the research domain__:\n\nWe did mention that our experiments are performed across extensive complex mathematical problems. To further clarify the contribution, we will enhance the domain-specific nature of this submission. However, due to the policy of the ICLR24 conference, such a minor revision on the title or abstract can only be made for the camera-ready version. We will certainly make the suggested revision when we have the opportunity to prepare the camera-ready version.\n\n__Answer to Q1__: \n\nYour basic understanding of $T=10$ is correct but not very precise. BoT is an automated prompting framework with a boosting mechanism. Thus, starting from a simple prompt without human annotations, BoT gradually accumulates an ensemble of trial-and-error reasoning experiences to enhance the prompt over iterations. In the experimental settings, we denote the number of iterations used by BoT as $T=10$, indicating that a total of 10 experiences will be incorporated into the prompt for problem-solving. Therefore, the initial simple prompt will be updated 10 times, leading to a maximum of 10 prompts. \n\nYour description of $M=15$ is accurate. In each iteration, BoT generates numerous binary trees of thoughts to explore various reasoning steps, which are subsequently aggregated for self-evaluation. The number of binary trees in the experimental settings is set to 15.\n\nTo enhance clarity, we have included an algorithm table for BoT in Section A of the appendix, offering a concise overview of this framework."}]}]}, "H9DYMIpz9c": {"paper_info": {"Primary Area": "transfer learning, meta learning, and lifelong learning", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Data Distillation, Meta Learning, Recommender Systems, Language Modeling", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "TL;DR": "We propose an efficient data distillation method for autoregressive machine learning tasks that achieves up to 120% downstream performance when training on synthetic data with as little as 0.1% of the original dataset size.", "Abstract": "We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences \u2014 Farzi Data \u2014 which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 \u2212 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9481", "PDF Url": "https://openreview.net/pdf?id=H9DYMIpz9c"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9481 by Area Chair hkYR", "Subheading": "Meta ReviewbyArea Chair hkYR16 Dec 2023, 17:02 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThis paper proposes a new algorithm for distilling sequential datasets into smaller synthetic sequences. The goal is to improve and accelerate the training of autoregressive generative models by converting the original data into a small, structured 3d summary tensor that mixes the interactions across different sequences in a compressed form. The approach follows meta-learning with the outer loop updates the summary tensor and the inner loop optimizes downstream model performance on a specific data summary. While similar ideas have been explored for vision, the reviewers appreciated the introduction of a meta-learning approach for language modeling which presents unique challenges due to the discreteness and variations in token lengths. The experiments also showed good gains at the tested scales. The reviewers pointed out two major concerns with the current work: first, the reviewers pointed out a fatal flaw in the theory which was a key motivation for the work; second, the large wall-clock times for data distillation beg a deeper empirical study into amortizing this cost over training of multiple models with the distilled dataset. Unfortunately, while the authors acknowledged the reviewer concerns, the reviewers commented on a lack of initiative on part of authors to make the necessary amends in the paper pdf. I would highly recommend the authors to address these issues and aim for a more comprehensive and balanced overview of their approach in a revised version of the paper.\nJustification For Why Not Higher Score:\nFatal flaw in theory. Empirical justification for amortizing high wall-clock time missing.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "Time benefits of using Farzi data", "Subheading": "Official CommentbyAuthors23 Nov 2023, 03:03Everyone", "Content": "Comment:\nWe note that multiple reviewers asked for time advantages of using the Farzi optimization. We believe this is a great point that is worth talking about, and almost the entirety of the data distillation literature doesn\u2019t focus on the wall-clock time improvements. However, since we absolutely agree with this line of thought, we present the wall-clock times of some of the major operations in Farzi (and will include it in the main-text) while summarizing the largest dataset used in our experiments (Netflix dataset, 476k sequences), all using the same codebase and hardware environments:\nData size\nOperation\nTime\n$[2000 \\times 200]$\nFarzi optimization (fake data optimization)\n7500s (15s per 500 outer loop steps)\n$[2000 \\times 200]$\nModel training on fake data\n172s total, optimal at 100 epochs ($\\equiv 0.2$M sequences)\n476k sequences\nModel training on entire dataset\n3700s total, optimal at 30 epochs ($\\equiv 14$M sequences)\nPlease note that, as other reviewers have pointed out, the cost of performing the fake data optimization can easily be amortized in very practical scenarios where we need to train multiple models (or even perform, e.g., hyper-parameter tuning) on the same dataset, which is almost always the case. Such amortization strictly depends on the ratio of the aforementioned quantities in the table, and in our case, the cost of data optimization is only $2\\times$ the cost of training a model on the entire dataset. This is a very reasonable amortization trade-off for more than an order of magnitude future training speed-up (172s vs. 3700s)."}, {"Heading": "Official Review of Submission9481 by Reviewer At7H", "Subheading": "Official ReviewbyReviewer At7H10 Nov 2023, 06:34 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposes FARZI, a data distillation method for auto-regressive ML tasks/event-sequence datasets. The method summarizes a large dataset into a set of synthetic sequences in latent space which can be decoded later. They show that model performance is upheld/enhanced when compared to training on the complete dataset on the downstream tasks of sequential recommendation and language modeling. For data distillation, the paper shows Adam to be better than SGD as inner loop optimizer, and derives an efficient reverse mode differentiation of Adam such that its memory complexity is independent of the number of inner loop steps.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nOriginality and Significance: The latent parametrization that makes FARZI optimization friendly, and the proposed trick that enables reverse mode differentiation of Adam such that its memory complexity is independent of the number of inner loop steps are great contributions and of practical value.\nQuality and Clarity: The paper is well written with extensive experiments whose details and evaluations are that are clearly described. The results are impressive. The method is able to achieve better performance on downstream tasks compared with using the full dataset.\nWeaknesses:\nIt is not clear whether this method will be practical and scale for larger language models and larger datasets. It would be great if the authors can elaborate on this.\nThere is not a clear analysis of the total time gains of this method in comparison with training from scratch. Providing some values would make the case for this method more compelling.\nQuestions:\nListed in weakness section.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to reviewer At7H", "Subheading": "Official CommentbyAuthors23 Nov 2023, 03:10Everyone", "Content": "Comment:\nWe really thank you for your time and effort in reviewing the paper and providing us with your valuable feedback!\nBelow are the responses to your important and well-needed questions, and please let us know if we can add / expand on something:\nQ: It is not clear whether this method will be practical and scale for larger language models and larger datasets.\nA: We split the response into two parts, one for larger datasets, and one for larger models:\nThe model is out-of-the-box scalable to as large datasets as we want, simply because the model only requires\nbatches of the dataset we want to summarize\n. For more context, the largest dataset used in the paper (Netflix) has almost half a million data-points, and we only use a batch size of $512$ in the outer loop. Even with such an extremely small batch size, only $2,000$ sequences synthesized by Farzi can train models equally well as training on all half a million sequences in the original dataset.\nBeing restricted to an academic research environment, we don\u2019t have the compute necessary to experiment with large language models (LLMs). Even though it would be a well-timed application to LLMs, we believe Farzi paves a clear way for future research in data distillation for LLMs. Nonetheless, we would like to point out that Farzi\u2019s reverse-mode Adam derivation makes it possible to perform data distillation for LLMs using no more than the same hardware that is used to train such LLMs. In other words, the memory complexity for Farzi is the same as training the base model.\nQ: There is not a clear analysis of the total time gains of this method.\nA: Great question! Since this question was also raised by other reviewers, we have put the results in a separate comment up top for the sake of not repeating, and are happy to present our impressive results!\nWe would like to end by requesting you to kindly reconsider your final rating as it strictly decides the outcome of this paper, which in your own self-opinion is of good practical value, and contains strong empirical evidence."}]}, {"Heading": "Official Review of Submission9481 by Reviewer FjiL", "Subheading": "Official ReviewbyReviewer FjiL05 Nov 2023, 17:10 (modified: 28 Nov 2023, 14:42)EveryoneRevisions", "Content": "Summary:\nThe paper introduces FARZI, a data distillation framework for machine learning tasks. The goal is to condense the original large dataset into a much smaller number of synthetic sequences, so that downstream performance on the synthetic data matches (or even improves) performance on the full real dataset. The authors cast the problem using a bi-level optimization formulation, similar to meta-model matching based dataset distillation. The naive formulation is infeasible due to the very large token vocabulary and the maximum sequence length. To address this, the authors propose to factorize the synthetic dataset into a latent data summary and a token-decoder matrix. This renders the optimization continuous (as opposed to discrete), while it provides flexibility to sample synthetic sentences from a distribution (as opposed to having a fixed small set of synthetic sentences). Furthermore, the authors suggest to replace SGD in the inner loop by the Adam optimizer. To mitigate the large memory footprint, they derive an efficient approximation for reverse-model differentiation of the Adam optimization. The authors assess FARZI on sequential recommendation and language modeling tasks, where they manage to match or even exceed the downstream full-data performance using as little as 0.1% of the original dataset. The authors conduct several experiments and ablation studies to shed light on various aspects of their framework.\nSoundness:\n2 fair\nPresentation:\n4 excellent\nContribution:\n2 fair\nStrengths:\nThe paper makes several interesting contributions. The meta-model matching based dataset distillation was originally proposed for continuous data (e.g., image data), as opposed to language data that use discrete tokens. The use of a latent space addresses this challenge by ensuring that the optimization can be performed in a continuous space, but by also allowing us to sample the synthetic sentences from a compact distribution. Furthermore, the observation that the Adam optimizer is a much better choice for the inner loop optimization (compared to SGD) is very interesting and dramatically improves downstream performance. To address the large memory footprint, the authors derive an efficient approximation of the reverse-mode differentiation of the Adam optimizer, which nicely complements their finding that Adam is better than SGD. Interestingly, this may be more broadly applicable in other bi-level optimization tasks (e.g., in a meta-learning context).\nThe paper is well written and the related work is covered quite extensively. The authors describe in detail the various insights of their framework. When it comes to the experimental evaluation, they provide a lot of information on the metrics, datasets, hyperparameters, objectives, and even architectures.\nThe experimental evaluation is quite convincing and supports the claims made by the authors. It is very interesting that FARZI can even outperform downstream performance on the full original dataset, which could indicate the improved robustness with dataset distillation. I liked the fact that the authors investigated various aspects of FARZI, such as the versatility of the synthetic data, the cross-architecture generalization, the performance of different meta-objectives, the cold start problem, and the impact of pre-trained trajectories.\nWeaknesses:\nEven though this paper makes interesting contributions to the DD literature for autoregressive tasks, it is not so obvious that it would be \nvery helpful for much larger text corpora and large language models with millions or billions of parameters. The memory footprint might end up being very large, rendering the whole framework infeasible. Furthermore, a compression rate of 0.1% may not be extremely helpful for very large datasets consisting of billions of sentences. This may limit the applicability of FARZI to settings consisting of \"reasonably large but not very large\" language corpora.\nIt was not clear to me how time-consuming the FARZI dataset generation process is. For example, how long did it take to generate the synthetic datasets for the tasks considered in this work? In particular, did FARZI improve the total runtime? For instance, if generating the synthetic data takes very long, then there may be very little benefit (if any) from this process. Furthermore, it is not automatically obvious that a smaller dataset can be trained faster than a larger one. There is the added question of the number of epochs required to reach convergence. The synthetic dataset may require more rounds. This was not obvious in the experimental evaluation. If I am not mistaken, I feel that the subject of runtime was only superficially touched in this work, and a more thorough discussion (with detailed pros and cons) would be needed.\n(Theoretically, this may not be a big issue if the same synthetic dataset could be successful used on several downstream tasks, but this is not immediately true. If we need dataset distillation for each separate task, then we may end up performing FARZI several times.)\nQuestions:\nCould the authors elaborate more on the total runtime (total time for synthetic dataset generation + total time for downstream training with synthetic vs. full data)? It would be helpful if the authors could shed light on the various questions/comments raised in Weakness (2) above.\nIn Equation (2), \\Omega is a set containing initializations for the inner loop, if I understand correctly. But instead of picking the initialization randomly, these come from a small number of training trajectories on the full dataset. If that is true, then the \\theta_i in the definition of \\Omega has nothing to do with the update rule for \\theta_t in Equation (2). This may still be confusing to some readers though because the same symbols are used (theta with a subscript, so the authors may want to clarify this point (i.e., what exactly is in \\Omega).\nI was not clear how exactly the authors chose the final hyperparameters for each setting. Did they exhaustively try all corresponding combinations in the hyperparameter table and picked the best one?\nIs a new synthetic batch created at the beginning of each outer-loop step based on the latent factorization?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to reviewer FjiL", "Subheading": "Official CommentbyAuthors23 Nov 2023, 03:14Everyone", "Content": "Comment:\nWe really thank you for your time and effort in reviewing the paper and providing us with your valuable feedback! Below are the responses to your well-motivated questions, which we hope will shed more clarity on Farzi\u2019s design. Please let us know if we can add / expand on something:\nQ: It is not so obvious that Farzi would be very helpful for much larger text corpora and large language models\nA: While we absolutely agree that large language models would be an exciting application for Farzi, we would like to mention that most research labs do not have the compute necessary for such line of work. Hence, we believe penalizing for such reasons (that is only if considered while rating this paper) might be a very strict requirement that implicitly undermines exploratory or academic research.\nQ: Could the authors elaborate more on the total runtime.\nA: Great question! Since this question was also raised by other reviewers, we have put the results in a separate comment up top for the sake of not repeating, and are happy to present our impressive results!\nQ: $\\theta_i$ in the definition of $\\Omega$ has nothing to do with the update rule for $\\theta_t$ in Equation (2)\nA: Thanks for pointing this out! There is definitely a clarification to be made here: $\\Omega$ contains the\nflat\nset of all episodic checkpoints for the training trajectories and when we sample $\\theta_0$ (0 here represents the inner loop time-index), it can come from the union of all the intermediate checkpoints available to us ($\\Omega$). We\u2019ll make sure to add the clarification in the pdf as well.\nQ: It was not clear how exactly the authors chose the final hyperparameters for each setting\nA: Just like any deep learning setup, we test a random subset (as per a reasonable grid search budget) of the grid of combinations listed in Tables 4 & 5 and report the best result after selecting the best hyper-parameters using a validation set.\nQ: Is a new synthetic batch created at the beginning of each outer-loop step based on the latent factorization?\nA: Thanks for the question! We list the sequence of steps in Farzi for some added clarity and will add this as an algorithm block in the appendix:\nBefore any optimization we randomly initialize $\\tilde{\\mathcal{D}}_{syn}$ and $\\mathbf{M}$\nSample a batch of fake sequences from $\\tilde{\\mathcal{D}}_{syn}$\nPerform one outer loop step as demonstrated in figure 2\nRepeat from step 2, till convergence\nWe would like to end by requesting you to kindly reconsider your final rating as it strictly decides the outcome of this paper, which in your own self-opinion contains important technical contributions, and contains strong empirical evidence."}]}, {"Heading": "Official Review of Submission9481 by Reviewer htDK", "Subheading": "Official ReviewbyReviewer htDK01 Nov 2023, 03:52 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposes a method for distillation of \"auto-regressive data\", in this case meaning any data that is represented as event sequences. This can include natural language text, but also general time-series data. Their method aims to summarize a dataset into a sequence of latent embeddings (which can subsequently be decoded) given a downstream task such that they achieve similar performance to training on the complete dataset. They do this through a meta-learning procedure, optimizing directly through Adam for data which lowers downstream task loss.\nSoundness:\n4 excellent\nPresentation:\n4 excellent\nContribution:\n3 good\nStrengths:\nMy review comes from the point of view of someone familiar with training on natural language (and associated downstream evaluation), but not general event forecasting problems. I was not familiar with the benchmarks used by the author prior to reading this paper.\nOriginality and Significance\nThe paper seems original. Aspects of this work (e.g. using meta-learning/second order methods) for distillation have been touched on in the past, but usually for smaller datasets, and generally not for auto-regressive tasks. Most past works I have seen which work on large corpuses revolve around finding mixing coefficients for existing datasets [1]. This method doesn't work on datasets of that size, however this shows an improvement in scaling.\nGetting a meta-learning approach to work on such dataset sizes is quite difficult, given difficulties with estimating second-order components over the full dataset. Scaling this to even larger language-style datasets would be an interesting (future) contribution.\nQuality and Clarity\nThis paper is quite well-written. Experimental details are clear, and the method is properly motivated. Diagrams clarify the algorithm and the key difficulties to this method are highlighted appropriately.\n[1] The Pile: An 800GB Dataset of Diverse Text for Language Modeling, Gao et al. 2021\nWeaknesses:\nWeaknesses\nThe authors touch on language datasets as a motivation, however do not study this (or other large-sequence tasks) due to practical model/sequence length scaling constraints. Are there reasonable paths forward that would allow this to scale to longer sequence lengths/larger models?\nGiven that the outer loop evaluates across the full original dataset, and the inner loop needs to be run several times to get updated parameters (Figure 5), what's the overall cost saving versus just training a model on the original dataset for more time (until matching student performance), if any?\nHave the authors thought about cases where there is significant noise in the training corpus? Given that the loss is computed with respect to the original dataset, it seems like this could be a problem if one ever tried to directly filter a noisy web-crawl.\nQuestions:\nAll questions have been included in the \"Weaknesses\" section above.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to reviewer htDK", "Subheading": "Official CommentbyAuthors23 Nov 2023, 03:19Everyone", "Content": "Comment:\nWe really thank you for your time and effort in reviewing the paper and providing us with your valuable feedback! Below are the responses to your well-motivated questions, which we hope will shed more clarity on Farzi\u2019s design. Please let us know if we can add / expand on something:\nQ: Are there reasonable paths forward that would allow this to scale to longer sequence lengths/larger models?\nA: Thanks for the question! Regarding longer sequence lengths, even in most LLM pre-training setups, typical input sequence length is in the order of a few thousands. This is certainly feasible with the setup proposed by Farzi\u2019s latent parameterization (but would struggle if the sequence length, e.g., exceed hundreds of thousands). Further, performing the Farzi optimization for larger models is also possible out-of-the-box with hardware/compute requirement no-more-than training the LLM itself. The only reason we weren\u2019t able to perform data distillation for LLMs is because we don\u2019t have the compute to train LLMs by itself.\nQ: What's the overall cost saving versus just training a model on the original dataset for more time?\nA: Great question! Since this question was also raised by other reviewers, we have put the results in a separate comment up top for the sake of not repeating, and are happy to present our impressive results!\nQ: Have the authors thought about cases where there is significant noise in the training corpus?\nA: Wonderful question, and highly relevant in, e.g., pre-training scenarios as you mentioned. Even though we don\u2019t have direct evidence in this paper, there is indeed very strong hope for automatic denoising while performing data distillation. Looking at Figure 4 in [1] which also performs data distillation, training on data summarized by data distillation is (1) definitely much better than training on random samples of noisy data; and (2) better than training on the full dataset when there is high-noise in the datasets. The intuition behind this is that data distillation implicitly promotes de-noising, as the overall optimization objective is to maximize model performance within a small data budget, implicitly minimizing the noise in such low-data scenarios.\nWe would like to end by requesting you to kindly reconsider your final rating as it strictly decides the outcome of this paper, which in your own self-opinion is well-timed, and a fresh approach to data optimization for autoregressive tasks like NLP.\n[1] Sachdeva, Noveen, et al. \"Infinite recommendation networks: A data-centric approach.\" Advances in Neural Information Processing Systems 35 (2022): 31292-31305."}]}, {"Heading": "Official Review of Submission9481 by Reviewer THWQ", "Subheading": "Official ReviewbyReviewer THWQ31 Oct 2023, 19:25 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper provides an extension of dataset distillation to sequence modeling along with a few other innovations, such as a low rank approximation of the distilled dataset and an efficient trick to save memory during meta-learning. Overall, the paper contains strong (albeit limited) empirical results on the sequence modeling (penn tree bank) and recommendation systems datasets.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n3 good\nStrengths:\nThe high level motivation of the problem is quite the need of the hour, as with larger models we need to better understand their dependencies on the data\nPursuit of this research direction could potentially yield methods that enable us to train SOTA transformer models for a fraction of the input cost\nEmpirical results are thorough, although a bit limited in terms of number of datasets for sequence modeling (only PTB is used)\nWeaknesses:\nA number of points about the approach were unclear to me from the writeup, and I would appreciate clarifications from the authors:\nIt is said that the complexity of the dataset distillation algorithm scales by the size of the vocabulary (page. 4) and the size of the sequence that we wish to model. I can see the latter to be the case, since the loss will now be summed over the entire sequence as opposed to one forward pass (so the complexity of the forward pass is increased). However, I do not see how the time complexity increases with the vocabulary size. Do we mean space complexity? Also, more than the forward pass the dominant factor in dataset distillation is the computation of a bunch of hessian vector products in the meta gradient. Those terms do not depend on the vocabulary size either\u2026 please clarify..\nIt would be nice to provide an intuition for what is saving the memory, making things O(1) in memory.  Currently the big algorithm block does not provide an intuition for how this approach is O(1) in memory regardless of the number of timesteps of unrolling. This is important to clarify, since this is an important contribution, if clearly explained. If this approach is essentially gradient checkpointing, then it is worth noting that Deng and Russakovsky already implement a version of this in their code.\nLooking at Eqn. 2, I am a bit puzzled as to how \\Omega, namely the trajectories from the real data are incorporated in the DD process. From what I am able to understand, \\theta_0 \\sim Omega -- namely the init is sampled from the pretrained trajectories, and then from the right hand side of eqn. 2 I understand that the rest of the trajectory is obtained using Adam on the synthetic data. Where is the role of the pretrained trajectories then? Please explain..\nRank regularization has been done in the previous work (Deng and Russakovsky) for dataset distillation. It should be cited that this has been done, and not be presented as a novelty..\nQuestions:\nMy major questions concern the clarifications about the approach listed above, without which it is really hard to judge the technical correctness / soundness of the paper.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to reviewer THWQ", "Subheading": "Official CommentbyAuthors23 Nov 2023, 03:26Everyone", "Content": "Comment:\nWe really thank you for your time and effort in reviewing the paper and providing us with your valuable feedback! Below are the responses to your well-motivated questions, which we hope will shed more clarity on Farzi\u2019s design. Please let us know if we can add / expand on something:\nQ: I do not see how the time complexity increases with the vocabulary size.\nA: Looking at Farzi\u2019s time complexity in Section 3, both the time and space complexities depend directly on the vocabulary size, i.e., $\\dim(\\mathcal{V})$. Let me provide further intuition on this:\nTaking the naive, non-parameterized Farzi data parameterization as a simpler reference, each entry in the $\\mu \\times \\xi \\times \\dim(\\mathcal{V})$ data summary is a\nparameter\nin the outer-loop optimization. Hence, both the sequence length ($\\xi$) and the vocabulary size ($\\dim(\\mathcal{V})$) directly contribute to the space complexity, as well as the time complexity, e.g., for computing and updating the meta-gradient for each of these parameters, that too for each of the $T$ steps in the inner loop.\nQ: What is making things O(1) in memory? Is it essentially gradient checkpointing?\nA: Thank you for the question! Definitely makes sense to provide an intuition for this. The key technological advancement here is Algorithm 1, and to be more specific Lines 13-15, which derive the reverse-mode meta-gradient calculation equations, when Adam is used in the inner-loop. Having access to these equations, we can now only keep the $d\\mathbf{m}, d\\mathbf{x}, d\\mathbf{w}$ vectors as loop carry-over variables and compute the exact meta-gradient for updating the data summary (i.e., $d\\mathbf{x}$) in $O(T)$ time using the for-loop in Algorithm 1, and making the space complexity independent of $T$. However, naive autodiff and standard meta-gradient libraries like higher would store\nall intermediate variables of the inner-loop\nmaking the space complexity linear in terms of $T$.\nFurther, regarding the comparison to (Deng and Russakovsky), please note that they first of all leverage prior work [1] which developed reverse-mode SGD (similar to our reverse-mode Adam, albeit much simpler due to being a first-order optimizer). Next, reverse-mode Adam is completely different from gradient checkpointing in the case of meta-learning where it\u2019s non-trivial and suboptimal [2]. On the other hand, our reverse-mode Adam is correct (optimal) because of our hand-derived meta-gradient calculations (Proposition 3.2).\nQ: How is $\\Omega$\u2014the trajectories from the real data\u2014incorporated in the DD process?\nA: Thanks for pointing this out! There is definitely a clarification to be made here: $\\Omega$ contains the\nflat\nset of all episodic checkpoints for the training trajectories and when we sample $\\theta_0$ (0 here represents the inner loop time-index), it can come from the union of all the intermediate checkpoints available to us ($\\Omega$). We\u2019ll make sure to add the clarification in the pdf as well.\nQ: Rank regularization has been done in previous work, and not be presented as a novelty.\nA: Thanks for pointing this out! We do not intend to claim latent parameterization of data, in its entirety as our novelty. However, we sincerely believe the following pieces of development to be critical and also novel to our propositions:\nPrevious work regarding parameterized data distillation (not just (Deng and Russakovsky)) have been motivated to achieve better performance using fewer total bytes to store the data. However, in our setting of autoregressive data distillation, we would like to explicitly note that naive data parameterization is infeasible computationally due to the vocabulary size dimension in our data.\nWe provide formal connections as to why rank regularization\nimproves\nperformance by reducing overfitting and promoting implicit regularization. Please note that while this conclusion is intuitive, we are the first to showcase a formal proof.\nWe would like to end by requesting you to kindly reconsider your final rating as it strictly decides the outcome of this paper, which in your own self-opinion is well-timed and contains strong empirical evidence.\n[1] Maclaurin, Dougal, David Duvenaud, and Ryan Adams. \"Gradient-based hyperparameter optimization through reversible learning.\" International conference on machine learning. PMLR, 2015.\n[2] Hascoet, Laurent, and Mauricio Araya-Polo. \"Enabling user-driven checkpointing strategies in reverse-mode automatic differentiation.\" arXiv preprint cs/0606042 (2006)."}]}, {"Heading": "Official Review of Submission9481 by Reviewer LDAP", "Subheading": "Official ReviewbyReviewer LDAP17 Oct 2023, 17:31 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe authors introduce a dataset distillation (DD) method called Farzi Data for data with a \"left to right\" (autoregressive) causal structure. Their algorithm has two novel elements: 1) the parameterization of the synthetic distilled data, which allows them to apply it to discrete data (such as the tokens in language modeling); and 2) a method for computing the outer loop gradient for DD when the inner loop is performed with Adam, which has a constant memory footprint independent of the number of inner optimization steps. They conduct extensive experiments with their proposed method on language modeling and sequential recommendation tasks. Compared to existing DD methods (adapted to discrete data via their parameterization), they obtain improved performance across the tested datasets, often obtaining downstream performance better than training a model on the entire original dataset.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n3 good\nStrengths:\nAlgorithmic Contribution.\nAlgorithm 1 for computing the gradient through the inner-loop optimization with Adam using constant memory is a significant contribution. Among existing dataset distillation methods, those which take into account the entire training trajectory on the distilled data tend to obtain better accuracy (as compared to other methods which use surrogates for this objective such as the gradient matching objective in dataset condensation). However, the computational burden of these methods (specifically the memory requirement, which necessitates keeping the entire computation graph) renders them infeasible for application to larger datasets. Farzi Data takes a significant step towards addressing this problem by introducing an algorithm for differentiating through an inner loop optimized with Adam, whose memory does not scale with the number of steps in the inner loop (see Fig. 5). This is an important improvement for DD to be practically useful in real ML applications.\nEmpirical Results.\nThe empirical results are also impressive. The authors obtain better performance than competing methods across several different real-world benchmarks. There are even scenarios where their distilled data consistently outperforms training on the entire original dataset (cf. Table 1), indicating that Farzi Data implicitly promotes some sort of \"data cleaning\" whereby samples that\nhurt\nmodel performance are removed or discounted. This is similar to, e.g., removing mislabeled points or data with negative Shapley values, but Farzi Data is not explicitly trained for this task.\nWeaknesses:\nPresentation and Clarity.\nWhile the actual prose of the paper was generally clear and easy to read, there are some major concerns with notation/presentation that limit understanding of some of the main contributions of the paper.\nP1. There are many cases where important notation is not defined. For instance, $\\mathrm{Rep}(\\mathcal{F}, \\mathcal{D})$ is defined in the Appendix, but not the main text, and is critical to interpreting Theorem 3.1. It is not stated what the terms $d\\mathbf{m}$, $d\\mathbf{x}$, and $d\\mathbf{w}$ in Algorithm 1 are supposed to be, so it is impossible to determine if the expressions are correct or not. How to construct the output of the algorithm from these quantities is also not clear. What is the correspondence of the quantities in Alg. 1 to the DD problem, i.e., what will we actually update using the meta-gradient once we know how to compute it? Some (but not all) of these details can be found in the Appendix, but as they are critical to being able to understand the results, they should be moved to the main text and given appropriate explanations.\nP2. Stylistically, there is also some nonstandard notation. For instance, $\\mathcal{O}(100)$ (3rd bullet point, pg. 2). I suppose the authors meant \"on the order of 100x\", but big-O notation has a mathematically precise meaning that doesn't make sense here. Another instance is Proposition 3.2. \"Correctness of Algorithm 1, Line 13\" is not a complete mathematical statement (or a complete sentence). The result should be stated completely and precisely.\nTheoretical Results.\nThere are also issues with the theoretical results.\nT1. The most critical problem is that the proof of the main theorem (Theorem 3.1) is not mathematically sound. Specifically, the authors want to show that the expected representativeness of their low-rank synthetic data parameterization is strictly less than the expected representativeness of a naive synthetic data parameterization, under some suitable conditions and for quadratic classifiers: $\\mathbb{E}[\\mathrm{Rep}(\\mathcal{F}, \\mathcal{D}_F)] < \\mathbb{E}[\\mathrm{Rep}(\\mathcal{F}, \\mathcal{D}_N)]$. ($\\mathcal{D}_F$ and $\\mathcal{D}_N$ stand for Farzi and naive data, respectively.) In their proof in Appendix B.1, they show that $\\mathbb{E}[\\mathrm{Rep}(\\mathcal{F}, \\mathcal{D}_F)] < B_1$ and $\\mathbb{E}[\\mathrm{Rep}(\\mathcal{F}, \\mathcal{D}_N)]$ for some bounds $B_1$ and $B_2$. Then, since $B_1 < B_2$, they conclude the desired result. This is not valid: $a < b$, $c < d$, and $b < d$ does not imply that $a < c$. There needs to be a\nlower\nbound on the representativeness for the naive parameterization.\nI remark that I believe the\nresult\nis (at least \"morally\") correct. The theorem essentially reduces to saying that the Rademacher complexity resulting from the low-rank parameterization is smaller than the Rademacher complexity from a general parameterization, which is intuitively obvious. However, the\nproof\nhas a fatal error and must be corrected somehow.\nT2. For Lemma B.3 to hold, there must clearly be some assumptions on the loss function $l$; in order to apply the lemma from Shalev-Shwartz, the Rademacher complexity of the loss composed with the models in $\\mathcal{F}$ must be considered, not $\\mathcal{F}$ itself. As stated, I believe this lemma is not correct and the loss must be accounted for. Apart from the logical error, the motivation for the use of quadratic classifiers in the theorem wasn't clear to me. What connection do such models have to the auto-regressive tasks that Farzi Data is applied to?\nT3. This is related to the presentation problems regarding the notation used in Algorithm 1, but the proof of Proposition 3.2 is also suspect. What is meant by $d\\mathbf{m} = d\\mathbf{m} + \\frac{\\partial w_t}{\\partial m_t} \\cdot d\\mathbf{w}$? Is $w_t$ supposed to be $\\mathbf{w}_T$, or is this expression meant to be a recursive formula? What about the formulas for the other quantities, and how are these combined to compute the meta gradient?\nIf these issues can be satisfactorily addressed, along with the questions in the section below, I would be willing to raise my score to accept, given how promising the empirical results are.\nQuestions:\nQ1. The authors mention that training with the reference trajectories $\\Omega$ is important for obtaining the best performance, as compared with training only from randomly initialized networks. However, it wasn't clear to me if this might just have been the result of a greater number of training steps when learning the distilled dataset. That is, are the results in Fig. 6(b) with the total number of meta-gradient steps constant, or do the additional precomputed trajectories result in more meta-gradient steps?\nQ2. On a related note, it was not clear to me exactly how the precomputed trajectories were used. My assumption was that instead of training the network in the inner loop only from random initializations, instead the network from the inner loop will be initialized with parameters from one of the training trajectories. Is this correct?\nQ3. Why isn't FMLP also used as a teacher network in Table 1?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to reviewer LDAP (Part 1/2)", "Subheading": "Official CommentbyAuthors23 Nov 2023, 03:35Everyone", "Content": "Comment:\nFirst of all, we really appreciate your review \u2014 we believe this is a good quality review where you ask very valid questions; irrespective of your low score recommendation. However, we do hope our responses shed some light on your key confusions and convince you to improve your score. It would really help us push this important line of work to one of the most technically capable audiences, at ICLR. Please let us know if we can add / expand on something:\nQ: Important notation is not defined in the main-text.\nA: Thank you for raising these super-valid points! We will make sure to address the following changes as you suggested to improve the readability of our paper:\nwhat the terms $d\\mathbf{m}, d\\mathbf{x}, d\\mathbf{w}$\n: We apologize for missing this detail. $d\\mathbf{m}, d\\mathbf{x}, d\\mathbf{w}$ are the meta-gradients of $f(w_T)$ w.r.t $m_0, x, w_0$ respectively. Most of this notation is re-used from [1], but we will make sure to add this to the main-text.\nwhat will we actually update using the meta-gradient in Algorithm 1\n: Using simple, gradient based optimization in the outer loop (we use Adam), once we have the meta-gradient from Algorithm 1, i.e., $d\\mathbf{x} \\triangleq df(w_T) / d\\mathbf{x}$; we use this gradient to update $\\mathbf{x} \\leftarrow Adam(\\mathbf{x}, d\\mathbf{x})$ in the standard way. Note that $\\mathbf{x}$ in our case is parameterized as $\\tilde{\\mathcal{D}}_{syn}$ and $\\mathbf{M}$.\nnon-standard $\\mathcal{O}(100)$\n: Absolutely, we\u2019ll make sure to write 100x instead of $\\mathcal{O}(100)$. Thanks!\nStatement of Proposition 3.2\n: We\u2019ll make sure to write the complete statement in the main-text instead of the current way. Thank you for pointing this out!\nQ: Issue in Theorem 3.1\nA: Thank you for catching our honest mistake at the end of Theorem 3.1 \u2014 we are highly appreciative! We tried taking the lower-bound approach as you suggested but to no-end: there are no commonly available lower-bounds for Radamacher complexities because of the underlying $\\sup()$ in its definition. Even trying to develop novel lower bounds isn\u2019t a trivial process and/or contribution. Hence, given the short timeline of this rebuttal, we have decided to remove Theorem 3.1 from our paper and totally admit to our mistake.\nWe do hope that you believe that removing this Theorem doesn\u2019t hurt our technical and practical contributions too much, as you have yourself suggested in your review.\nQ: Notation questions in Proposition 3.2\nA: Thanks for pointing this out - we can definitely improve upon the clarity here. You\u2019re correct by mentioning that this is a recursive formula, starting from $t=T$ going all the way to $t=1$. As outlined in Algorithm 1, $d\\mathbf{m}, d\\mathbf{x}, d\\mathbf{w}$---which are the meta-gradients of $f(w_T)$ w.r.t $m_0, x, w_0$ respectively---are variables which are carried over, with the equations in Algorithm 1 defining the update rules for our recursive meta-gradient calculation. Please note that $d\\mathbf{x}$ which represents $df(w_T) / d\\mathbf{x}$ is the final meta-gradient used for updating our synthetic data ($\\tilde{\\mathcal{D}}_{syn}$ and $\\mathbf{M}$) that we\u2019re interested in. Similarly, for other meta-learning applications like learning suitable weight initializations (e.g., MAML), we can use the same Algorithm 1 to estimate other meta-gradients like $d\\mathbf{w}$ which represents $df(w_T) / dw_0$, if we use Adam in the inner-loop.\nPlease note that our Algorithm 1 shares the same spirit as the popular reverse-mode SGD algorithm with its recursive formulation [1], albeit much simpler due to being a first-order optimizer.\nQ: Are the results in Fig. 6(b) with the total number of meta-gradient steps constant?\nA: Great question again! We would like to mention here that all of the results in Fig. 6(b) are performed with the outer loop running till convergence of a maximum of 4000 steps. Please note that realistically, convergence for all settings in our paper happens between 1000-2000 steps, at most. We\u2019ll make sure to add a clarification in the main text.\nWe would also like to provide some added intuition behind using the pre-trained trajectories as is currently used in Farzi. In the model weight space, $\\Omega$ contains a set of points with varying levels of quality (models are trained starting from random till convergence, with intermediate checkpoints being stored). Farzi\u2019s inner loop, using these points as starting points, now has a better exploration of the entire model weight space, and the final data will be better optimized to train models of varying quality. This makes the final data summary more robust and enables it to train models at different stages of training.\n[1] Maclaurin, Dougal, David Duvenaud, and Ryan Adams. \"Gradient-based hyperparameter optimization through reversible learning.\" International conference on machine learning. PMLR, 2015."}, {"Heading": "Response to reviewer LDAP (Part 2/2)", "Subheading": "Official CommentbyAuthors23 Nov 2023, 03:36Everyone", "Content": "Comment:\nQ: How exactly were the precomputed trajectories used?\nA: Thanks for pointing this out! There is definitely a clarification to be made here: $\\Omega$ contains the\nflat\nset of all episodic checkpoints for the training trajectories and when we sample $\\theta_0$ (0 here represents the inner loop time-index), it can come from the union of all the intermediate checkpoints available to us ($\\Omega$). We\u2019ll make sure to add the clarification in the pdf as well.\nQ: Why isn't FMLP also used as a teacher network?\nA: Great catch! This is motivated primarily by FMLP being very similar to SASRec in its architecture and inductive biases and we\u2019re mostly interested in ascertaining cross-architecture generalization, e.g., Transformer $\\mapsto$ RNN and vice-versa. This allows us to save experiment time and not compute the full 3x3 matrix in Table 1. We\u2019ll make sure to add this clarification in the main text.\nWe would like to end by once again requesting you to kindly reconsider your final rating as it strictly decides the outcome of this paper, which in your own self-opinion contains significant technical contributions along with strong empirical evidence."}, {"Heading": "Official Comment by Reviewer LDAP", "Subheading": "Official CommentbyReviewer LDAP28 Nov 2023, 12:11 (modified: 15 Mar 2024, 00:27)EveryoneRevisions", "Content": "Comment:\nThanks to the authors for their detailed response. I believe your proposed changes will improve the paper. However, as it stands, I still cannot recommend acceptance for the following reasons:\nI don't see an updated pdf, so I cannot verify whether or not the updates to the notation, definitions, etc. were sufficient for the paper to be acceptable for publication.\nRegarding Theorem 3.1, I agree with the authors that simply removing it from the paper would be acceptable. However, the result of this theorem was used as motivation for the novelty of your procedure when responding to other reviewer comments (last question in the response to Reviewer THWQ), so there is still a problem here as well. It also forms a significant part of the paper as presented, and if it is removed, this may have a large impact on other reviewers' scores.\nIn short, I don't believe it's possible for the proposed changes to be fairly assessed for the paper to be accepted to ICLR. I recommend the authors to revise the paper and resubmit to a future conference."}]}]}, "rp5vfyp5Np": {"paper_info": {"Supplementary Material": "pdf", "Primary Area": "reinforcement learning", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "deep reinforcement learning, preference-based reinforcement learning, adversarial reinforcement learning", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "TL;DR": "A behavior-oriented adversarial attack method against deep reinforcement learning agents", "Abstract": "Evaluating the performance of deep reinforcement learning (DRL) agents under adversarial attacks that aim to induce specific behaviors, i.e., behavior-oriented adversarial attacks, is crucial for understanding the robustness of DRL agents. Prior research primarily focuses on directing agents towards pre-determined states or policies, lacking generality and flexibility. Therefore, it is important to devise universal attacks that target inducing specific behaviors in a victim. In this work, we propose BATTLE, a universal behavior-oriented adversarial attack method. In BATTLE, an intention policy is trained to align with human preferences for flexible behavior orientation, while the adversary is trained to guide the victim policy to imitate the intention policy. To improve the attack performance, we introduce a weighting function that assigns importance weights over each state. Our empirical results over several manipulation tasks of Meta-world show the superiority of BATTLE in behavior-oriented adversarial attack settings, outperforming current adversarial attack algorithms. Furthermore, we also demonstrate that BATTLE can improve the robustness of agents under strong attacks by training with adversary. Lastly, we showcase the strong behavior-inducing capability of BATTLE by guiding Decision Transformer agents to act in line with human preferences across various MuJoCo tasks. Our videos are available inhttps://sites.google.com/view/jj9uxjgmba5lr3g.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9480", "PDF Url": "https://openreview.net/pdf?id=rp5vfyp5Np"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9480 by Area Chair n9MK", "Subheading": "Meta ReviewbyArea Chair n9MK08 Dec 2023, 02:35 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThis paper introduces BATTLE, an adversarial attack framework in RL that leverages human preference-based intention policy learning. The paper's strengths and weaknesses are discussed in detail by the reviewers:\nStrengths:\nHuman Preference-Based Intention Policy: Learning intention policy from human preferences is an interesting concept that adds a unique dimension to the field of adversarial attacks in RL.\nThorough Experiment Setups: The experiments cover both classic RL agents and decision transformers.\nTheoretical Analysis and Convergence Guarantees: The paper includes a theoretical analysis of the algorithm.\nWeaknesses:\nComplexity and Uncertainty in Intention Policy Learning: The process of learning intention policy based on human preferences introduces extra requirements and uncertainties, including the expensive collection of human preference data and potential biases.\nClarity and Writing Quality: The paper suffers from issues in clarity, with ambiguous definitions and numerous grammatical mistakes. Important concepts like \"intention policy\" and \"success rate\" are not clearly defined.\nLack of Practicality and Scalability: The reliance on extensive human labeling and assumptions about adversaries' capabilities raise concerns regarding the method's real-world applicability and scalability.\nInadequate Methodological Details and Evaluations: Critical details about the training of the victim policy approximator and the volume of data required are missing. The choice of baselines and limited defense methods used in experimentation also raise questions about the fairness and comprehensiveness of the evaluations.\nConfusing Presentation and Incomplete Discussion of Limitations: The paper lacks clarity in the presentation of experimental results and methodologies. It also omits a discussion of its limitations, which is crucial for a balanced understanding of the work.\nSuggestions for Improvement:\nImprove Clarity and Writing: The paper would benefit significantly from revisions to clarify key terms and concepts, improve the quality of writing, and address grammatical errors.\nAddress Methodological Gaps: Providing more details about the training process, data requirements, and rationale for using preference-based RL would strengthen the paper.\nEnhance Practicality and Real-World Relevance: Including more motivating examples from real-world scenarios and addressing the practicality of assumptions would make the work more applicable to real-world situations.\nBroaden Evaluations: Expanding the range of defense methods tested and providing a fairer comparison with existing baselines would improve the evaluation's comprehensiveness and fairness.\nDiscuss Limitations: Incorporating a discussion on the limitations of the proposed method would provide a more balanced and complete understanding of the research.\nIn summary, while BATTLE presents an interesting and innovative approach to adversarial attacks in RL, it needs refinement in terms of clarity, practicality, methodological detail, and comprehensive evaluation to fully realize its potential impact in the field.\nJustification For Why Not Higher Score:\nSee weaknesses.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "Reproducibility", "Subheading": "Public CommentbyPeter Chen16 Nov 2023, 19:46Everyone", "Content": "Comment:\nDear author,\nWe are really interested in your work, is it possible to release your source code?\nMany thanks.", "Replies": [{"Heading": "Response to Public Comment", "Subheading": "Official CommentbyAuthors17 Nov 2023, 07:20Everyone", "Content": "Comment:\nThank you very much for your interest in our work. We are glad to hear that our research has caught your attention. We are currently in the process of refining our code and updating the paper. Once our revisions are finalized, I'll be sure to get in touch with you."}]}, {"Heading": "Official Review of Submission9480 by Reviewer uWsW", "Subheading": "Official ReviewbyReviewer uWsW31 Oct 2023, 17:06 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper studies behavior-oriented attacks agains deep RL agents, where the adversary forces the victim to have specific behaviors. The proposed attack first learns an intention policy based on human preference, and then trains an adversary to perturb the victim observation such that the behavior follows the intention policy. The adversary also adopts importance weights of states to optimize the attack objective. Experiments on multiple meta-world and mujoco show that the proposed method is able to manipulate the victim with high success rates, including offline policies based on decision transformer. The method can be also used to improve the robustness of agents.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThis paper proposes an interesting type of attacks that are oriented by desired behaviors. Compared to prior works focusing on reward minimizing, the proposed attack can be more widely applicable. In real-world environments where rewards are not well-defined, such attack objective can be interesting to investigate. Learning the intention policy from human preference is also an interesting idea, although I have some concerns on it (see weakness).\nWeaknesses:\nThe human preference-based intention policy learning brings extra requirement and uncertainty to the process - the collection of human preference data can be expensive. More importantly, to obtain human preference labels, one need to first collect diverse behavior data so that human can pick the intended policy. Would the collection of the behavior data already involve a pre-defined target policy? (If that's the case, why not directly use the target policy for attacks?)\nIn experiments, the authors mainly evaluate the attack success rate. However, it is not clear how the success rate is defined. Is it based on whether the victim acts as the intention policy suggests? But would it be biased since the intention policy is just an approximation of the real human intention? What if the intention learning does not learn a desired reward model or intention policy?\nFor baselines, the authors used the codebases of SA-RL and PA-AD and modified their attack's reward as the learned reward. However, this straightforward modification of the PA-AD baseline contradicts with the original method (PA-AD's formulation is for a reward-minimizing adversary, so directly replacing the attacker's reward may not work). Since the original PA-AD method is to use an RL director to find the target policy and to use an actor to conduct targeted attack, a more natural modification of PA-AD in the behavior attack scenario can be to directly use the learned intention policy as the target of actor ($\\hat{a}$ in Equation (G) in the original paper).\nQuestions:\nHow are the behavior sequences generated for human preference labeling?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer uWsW (Part 1/2)", "Subheading": "Official CommentbyAuthors17 Nov 2023, 12:58Everyone", "Content": "Comment:\nWe thank reviewer uWsW for constructive comments. Below are our point-wise responses to each of your concerns:\nQ1: The human preference-based intention policy learning brings extra requirement and uncertainty to the process - the collection of human preference data can be expensive. More importantly, to obtain human preference labels, one need to first collect diverse behavior data so that human can pick the intended policy. Would the collection of the behavior data already involve a pre-defined target policy? (If that's the case, why not directly use the target policy for attacks?)\nA1\n: Thanks for your question regarding the requirement and uncertainty involved in collecting human preference data in PbRL. In the PbRL paradigm, humans are required to provide preferences over pairs of trajectories generated from the victim policy. Therefore, the process does not involve a pre-defined target policy. In fact, the intention policy acts as an adaptive target policy, dynamically aligning with human preferences. There are several reasons why we do not use a pre-defined target policy: (1). It is often challenging to find a pre-defined target policy that precisely meets the specific requirements of a given task or scenario. (2). Relying on a pre-defined target policy often leads to suboptimal outcomes, which fails to adapt to the nuances and variations in different environments.\nQ2: In experiments, the authors mainly evaluate the attack success rate. However, it is not clear how the success rate is defined. Is it based on whether the victim acts as the intention policy suggests? But would it be biased since the intention policy is just an approximation of the real human intention? What if the intention learning does not learn a desired reward model or intention policy?\nA2\n: Thank you for your question. The success rate is determined by evaluating whether the victim performs behaviors as intended by humans. For example, if the human intention is for the victim to open a window, the success rate is calculated based on whether the window is actually opened by the victim under adversarial attacks. Specifically, we have adopted the  success metric directly from Meta-world [1], which is typically based on the distance between the task-relevant object and its final goal pose. For the complete list of success metrics and thresholds for each task, see Appendix 12 of Meta-world.\nQ3: For baselines, the authors used the codebases of SA-RL and PA-AD and modified their attack's reward as the learned reward. However, this straightforward modification of the PA-AD baseline contradicts with the original method (PA-AD's formulation is for a reward-minimizing adversary, so directly replacing the attacker's reward may not work). Since the original PA-AD method is to use an RL director to find the target policy and to use an actor to conduct targeted attack, a more natural modification of PA-AD in the behavior attack scenario can be to directly use the learned intention policy as the target of actor ($\\hat{a}$ in Equation (G) in the original paper).\nA3\n: Thank you for your insightful comments. Let me address each point separately:\nReward Model\n: From a general perspective, the reward model represents the direction of optimization for the adversarial policy. In the original SA-RL [2] and PA-AD [3], the adversary's reward model is the negative of the victim's reward function $r_v$, i.e. $-r_v$. In our work, however, the reward model is learned from human preferences, which allows expression of various intentions and thus enables universal adversarial attacks. In our experiments, the reward learning is the same across all methods, with the primary differences being in the adversary's learning. BATTLE's outstanding performance is attributed to its bi-level optimization framework, where the intention policy and the weighting function effectively collaborate to enhance the adversary's training.\nModification of PA-AD\n: We also provided oracle versions of SA-RL and PA-AD, utilizing ground-truth rewards as the reward model. As shown in Figure 4, SA-RL demonstrated significant improvement in most tasks, but the same was not observed for PA-AD. A possible explanation for this could be related to the inherent design of PA-AD, which performs optimally by accessing the victim's gradients in scenarios using a fixed reward model. However, when it comes to dynamically learning the reward model, this approach may not be as successful.\n(Citations included in Part 2)"}, {"Heading": "Response to Reviewer uWsW (Part 2/2)", "Subheading": "Official CommentbyAuthors17 Nov 2023, 12:59Everyone", "Content": "Comment:\n(continue)\nQ4: How are the behavior sequences generated for human preference labeling?\nA4\n: Thanks for your question. To provide a clear explanation, let's consider an example from our preference labeling process. We collect pairs of victim trajectories under adversary attacks, denoted as $(\\sigma_v^0, \\sigma_v^1)$, which correspond to two adversary-generated trajectories $(\\sigma_a^0, \\sigma_a^1)$. When a human prefers $\\sigma_v^0$ over $\\sigma_v^1$ (i.e., $\\sigma_v^0 \\succ \\sigma_v^1$), it implies that the attack in $\\sigma_a^0$ is more effective or aligns better with human intent, leading to $\\sigma_a^0 \\succ \\sigma_a^1$. Using the preference data on the adversary's trajectories, we can learn an adversary that aligns with human intentions.\nReference\n[1] Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning. CoRL, 2019.\n[2] Robust reinforcement learning on state observations with learned optimal adversary. ICLR, 2021.\n[3] Who is the strongest enemy? towards optimal and efficient evasion attacks in deep rl. ICLR, 2022."}, {"Heading": "Reply", "Subheading": "Official CommentbyReviewer uWsW21 Nov 2023, 13:21Everyone", "Content": "Comment:\nThank you for the response and clarifications. After reading the other reviews, I think there are still some flaws in the paper such as motivation, writing and experiments. So I will maintain my current score."}]}, {"Heading": "Official Review of Submission9480 by Reviewer nUpM", "Subheading": "Official ReviewbyReviewer nUpM31 Oct 2023, 01:37 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThey introduce a method to attack reinforcement learning policies. There are three key components. First, they use PbRL to train a policy to exhibit the desired behavior. This is a relatively novel step because most work in adversarial RL assumes that the desired target behavior is already known and incentivized by a reward function. Second, they train a weighting function to help prioritize relevant states and keep the next step from policy drift. Finally, they attack the target policy with an adversarial policy that makes it behave similarly to the target behavior using the weighting function.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nWorking with both classic RL agents and decision transformers makes for much more thorough experiments.\nAdversarial policies that result in targetedly bad behavior is an area of research I believe is important and neglected.\nWeaknesses:\nWriting\nI have found some of the writing to be confusing and verbose. For example, \u201cintention policy\u201d is not defined until multiple mentions in. The description of what it is in the abstract is very ambiguous. \u201cInner\u201d and outer\u201d level loss are not described. I don\u2019t see a definition for \u201csuccess rate\u201d in the paper. I don\u2019t think the paper does as good of a job as it could with laying out things in a way that is clear and quick to understand.\nI do not understand the rationale in the first paragraph of section 4.1. I do not think this makes sense as an explanation of why an intention policy is needed instead of a direct attack.\nI do not understand figure 1. Why is there an arrow between reward learning and the replay buffer?\nNumerous grammar mistakes. I would recommend using a grammarly browser plugin.\nThis may speak to either issues with my reading of the paper, its writing, or the quality of the experiments. But I am unsure why BATTLE trains an intention policy with PbRL instead of just training the adversarial policy directly with PbRL. This would seem to be substantially simpler.\nI do not understand how SA-RL can perform so poorly relative to BATTLE unless it is just due to reward shaping. What is the reward function used for SA-RL? If an adversarial policy is directly trained to minimize the agent\u2019s reward, how can this do worse than BATTLE at making the target agents\u2019 reward be minimized? If the key difference truly is just reward shaping, then this paper would just seem to be one about how PbRL makes reward shaping automatic and implicit. And if so, then this paper would seem to have no novelty.\nRelatedly, I do not understand why this paper is about adversarial attacks. BATTLE could be used to make RL policies do anything \u2014 not just to targetedly adversarially attack them. But in reality, to make RL policies do things, we just finetune them directly. This relates to why I do not understand why the intention policy was used. Why not just finetune the target angent or adversary directly wit pbrl?\nQuestions:\nWhich experiments were performed with real humans and which were with synthetic feedback?\nSee weaknesses\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer nUpM", "Subheading": "Official CommentbyAuthors17 Nov 2023, 13:01Everyone", "Content": "Comment:\nWe thank reviewer nUpM for the constructive comments and insightful questions. Below, we provide our point-wise responses to each of your concerns:\nQ1: Writing\nA1\n: Thank you for your feedback on our writing. We'd like to clarify the following points.\n(a) The primary goal is to ensure that the victim's behavior under adversarial attacks is aligned with intention policy, which is trained following the framework of PbRL. (b) The inner and outer losses correspond to Equations (6) and (7) in our paper, respectively. (c) The success rate is determined by evaluating whether the victim performs behaviors as intended by humans. For example, if the human intention is for the victim to open a window, the success rate is calculated based on whether the window is actually opened by the victim under adversarial attacks. Specifically, we have adopted the success metric directly from Meta-world [1], which is typically based on the distance between the task-relevant object and its final goal pose. For the complete list of success metrics and thresholds for each task, see Appendix 12 of Meta-world. (d) In Figure 2, the arrow connecting the reward learning component to the replay buffer is intended to represent reward estimation, we will revise it in the revision.\nQ2: This may speak to either issues with my reading of the paper, its writing, or the quality of the experiments. But I am unsure why BATTLE trains an intention policy with PbRL instead of just training the adversarial policy directly with PbRL. This would seem to be substantially simpler.\nA2\n: Thank you for your valuable feedback. As shown in the results presented in Table 2 of our paper, specifically the column 'BATTLE w/o $\\pi_\\theta$', which indicates that directly training the adversarial policy using PbRL was not as effective. The crucial insight from our research is that the intention policy offers essential step-by-step guidance that significantly enhances the efficiency and effectiveness of the adversarial policy development process.\nQ3: I do not understand how SA-RL can perform so poorly relative to BATTLE unless it is just due to reward shaping. What is the reward function used for SA-RL? If an adversarial policy is directly trained to minimize the agent\u2019s reward, how can this do worse than BATTLE at making the target agents\u2019 reward be minimized? If the key difference truly is just reward shaping, then this paper would just seem to be one about how PbRL makes reward shaping automatic and implicit. And if so, then this paper would seem to have no novelty.\nA3\n: Thank you for your question. In our paper, the focus is on a universal adversary that can manipulate the victim to perform behaviors as desired by humans, rather than simply minimizing the victim's cumulative reward. In our experiments, the reward model for all methods is derived from human preferences. The key difference lies in how the adversarial policy is trained. BATTLE's superior performance is largely due to its bi-level optimization framework, which integrates the intention policy and a weighting function.\nQ4: Relatedly, I do not understand why this paper is about adversarial attacks. BATTLE could be used to make RL policies do anything \u2014 not just to targetedly adversarially attack them. But in reality, to make RL policies do things, we just finetune them directly. This relates to why I do not understand why the intention policy was used. Why not just finetune the target angent or adversary directly wit pbrl?\nA4\n: Thank you for your question. Our work aims to propose a universal adversarial attack rather than policy tuning, where techniques like RLHF [1] or DPO [2] involve directly training the policy. Instead, our focus is on how to train a universal adversary capable of manipulating a victim to act according to human-desired behaviors.\nQ5: Which experiments were performed with real humans and which were with synthetic feedback?\nA5\n: Thanks for your question. To ensure systematic and consistent evaluation across all our tests, we opted to use synthetic feedback for all experiments.\nReference\n[1] Training language models to follow instructions with human feedback. NeurIPS, 2022.\n[2] Direct Preference Optimization: Your Language Model is Secretly a Reward Model. NeurIPS, 2023."}, {"Heading": "Reply", "Subheading": "Official CommentbyReviewer nUpM20 Nov 2023, 10:59 (modified: 21 Nov 2023, 10:07)EveryoneRevisions", "Content": "Comment:\nI would not consider accepting this paper without a commitment to a large overhaul of the writing. I believe its overall clarity and organization to be low compared to most literature.\n2-3. I find this response fairly vacuous, and I am very skeptical that this baseline was either well-designed or well-executed. If it is possible to use feedback to train the intention policy, I remain unconvinced that the same source of feedback should not just be used in general to fine-tune the target policy. If adding in the intention policy worked in the control here, my experience as a researcher tells me to suspect that this might be a poorly-executed baseline.\nIt could be partially my fault for missing when the paper states that no human feedback was used. But given its overall writing and the continued description of the method as hinging human feedback in the reply to (3), I don't think this is all my fault, and I encourage the authors to make it clear that no human feedback was used. The fact that none was used also might be a source of variability between controls and the main experiment and why the control did not seem to work as well."}, {"Heading": "Response to Reviewer nUpM", "Subheading": "Official CommentbyAuthors21 Nov 2023, 05:23Everyone", "Content": "Comment:\nTo address your concerns effectively, I'd like to first clarify the threat model of the proposed universal attack. Threat Model:\nAdversary's Limited Access\n: The adversary cannot access the victim's gradients or reward function.\nDynamic Learning of Attack Goal\n: Unlike previous approaches with pre-defined goals, our attack goal requires dynamic learning, which is one key aspect of our method's novelty.\nAdversary's Strategy and Constraints\n: The adversary perturbs the state $s$ into $\\tilde{s}$ restricted by $\\mathcal{B}(s)$ (i.e., $\\tilde{s} \\in \\mathcal{B}(s)$). $\\mathcal{B}(s)$ is defined as a small set {${\\tilde{s} \\in \\mathcal{S}: \\parallel s-\\tilde{s} \\parallel_p \\le \\epsilon}$} which limits the attack power of the adversary, and $\\epsilon$ is the attack budget. This follows the settings in previous methods.\nTherefore, the success of attacking the victim hinges on the algorithm's effectiveness, the amount of feedback provided, and the attack budget.\nWe see that SA-RL (PbRL) and PA-AD (PbRL), which essentially involve training the adversary or director directly with PbRL, do not work as well when there are strict limits, like less feedback and smaller attack budgets. Our additional experiments specifically examined the impact of varying amounts of feedback and attack budgets. When provided with more feedback and larger attack budgets, the performance of SA-RL (PbRL) improved noticeably. Attack budgets, in particular, have a significant influence. SA-RL (oracle) utilizes ground-truth rewards but still exhibits suboptimal performance in some scenarios. These findings indicate that training the adversarial policy directly with PbRL can work. However, this requires a higher volume of feedback and a larger attack budget, making it costly and impractical. In contrast, BATTLE can significantly enhance training efficiency and performance by introducing an intention policy and utilizing the proposed learning framework."}]}, {"Heading": "Official Review of Submission9480 by Reviewer h9xo", "Subheading": "Official ReviewbyReviewer h9xo30 Oct 2023, 23:22 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper introduces BATTLE, an adversarial attack framework targeting DRL agents. The main focus is on behavior-oriented adversarial attacks, which aim to induce specific behaviors in a DRL agent, as opposed to merely reducing the agent's rewards or driving it to a pre-determined state. BATTLE uses an intention policy aligned with human preferences and an adversary to guide the victim DRL agent to imitate the intention policy. A weighting function is also introduced to optimize the effectiveness of the attack. The authors claim that BATTLE outperforms existing methods in inducing specific behaviors and can also be used to improve the robustness of DRL agents when used in an adversarial training setup.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe availability of both code and demos enhances the paper's reproducibility. The paper enhanced the proposed methodology with convergence guarantees for BATTLE, adding rigor to the work.\nWeaknesses:\nThe presentation quality could benefit from further refinement for better clarity and impact.\nThe method's reliance on extensive human labeling hampers its real-world applicability, raising concerns about scalability.\nThe paper could be strengthened by including more motivating examples from real-world scenarios. The assumption that an adversary can modify observations is strong and raises questions about practicality. For instance, if an adversary has the ability to control the sensor, they might as well directly control the effector, making an agent-based adversarial approach seem more practical. Additionally, the rationale for using preference-based RL remains unclear.\nThe paper lacks some critical methodological details. For example, it doesn't specify how the victim policy approximator is trained or the volume of data required, leaving gaps in the understanding of the implementation.\nThe experimental setup and evaluations could be more convincing. The choice of baselines (PA-AD and SA-RL), which are un-targeted attacks, makes the comparison seem potentially unfair. Moreover, while various defense methods like adversarial training, robust learning, policy ensemble, and policy distillation exist, the authors have limited their experimentation to ATLA.\nQuestions:\nCould the authors elaborate on potential real-world applications for the proposed method and discuss the challenges that might arise in such contexts?\nExpanding the experimental results to include additional comparison metrics would be valuable. Specifically, how does PALM fare against targeted attacks and various other defense methods?\nWhat is the extent of annotation required, especially in relation to the complexity of the task at hand?\nCould you provide details on the training process for the victim policy approximator, including the amount of data needed for effective training?\nHow generalizable is the weighting function across different types of tasks and domains?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer h9xo", "Subheading": "Official CommentbyAuthors17 Nov 2023, 13:04Everyone", "Content": "Comment:\nWe thank reviewer h9xo for the insightful comments. Below, we address each of your points in detail:\nQ1: Could the authors elaborate on potential real-world applications for the proposed method and discuss the challenges that might arise in such contexts?\nA1\n: Thank you for your question. As shown in Table 1 of our paper, one of the practical applications of our method is in adversarial training, where it significantly enhances the robustness of RL agents, outperforming methods like SA-RL and PA-AD. This application is particularly relevant in areas where robustness against adversarial attacks is critical.\nQ2: Expanding the experimental results to include additional comparison metrics would be valuable. Specifically, how does PALM fare against targeted attacks and various other defense methods?\nA2\n: Thanks for your question. As detailed in Table 1 of our paper, we subjected robustly trained agents to three different types of attacks to evaluate their robustness. Among these, our BATTLE-ATLA method demonstrated the most robust performance under multiple attack scenarios. This suggests that BATTLE-ATLA is highly effective in challenging even well-defended agents. Furthermore, we applied various defense methods to gauge the robustness of these agents. Our findings indicate that BATTLE consistently outperformed other strategies, showcasing its strong capability in executing universal attacks against robust models. These results collectively highlight BATTLE's efficacy as both an offensive and defensive tool in adversarial settings.\nQ3: What is the extent of annotation required, especially in relation to the complexity of the task at hand?\nA3\n: In the manipulation scenario, we utilized approximately 9000 labels across all tasks. For the opposite behaviors scenario, the number of labels varied based on the specific task: 1000 for Window Close, 3000 for Drawer Close, 5000 each for Faucet Open, Faucet Close, and Window Open, and 7000 each for Drawer Open, Door Lock, and Door Unlock. Detailed information about the implementation and labeling process is provided in Section 5.1 and Appendix F of our paper.\nIt is important to emphasize that BATTLE represents an advancement over previous work in two key aspects:\nBATTLE does not require knowledge of the victim's reward function. Labeling preference is more feasible and less challenging, as acquiring the victim's reward function is often impractical.\nUnlike some previous methods, BATTLE does not require access to the victim's gradients. It operates by observing the external outputs of the system, which makes it significantly more practical.\nQ4: Could you provide details on the training process for the victim policy approximator, including the amount of data needed for effective training?\nA4\n: Thanks for your question. We train the victim models on the Meta-world tasks using the SAC algorithm [1]. Specificially, we employ fully connected neural networks for the policy and Q function. The detailed hyperparameters used in our experiments are provided in Table 5. As for the amount of data, we train each agent for 1 million time steps, which can achieve around 100% success rate on original tasks.\nReference\n[1] Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. ICML, 2018."}, {"Heading": "Official Comment by Reviewer h9xo", "Subheading": "Official CommentbyReviewer h9xo22 Nov 2023, 00:02Everyone", "Content": "Comment:\nThanks the authors for their response. However, my concerns are still Thanks to the authors for their response. However, my concerns regarding the motivation and experiments remain unaddressed. I'll keep the scores."}]}, {"Heading": "Official Review of Submission9480 by Reviewer 6m3H", "Subheading": "Official ReviewbyReviewer 6m3H22 Oct 2023, 11:03 (modified: 22 Nov 2023, 08:55)EveryoneRevisions", "Content": "Summary:\nThis paper introduces BATTLE, a novel universal behavior-oriented adversarial attack method designed to induce specific behaviors in deep reinforcement learning (DRL) agents. Unlike prior approaches that focus on directing agents towards predetermined states or policies, BATTLE employs an intention policy aligned with human preferences for flexible behavior orientation, guiding the victim agent to imitate it. The paper demonstrates the effectiveness of BATTLE through empirical results across various manipulation tasks in Meta-world, showing its superiority over existing adversarial attack algorithms. Additionally, BATTLE enhances the robustness of DRL agents by training with the attacker, achieving a convergence guarantee under mild conditions, and proving effective even against the latest Decision Transformer agents. In summary, the paper makes contributions in the realm of behavior-oriented adversarial attacks on DRL agents, both in theory and practical applications.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n3 good\nStrengths:\nThe paper introduces an interesting and novel concept, proposing a new type of attack based on preference-based RL.\nThe design of the inner-level optimization and outer-level optimization is well-founded, and the paper provides theoretical analysis of the algorithm.\nThe paper sets up different scenarios for evaluating various agents and conducts detailed ablation studies.\nWeaknesses:\nThe writing needs improvement, particularly in clarifying several terms and diagrams. For example, some terms like \"find a precise weighting function to balance the state distribution\" need better explanation. Clarification is also needed for the diagram in Figure 2.\nThe presentation of experimental results is somewhat confusing. The differences of scenarios in Figure 4 and 5 are not clear, and additional explanations are required for the target coordinates mentioned for Figure 4. Captions of Figures 7 (a) and (b) might need to be swapped, and sections (c) and (d) require clearer explanations.\nThe paper lacks a discussion of limitations, which should be addressed.\nQuestions:\nIt's disappointing that the paper doesn't include RADIAL-RL[1] or WocaR-RL[2] as baselines when discussing robust training. Even if only ATLA is selected as a robust baseline, it would be valuable to mention other adversarial robust RL papers in the related work.\nIn the introduction, the paper illustrates the practical implications of targeted attacks on robotics, but the concern is raised that BATTLE is a white-box attack applying perturbations to states. In the context of robotics, its practical applicability is very limited. The paper could benefit from a more thorough clarification or discussion of this concern and its potential implications for practical applications. It fails to persuasively underscore the significance and relevance of this work within the field.\n[1]Robust deep reinforcement learning through adversarial loss\n[2]Efficient adversarial training without attacking: Worst-case-aware robust reinforcement learning\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer 6m3H", "Subheading": "Official CommentbyAuthors17 Nov 2023, 13:05Everyone", "Content": "Comment:\nWe thank reviewer 6m3H for the constructive comments. We value the insights provided and are committed to addressing each of your concerns. Below are our point-wise responses:\nQ1: The writing needs improvement, particularly in clarifying several terms and diagrams. For example, some terms like \"find a precise weighting function to balance the state distribution\" need better explanation. Clarification is also needed for the diagram in Figure 2.\nA1\n: We apologize for the confusion. Regarding the term 'find a precise weighting function to balance the state distribution,' our intent is to convey the process of training a weighting function that can appropriately assign weights to various states. This function plays a critical role in balancing the influence of different states during the learning process. We will provide a more detailed description of this and update Figure 2 to enhance its clarity in the next version.\nQ2: The presentation of experimental results is somewhat confusing. The differences of scenarios in Figure 4 and 5 are not clear, and additional explanations are required for the target coordinates mentioned for Figure 4. Captions of Figures 7 (a) and (b) might need to be swapped, and sections (c) and (d) require clearer explanations.\nA2\n: Thanks for your question. Regarding Figures 4 and 5, the scenarios presented in these figures differ in the objectives set for the adversary. In the manipulation scenario, as depicted in Figure 4, the adversary's objective is to manipulate the victim into grasping fixed objects located far from the original target location. On the other hand, in the opposite behavior scenario shown in Figure 5, the attacker's goal is to induce the victim to behave in a manner that is contrary to its original behavior. More detailed can be found in Appendix F.4. As for Figure 7, we will revise it in the revison.\nQ3: The paper lacks a discussion of limitations, which should be addressed.\nA3\n: Thanks for your question. In the revised version of our paper, we will include a section dedicated to discussing the limitations.\nQ4: It's disappointing that the paper doesn't include RADIAL-RL[1] or WocaR-RL[2] as baselines when discussing robust training. Even if only ATLA is selected as a robust baseline, it would be valuable to mention other adversarial robust RL papers in the related work.\nA4\n: We appreciate the reviewer's insightful feedback. While our paper primarily focuses on introducing a novel adversarial attack method, we are pleasant to include more powerful baselines when discussing robust training to make our discussion more comprehensive.\nQ5: In the introduction, the paper illustrates the practical implications of targeted attacks on robotics, but the concern is raised that BATTLE is a white-box attack applying perturbations to states. In the context of robotics, its practical applicability is very limited. The paper could benefit from a more thorough clarification or discussion of this concern and its potential implications for practical applications. It fails to persuasively underscore the significance and relevance of this work within the field.\nA5\n: Thanks for your question. It is crucial to note that BATTLE is designed as a black-box attack method, not a white-box attack as might have been perceived. This means that BATTLE does not require access to the gradients of the victim model. Instead, it operates by observing the external outputs of the system, making it much more applicable in real-world scenarios where internal access to a system, such as a robotic platform, is typically not feasible."}, {"Heading": "Thanks to the response", "Subheading": "Official CommentbyReviewer 6m3H22 Nov 2023, 08:50 (modified: 22 Nov 2023, 08:53)EveryoneRevisions", "Content": "Comment:\nI appreciate the response from the authors, which has led to improvements in the writing of this paper. While I still have some concerns regarding the motivation and presentation, I have decided to raise my score."}, {"Heading": "Thank you for your reply!", "Subheading": "Official CommentbyAuthors22 Nov 2023, 09:17Everyone", "Content": "Comment:\nDear Reviewer,\nWe are grateful for your assistance in enhancing the quality of our paper and for your decision to update the score. Your constructive comments and suggestions have been invaluable to us.\nBest wishes,\nThe Authors."}]}]}, "miGpIhquyB": {"paper_info": {"Primary Area": "generative models", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Large Language Model, dataset generation", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "TL;DR": "We investigate text dataset generation with LLMs and obtain valuable insights about both the generated datasets a wide range of different LLMs.", "Abstract": "There has been increased interest in using Large Language Models (LLMs) for text dataset generation subject to a desired attribute, e.g., for use in downstream fine-tuning or training. These works generally focus on a single quality metric of the generated text, typically accuracy on a downstream task. However, this fails to consider whether the model even has the ability to faithfully model the data distribution of the desired real-world domain. In contrast, in this work, we additionally focus on important distributional metrics agnostic to the downstream task, such as data diversity and faithfulness. We show that even in simple domains, generated datasets reveal inherent trade-offs between these metrics across models and training regimes. Further, we find that our metrics not only describe the generated dataset, but also capture key aspects of the underlying model. This allows us to characterize the generated datasets, individual models and by comparison the properties of different model families and training paradigms. By focusing on sub-distributions well-represented in the training data of LLMs, we can, for example, show that popular instruction-tuning techniques strongly decrease the LLM\u2019s text generation abilities, with respect to distributional aspects like diversity.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "Supplementary Material": "zip", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9477", "PDF Url": "https://openreview.net/pdf?id=miGpIhquyB"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9477 by Area Chair w8dC", "Subheading": "Meta ReviewbyArea Chair w8dC13 Dec 2023, 20:59 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThis paper discusses multiple automatic metrics to evaluate synthetic data generated by LLMs, considering different models and training regimes, namely: faithfulness, diversity, conformity, complexity, and performance. Findings reveal differences in instruction-tuned models for data generation vs other model families, where they find a loss of diversity.\nStrengths: This paper presents valuable work towards evaluating generations of language models, and reveals important insights into what might be determining the quality of generated data under different training regimes, especially instruction tuning which has led to many generated datasets.\nWeaknesses: Reviewers pointed out some issues with the selection and definition of the metrics, and lack of empirical evidence to conclusively infer some of the claims.\nJustification For Why Not Higher Score:\nPlease see weaknesses; there were some issues with the definitions of some metrics, and some experimental settings which were not fully explored.\nJustification For Why Not Lower Score:\nn/a"}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 10:09Everyone", "Content": "Comment:\nWe thank the reviewers for their feedback and comments. In particular we are delighted that they found that the studied task is interesting (Reviewer Afrd), is sorely needed in dataset generation (Reviewer PKWU) and that our findings are interesting (Reviewer fcAm) and informative (Reviewer PKWU, Reviewer wzFz).\nHere we briefly outline the changes made to the manuscript, mostly clarifications, and a recurring point in the reviews. We address other questions in individual replies.\nChanges to the Manuscript\nIn Appendix D, we analyze the tradeoffs between complexity, diversity, faithfulness and conformity over model size (rather than temperature in Section 4.1).\nAdded further clarification in Section 3 for the purpose of introducing faithfulness and complexity and what aspects of the synthetic dataset they are influenced by.\nDiscussed the third observed tradeoff in Section 4.1 between faithfulness and complexity more accurately and described the exact nature of this tradeoff.\nIncluded the value of the characteristics on the reference dataset in Figure 2 in Section 4.1, Table 2 in Section 4.2 and Table 9 in Appendix E.\nMade minor edits to fix typos and enhance grammar.\nQ1: Based on your work, what are the recommendations for practitioners to use LLMs for training data generation?\nWe encourage practitioners to pay attention to all characteristics of a specific dataset and to look further than just downstream performance and diversity. Recall that all of our characteristics measure concrete effects and biases introduced to the generated datasets, indicating that a wider and more detailed view on dataset generation can be beneficial to many downstream tasks.\nConcretely, we recommend practitioners to consider the specific use cases when selecting LLMs for training data generation. While ChatGPT is a popular choice, it may not always be the most suitable. For instance, if dataset conformity is a priority, choosing vanilla models could yield better results. Each model introduces its unique biases and strengths to the generated data, e.g., by generating faithful but non-conform samples for instruction-tuned models. Therefore using a combination of different LLMs could increase performance and mitigate problems or biases related to a specific model.\nAnother important aspect for dataset generation is the sampling temperature for the generated dataset. Changing this temperature can significantly alter results, and we therefore encourage practitioners to test different sampling temperatures for optimal performance.\nFinally, we would encourage the inclusion of examples in the prompt. We found that especially the conformity and diversity of instruction-tuned models increases dramatically when introducing few-shot samples and found it also improved the performance on the downstream task."}, {"Heading": "Official Review of Submission9477 by Reviewer PKWU", "Subheading": "Official ReviewbyReviewer PKWU30 Oct 2023, 15:20 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis work studies the attributes of dataset generation, which has recently been explored as a way to train task networks without needing a natural, human-generated dataset. Particularly, this work studies 4 domains/tasks that dataset generation can be applied to (e.g. SST-2), and studies the trade-offs between different attributes: faithfulness, diversity, conformity, complexity, and performance, all of which the authors measure automatically. The authors find significant differences between different model types, especially finding that instruction-tuned models differ from classical LMs. Neither paradigm seems to completely dominate.\nSoundness:\n3 good\nPresentation:\n4 excellent\nContribution:\n3 good\nStrengths:\nOverall, this type of contribution is sorely needed in dataset generation, which is still not a well-understood field\nThe attributes to study are diverse and relevant\nVery interesting and informative conclusions drawn about the tradeoffs, e.g. the loss of diversity in generated datasets when using instruction-tuned models\npaper is well presented and quite clear\nWeaknesses:\nI have concerns wrt the measurement of some of the aspects:\nfaithfulness is measured as the accuracy on the synthetic set with a model trained on the reference (human) set. While being unfaithful is one reason this value may be low, it is not the only one. It is easy to imagine a\nfaithful\ndataset on which this classifier will perform poorly, due to issues like style shift or poor generalization of the classifier. To be more concise: staking faithfulness on the accuracy of a classifier ignores the fact that this may be an issue of the classifier rather than the dataset that is being evaluated.\nsimilar issue with complexity, which is measured as inverse accuracy on a held out chunk of the synthetic set. While I agree that lower complexity will indeed raise this accuracy, high complexity is not the only reason this accuracy may decrease.\nOverall, I would suggest renaming these metrics. They likely correlate with the values they are described as, but it is overly presumptuous to label them this way as there are many other factors. More direct names (e.g. complexity -> self-accuracy or something like this) might be more accurate, leaving discussion of factors affecting these values (like complexity) to the discussion\nTradeoffs (Figure 2) are only shown in terms of temperature, which may be a confounding factor. It would be good to show other curves, e.g. for values of top-p, because it is not clear if these tradeoffs may have to do specifically with the specific warping effect that temperature has on sampling distributions. Alternatively, being more precise in the paper text, that these are tradeoffs over temperature as the variable, rather than general tradeoffs.\nQuestions:\nHave you tried variables besides temperature to test the tradeoffs?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n8: accept, good paper\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 10:17Everyone", "Content": "Comment:\nWe thank the reviewer for their insightful questions, which we address below. We are delighted to read that they find our work to be sorely needed, well-written and our results to be informative and interesting.\nQ: What is the influence of style shift or poor generalization of the classifier on the faithfulness and complexity metrics?\nThey are an essential part of the proposed metrics for these characteristics. Regarding faithfulness, we specifically chose this name rather than correctness, to indicate that the classifier accuracy is not only dependent on the correctness of a label, but also whether or not the associated sample is faithful or in line with reference data samples. A lower faithfulness can therefore be caused by either incorrect labels or samples that are so far from reference samples that they cannot be classified correctly. While poor generalization could impact the measured faithfulness values, this issue would uniformly affect all models evaluated in our study, thereby not changing the outcomes or conclusions.\nRegarding complexity, we note that we introduce it as the complexity associated with the entire dataset rather than the complexity associated with specific samples. Simple datasets have a higher generalization error which is a problem due to the high variety in real-world samples. While complex or difficult samples is one reason for an increased complexity of a dataset, it could for example also be caused by a wider variety of samples.\nWe have now further emphasized these effects in Section 3 of the paper, where they are first introduced.\nQ: Can you show the tradeoffs against other variables besides temperature?\nGreat question!\nWhile our experiments include many variables (model size, model family, temperature, etc.) that are interesting, we use temperature for the plots as (i) we find it to be the main driver of the trade-offs, and (ii) it is a continuous variable and therefore lends itself to plotting.\nThe only other continuous variable is model size via parameter count, which we include in the new Appendix D where we find that the observed tradeoffs are the same when the dependent variable is model size instead of temperature. This shows that the observed tradeoffs are not specifically due to a warping effect on the output distribution introduced by changing the temperature.\nFurther, we find that categorical variables reveal key-tradeoffs: Comparing vanilla and instruction-tuned models on the (existing temperature-base) plots, we find that instruction-tuned models have lower diversity and higher faithfulness for the same temperature. However, the plots show that when compensating for temperatures, instruction-tuned models generally perform better on this tradeoff as they are more at the top right of the plot. Similarly, we find that Falcon-7b performs worse on both the diversity vs faithfulness and diversity vs conformity tradeoffs, indicating that it is not such a powerful model.\nAdditionally, for all open-source and GPT3-175B models and for each data domain, we now also generated datasets using nucleus sampling with a parameter of 0.9 and added them to all relevant figures in the paper, i.e., Figure 2, Figure 5 and Figure 6. As can be seen there, this extra point follows all the curves formed by the temperature and therefore supports the conclusions made in the paper.\nWe hope to have been able to address all the reviewers\u2019 concerns, are happy to answer any follow-up questions they might have, and are looking forward to their reply."}, {"Heading": "Official Comment by Reviewer PKWU", "Subheading": "Official CommentbyReviewer PKWU22 Nov 2023, 23:39Everyone", "Content": "Comment:\nThank you for a comprehensive response, this answers my questions."}]}, {"Heading": "Official Review of Submission9477 by Reviewer wzFz", "Subheading": "Official ReviewbyReviewer wzFz30 Oct 2023, 14:36 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper studies the text generation capabilities of various large language models, proprietary and open, instruction-tuned and vanilla, by evaluating synthetic datasets generated from them. The datasets are evaluated in terms of\ndiversity in vocabulary\ncomplexity, or difficulty in modeling them given by the performance of a model trained and evaluated in-distribution.\nBy comparing the generated datasets to existing (reference) datasets in similar tasks and domains, they are also evaluated in terms of\n3) faithfulness, given by the performance of models trained on the reference datasets and evaluated on the generated ones\n4) conformity, given by a measure of distributional similarity between the reference and generated datasets\n5) performance, given by the performance of models trained on the generated datasets and evaluated on the reference datasets\nBased on this evaluation framework, the paper discusses the tradeoffs between these aspects of generation quality, how they change across model families, and how instruction tuning affects these tradeoffs.\nSoundness:\n4 excellent\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nThe evaluation framework is sensible and analyzing the capabilities of language models in terms of the tradeoffs between various aspects of generation quality is quite informative. The results of studying the effect of model size, the impact of instruction tuning, and that of the level of instruction tuning can potentially inform how to finetune future versions of language models.\nWeaknesses:\nThis study has some missing details, several limitations, and potential confounders not accounted for in the experiments.\nMissing details\nMD1:The evaluation is done over four classification datasets, but the actual details of the tasks are missing in Section 4. Particularly for AGNews and ELI5, it is unclear what is being classified After reading the Appendix, the AGNews task seems to be some news genre classification, and the ELI5 task seems to be subreddit classification (maybe it should just be called \"subreddit classification\"?) This issue can easily be fixed by including explicit details in Section 4.\nMD2: The motivation behind the chosen evaluation metrics is somewhat unclear. Particularly, faithfulness, conformity, and performance seem to be measuring the difference between the generated and reference data distributions. Why do we need these three variants? Relatedly, one would expect these metrics to correlate highly with each other. Analyzing this further would be helpful.\nLimitations and potential confounders\nL1: It is unclear how noise in the datasets (due to inaccurate labels) affects the trends seen in tradeoffs. For example, is the increase in diversity beyond the the conformity threshold in Fig 2 simply be due to noise? Having humans classify (subsets of) the generated datasets, and introducing the accuracy of the synthetic datasets as an additional metric could make this clearer.\nL2: The biases in the reference datasets could also be affecting conformity, faithfulness and performance. It might help to include multiple reference datasets per domain-task combination to evaluate whether the trends hold across them.\nL3: It is possible that the models used for generating datasets have seen the reference datasets either during pretraining or instruction-tuning. This would inflate the quality measures according to conformity, faithfulness, and performance. This issue cannot be dealt with directly, but it would help to check the zero-shot performance of the large language models on the reference datasets, and take it int account while inferring the tradeoffs.\nQuestions:\nIt would be helpful to put the reported diversity and complexity values in context. What are these values for the reference datsets?\nCan you elaborate on the motivation behind the three metrics comparing generated and reference datasets (see MD2)?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 10:16Everyone", "Content": "Comment:\nWe thank the reviewer for their insightful questions, which we address below. We are delighted to read that they find our work to be a useful framework and appreciate our empirical study.\nQ: Can you include specific details of the classification tasks in Section 4?\nWe have now specified what purpose each classification task has in Section 4.\nQ: Why do we need complexity, faithfulness and performance?\nThese three characteristics are all measured as an accuracy of a classifier trained and evaluated on different data. Intuitively, these metrics measure three separate and interesting characteristics of the datasets: faithfulness for how correct the generated samples are, complexity for how difficult it is to fit a model on the generated dataset (and therefore how complex it is) and performance for the final performance of the dataset on the downstream task. Furthermore, as discussed in section 4.1, the interaction between these metrics is quite interesting and shows interesting tradeoffs that exist between them.\nFirst, faithfulness and complexity are very highly correlated as noted in the third tradeoff we discuss in Section 4.1. The high correlation between faithfulness and complexity is a consequence of their interaction. In an ideal scenario, the model trained on the reference dataset would be the exact same as the one trained on the synthetic dataset. Thus, faithfulness would be exactly equal to $1 - \\text{complexity}$. Therefore, the interesting aspect of this behavior relates less to the tradeoff itself, but more to the deviations from this ideal scenario, as discussed in the paper. A specific dataset can have a lower complexity than the one expected as the ideal outcome, indicating that the resulting data is quite simple for the amount of faithful samples it contains.\nSecond, the dependence of the downstream performance on faithfulness and complexity is solely through their sum. This is caused by the above discussed tradeoff and now more thoroughly discussed in Section 4.1.\nQ: How does the noise in the generated datasets affect the results? Can it have an influence on the diversity/conformity tradeoff?\nThe key problem of synthetic datasets is their inherent noise, which is crucial for understanding and analyzing them. We have designed specific characteristics in our approach to identify different types of noise in these datasets. For example, faithfulness is a measure of noise resulting from incorrect labeling, whereas conformity measures noise caused by shifts in style and tone.\nOur findings, particularly the tradeoff between diversity and conformity, show the interplay of different noise types within the dataset. When diversity is low, the difference in style between the synthetic and reference datasets is a consequence of a too narrow generation of the data distribution and results in low conformity. On the other hand, high diversity often leads to the inclusion of irrelevant examples, which contributes to the style shift and therefore results in a low conformity. This interaction results in a quadratic relationship between diversity and conformity in the generation process.\nQ: Why did you not include several reference datasets for each data domain to study the effects of the biases on conformity, faithfulness and performance?\nWhile biases in the reference datasets could affect these three metrics, we do not believe this has significant influences on the conclusions drawn in the paper. We chose four common data domains with a clearly defined style and purpose. The chosen datasets are widely known and form a representative sample of the specific data domain and while it is possible that several nuances are not captured as well as they could, it is unlikely that these affect the classification accuracy in a significant way.\nFurthermore,  the conclusions drawn in the paper hold over all data domains and for all models, indicating that even if there were biases, they are consistent throughout the generation process and do not influence our characteristics. The conclusions drawn from the data have an intuitive explanation, e.g., the better conformity and diversity of vanilla models with respect to their instruction-tuned counterparts. This also indicates that biases do not play a significant role in the generation process.\nQ: What would the influence of models having seen these datasets during training be?\nIt is likely that parts of these datasets were included in the training data of the models. This is intentional, as we want to measure how effectively the models have learned the distribution associated with these common data domains. This approach enables us to evaluate and compare the performance of both standard (vanilla) and instruction-tuned models. Additionally, it helps us to identify and analyze the changes in style that result from instruction tuning and RLHF."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 10:17Everyone", "Content": "Comment:\nQ: Can you provide the values for the reported diversity and complexity values for the reference datasets?\nWe added the values of the reference data in Figure 2 and Table 2 and included the metrics for each dataset separately in Table 9 in Appendix E.\nWe hope to have been able to address all the reviewers\u2019 concerns, are happy to answer any follow-up questions they might have, and are looking forward to their reply."}]}, {"Heading": "Official Review of Submission9477 by Reviewer fcAm", "Subheading": "Official ReviewbyReviewer fcAm22 Oct 2023, 16:34 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis work studies the quality of synthetic data generated by LLMs. The major contribution of this work is proposing a framework to evaluate LLM's ability to generate synthetic data for specific tasks, and compare behavior across different LLMs. The evaluation framework consists of five different axes: performance, complexity, conformity, diversity and faithfulness. These properties are either evaluated using accuracy-based metrics, or modified version of existing tools (e.g., distict-n, mauve, etc.). Using this framework, this work compares LLMs with different size, from different model families and with or without instruction tuning. The empirical study reveals interesting tradeoffs among the five axes, and also report general performance trends on overall performance.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nGenerating synthetic datasets is a very popular application of LLMs. This work provides a useful framework on evaluating this ability of LLMs.\nThe empirical study shows interesting tradeoff from the models, and the reported performance trends can be useful for related applications.\nWeaknesses:\nI like the general idea of the proposed evaluation framework, but my biggest concern about this framework is the heavy use of DistilBERT accuracies in the evaluation framework. For the faithfulness metric, the framework is evaluating the performance of DistilBERT on the generated dataset. This confounds faithfulness with the difficulty (or complexity) of the dataset. This makes some of the finding questionable. For example, is there really a tradeoff between faithfulness and diversity/complexity, or is this correlation comes from the correlation between difficulty and diversity/complexity? I wonder if the authors can provide gold evaluation results for the DistilBERT models.\nThis study only focuses on synthetic data generation for relatively simple classification tasks. It would be great if this work can include evaluation on some more complex tasks.\nWhile this paper proposes four other properties addition to the performance. There is not much discussion on the relationship between these properties and the final performance. So while this study show many interesting findings, it is unclear what users should do besides checking the performance rankings.\nQuestions:\nFor the value k in the diversity metric, are you keeping the example size the same, or the token size same?\nHow do design or select prompts for the study conducted in your paper? Have you checked the sensitivity of the findings with respect to different prompts?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 10:15Everyone", "Content": "Comment:\nWe thank the reviewer for their insightful questions, which we address below. We are delighted to read that they find our work to be a useful framework and appreciate our empirical study.\nQ: Is there a tradeoff between faithfulness and diversity/complexity, or does this correlation come from the correlation between difficulty and diversity/complexity?\nWe can reason about the potential effect of difficulty as a potential confounding variable to validate that these tradeoffs are not caused by this behavior.\nRegarding the tradeoff between faithfulness and diversity we note that when we increase the model's temperature to boost diversity, it starts choosing less likely words. This could make the output more difficult for a classifier to label correctly, therefore causing an increased difficulty to potentially reduce faithfulness. However, it is much more likely that the output becomes unfaithful because choosing less likely words doesn't necessarily equal creating complex or difficult content. It is more about straying from the expected or accurate output. The observation that this tradeoff has a similar slope across all vanilla models further supports this argument, since one would expect the slope to be significantly different for smaller models, which are less likely to generate correct but difficult samples.\nWhen we consider the tradeoff between faithfulness and complexity, we note that in a perfect scenario, a model that is trained on synthetic data should perform identically to one trained on a reference dataset. In such a case, the relationship between faithfulness and complexity would be straightforward: faithfulness would be the inverse of complexity. Therefore, what is really important is how actual model performance deviates from this ideal. For example, if a dataset results in a lower complexity than what this ideal relationship predicts, it implies that the data generated is simpler than expected, given its level of faithfulness. A difficult sample, being defined as a sample example that is misclassified by both classifiers, is irrelevant for this comparison since the inclusion or exclusion of this sample would not change the deviation from the ideal scenario.\nQ: Can you provide gold evaluation results for the DistilBERT models?\nWe included the values of the reference data in Figure 2 and Table 2 and included the metrics for each dataset separately in Table 9 in Appendix E.\nQ: Why did you not include some more complex tasks?\nWe specifically selected our tasks to evaluate the data generation capabilities of language models on common data domains. This allowed us to investigate the difference between the modeling capabilities of vanilla models and instruction-tuned models. Dataset generation for more complex tasks would significantly complicate the evaluation, since they typically entail advanced prompting techniques, and instruction-following capabilities would become essential for generating faithful samples. We also note that we use our metrics as a proxy to measure the data quality in general and not necessarily just for the downstream task of classification.\nQ: What are the recommendations for practitioners to use LLMs for training data generation?\nPlease see Q1 of our main reply.\nQ: For the value $k$ in the diversity metric, are you keeping the example size the same, or the token size the same?\nWe are keeping the token size the same. Since samples generated by different models might on average differ in size, keeping the sample size the same is not enough to compensate for the size dependency of the used metric."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 10:15Everyone", "Content": "Comment:\nQ: How do you design or select prompts for the study conducted in your paper? Have you checked the sensitivity of the findings with respect to different prompts?\nOur (very simple) prompts were selected after ensuring that each model could follow the instructions and generate reasonable completions by manual inspection. This is done by a simple sanity check. Specifically, we generated a number of samples (typically around five) using the prompt and if all of these remained \"on topic\", we then used that prompt.\nWe note that the goal of prompts is different for dataset generation than in other areas of machine learning, such as Q&A and reasoning. While in these areas, there is only a single correct answer and the prompt needs to be optimized for that, in data generation the purpose of the prompt is to guide the language model towards the correct distribution. While different prompts could result in different distributions, we do not believe this influences any of our results. This claim is validated by the fact that our conclusions hold over multiple model families and models. If our conclusions were sensitive to the specific prompt used, one would expect that the use of different models, which could behave differently under the same prompt, would result in different conclusions. Furthermore, we note that the cost and time associated with generating these datasets is high, with dataset generation for a single model taking several days on an H100 GPU, it is thus not feasible to perform extensive multi-model prompt tuning.\nWe hope to have been able to address all the reviewers\u2019 concerns, are happy to answer any follow-up questions they might have, and are looking forward to their reply."}]}, {"Heading": "Official Review of Submission9477 by Reviewer Afrd", "Subheading": "Official ReviewbyReviewer Afrd12 Oct 2023, 19:35 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper examines the generation of text datasets using Large Language Models (LLMs) with a focus on distributional metrics like data diversity and faithfulness. It reveals trade-offs between these metrics across different LLMs and training methods, highlighting the impact of popular instruction-tuning techniques on LLM text generation abilities.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThe studied task on using LLMs for data generation is interesting and can be useful for the research community.\nThe authors conduct experiments on various datasets and LLMs (including both open-sourced and close-sourced models).\nThe paper is overall easy to read.\nWeaknesses:\nThe authors only consider the most simple prompts for the target tasks. However, there are several works that aim to improve the quality of prompts to yield higher-quality datasets, some examples include:\nChung et al. \"Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions.\" ACL 2023.\nYu et al. \"Large language model as attributed training data generator: A tale of diversity and bias.\" NeurIPS D&B Track, 2023.\nIt is also important to note that some dimensions (e.g. diversity) have already been studied in this work. As a result, some of the conclusions in this paper are already known and there are not many new insights about using LLMs for data generation.\nUnsupported Claims. The paper raises a claim that \"reinforcement learning with human feedback (RLHF) in ChatGPT leads to a significant degradation in synthetic dataset generation capabilities.\" However, the paper lacks a clear explanation of how the authors attribute this performance drop specifically to RLHF. A more detailed description of the experimental setup and results related to this assertion would enhance the paper's clarity and credibility.\nIn the main paper, the author only considers the average performance over different patterns, which can be less informative as different datasets show diverse patterns (according to Figure 5).\nFor the metrics, it is somehow not clear why using\nunique number of tokens\nas the metrics of Diversity.\nQuestions:\nCould you elaborate on why this paper primarily relies on simple prompts for target tasks, especially when recent research has emphasized advanced prompt engineering techniques for improving dataset quality? How might incorporating more sophisticated prompts affect the study's outcomes?\nGiven that some dimensions, like diversity, have already been studied in this work, what new insights or contributions does this paper bring to the field of using LLMs for data generation?\nIn the paper, you assert that \"reinforcement learning with human feedback (RLHF) in ChatGPT leads to a significant degradation in synthetic dataset generation capabilities.\" Could you provide a more detailed explanation of the experimental design and results that support this claim?\nWhat conclusions can be made after your study? What are the recommendations for practitioners to use LLMs for training data generation? Currently, it is not very clear after reading this paper, so I feel readers will not benefit much from this paper.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 10:10Everyone", "Content": "Comment:\nWe thank the reviewer for their insightful questions, which we address below. We hope that our reply will be able to address their reservations and lead to a more positive view on our work.\nQ: Could you elaborate on why this paper primarily relies on simple prompts over sophisticated prompt engineering?\nThe goal of our paper is to analyze the behavior and innate text generation capabilities of language models. We believe dataset generation provides an alternative to the more standard perplexity and loss measures for this capability, as it enables us to investigate how well the language model can model common distributions present in the training data rather than focusing on individual samples. The purpose of the prompt is therefore only to guide the language model towards the distribution of the data domain under investigation. While more advanced prompt engineering would create the possibility of generating datasets with a higher performance, it would also make it impossible to compare vanilla models with their instruction-tuned variant since prompt engineering techniques work differently with instruction-tuned models and it would therefore not allow us to measure the distributional shift between these models. To investigate more foundational characteristics, we thus focus on the relatively unprompted, direct model distribution, not a highly prompt-conditioned, specific model variant.\nQ: Why are you using a unique number of tokens as the metric diversity?\nWe experimented with several metrics to see what the best metric for measuring diversity was. Two of those metrics were Self-BLEU (Zhu et al., 2018) and average pairwise sentence similarity (e.g., used in Yu et al., 2023). We found that Self-BLEU was highly correlated with the unnormalized version of our diversity metric, but took a lot longer to compute. Pairwise sentence similarity did not seem to capture diversity of the entire dataset as it has several modes of collapse based on the fact that it only looks at a dataset by comparing two sentences at a time. One simple example is a dataset that contains only two unique sentences which are repeated. In this case, Self-BLEU and our used metric correctly indicate a diversity close to 0, while average sentence similarity will measure a very high diversity if the two sentences are dissimilar.\nQ:  Given that some dimensions, like diversity, have already been studied in previous work, what are your novel insights or contributions?\nIn contrast to prior work, which uses dataset generation to get optimal performance for a downstream task, we use it as a tool to evaluate language models and compare the differences between them. Our framework allows us to evaluate the models along five dimensions and compare a wide range of models in terms of innate dataset generation capabilities.\nWe find and discuss several tradeoffs between the evaluated dimensions and analyze the differences between model families and training paradigm. For example, the fact that Llama-2 only increases performance with respect to Llama on the conformity metric, indicates that while Llama-2 has a better understanding of generated text and can generate samples more similar to our reference datasets, it does not improve its understanding of the actual outputs, since its faithfulness remains the same compared to Llama.\nFurther, we find that there are significant differences between vanilla models and instruction-tuned variants. We show that while instruction-tuned models are more faithful, a trait they are specifically trained for, they exhibit less conformity and diversity, indicating that this training paradigm might not be optimal for dataset generation. We can explain this by noting that fine-tuning happens on specific tasks and instructions which are not indicative of data usually found on the internet. Therefore, the fine-tuning stage biases the model to output data that is more formal and grammatically correct than what appears in human-generated datasets."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 10:13Everyone", "Content": "Comment:\nQ: Can you provide a more detailed explanation of the experimental design and results that support RLHF negatively impacting ChatGPTs text generation capabilities?\nWhen comparing ChatGPT (InstructGPT-3.5-175B$_\\text{chat}$) with all other models evaluated in the study, we find that the downstream performance of datasets generated by ChatGPT is among the lowest, only the smallest models exhibit a worse performance. This fact shows that the innate dataset generation capabilities for ChatGPT are worse than for other models. When comparing the various metrics, we find that ChatGPT generates incredibly faithful datasets, but these datasets are also very simple, not diverse and non-conform. Since the RLHF phase optimizes the model to answer questions politely, correctly and without grammatical mistakes, it only generates data that abides by these styles. While one could construct more complicated prompts to adjust for this, it would require the user to convey the distribution of the data domains in specific instructions which is error prone and difficult.\nIn order to clarify this point, we changed our statement to \u201cWe further find that ChatGPT (InstructGPT-3.5-175B$_\\text{chat}$) generates very faithful datasets, but lacks in all other models in terms of complexity, diversity and conformity resulting in a worse downstream performance compared to other models.\u201d\nQ: What are the recommendations for practitioners to use LLMs for training data generation?\nPlease see Q1 of our main reply.\nWe hope to have been able to address all the reviewers\u2019 concerns, are happy to answer any follow-up questions they might have, and are looking forward to their reply."}, {"Heading": "Thank you for the detailed response", "Subheading": "Official CommentbyReviewer Afrd21 Nov 2023, 16:10 (modified: 21 Nov 2023, 16:12)EveryoneRevisions", "Content": "Comment:\nThe reviewer appreciates the author's response. While I appreciate the explanation provided, I find it somewhat unconvincing for several reasons:\nI have taken a look at Table 5, and found the prompts used for different models are not the same. This may introduce additional noise in evaluation. Besides, the authors only conducted experiments with a single set of prompt templates/verbalizer, which made the evaluation less convincing, as these templates can have a huge impact on the quality of the generated dataset.\nFor the prompt engineering techniques, the response underestimates the potential of sophisticated prompt engineering. Advanced prompts can be designed to guide models towards specific data distributions. The argument that advanced prompts would make comparisons impossible seems overly simplistic.\nAbout the diversity metric, maybe my understanding is wrong, but this metric seems to be biased toward longer sequences. Besides, the faithfulness, conformity, and complexity all rely on additional language models for calculating the results. Not sure if the result is robust to selection of the model.\nFor the discussion of the insights, I feel the claim \u201cWe show that while instruction-tuned models are more faithful, a trait they are specifically trained for, they exhibit less conformity and diversity, indicating that this training paradigm might not be optimal for dataset generation.\u201d Is overclaimed and may not be true in practice. From the experiment you run, the only conclusion you can draw is \u201cthe instruction-tuned model does not work well with the specific prompt used in the experiments\u201d.  For instance, instruction-tuned models may excel with more complex prompts and benchmarks like GLUE/SuperGLUE, which should be considered in a broader assessment. It is not appropriate to dismiss other models without providing comprehensive results.\nThe recommendation for practitioners seems to be very hand-waving and not \u201cpractical\u201d at all. First, the statement, \"While ChatGPT is a popular choice, it may not always be the most suitable,\" lacks specificity and could apply to any model (e.g., LLama2, Vicuna). A more informative statement might be, \"While ChatGPT is a popular choice, it may not always be the most suitable when considering the simple prompts used in our paper.\" Second, the claim that \u201ca combination of different LLMs could increase performance and mitigate problems or biases related to a specific model\u201d is also not informative. Is there any result in the paper about the performance of mixing these different generated data together? Third, the recommendation to \"test different sampling temperatures for optimal performance\" has been previously mentioned in [Chung et al., 2023], and it may not be practical to heavily tune this parameter within the strict few-shot/zero-shot setting [Bragg et al., 2021]. Therefore, it may be worth reconsidering the practicality and impact of this recommendation in real-world scenarios.\nReference:\nJohn Chung, Ece Kamar, and Saleema Amershi. 2023.\u00a0Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions. In\u00a0Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 575\u2013593, Toronto, Canada. Association for Computational Linguistics.\nBragg, Jonathan, et al. \"Flex: Unifying evaluation for few-shot nlp.\"\u00a0Advances in Neural Information Processing Systems\u00a034 (2021): 15787-15800."}]}]}, "SMZnJtkNX5": {"paper_info": {"Keywords": "Spiking neural networks, High-performance computing, GPU acceleration", "Abstract": "Inspired by neurobiological structures, Spiking Neural Networks (SNNs) are heralded as a significant advancement in deep learning, given their potential for superior computational efficiency. However, this potential often remains untapped on contemporary hardware platforms. Specifically, when deployed on standard GPUs, SNNs tend to require extended computation times, placing them at a disadvantage compared to traditional Artificial Neural Networks (ANNs). Such inefficiencies have somehow diminished enthusiasm for SNN research and presented the tangible challenge to achieving scalability. To address such a challenge, this study introduces a temporal parallelization method specifically tailored for accelerating the propagation dynamics of SNNs on GPUs. Furthermore, we furnish two distinct implementations\\footnote{The source code will be made publicly available.} based on the CUDA and JAX frameworks respectively, ensuring adaptability across both single and multi-GPU setups. When benchmarked against several established SNN implementations, the empirical analysis confirmed the efficacy of our proposed method. Notably, with the Leaky Integrate-and-Fire model as a test case, the CUDA-based implementation achieved5\u00d7to40\u00d7acceleration on the A100 GPU.", "Supplementary Material": "pdf", "Primary Area": "infrastructure, software libraries, hardware, etc.", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9476", "PDF Url": "https://openreview.net/pdf?id=SMZnJtkNX5"}, "review_info": [{"Heading": "Official Review of Submission9476 by Reviewer 81ci", "Subheading": "Official ReviewbyReviewer 81ci31 Oct 2023, 13:18 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe authors describe a method and code for accelerating spiking neural\nnetworks (SNNs) on GPUs. They first claim to parallelize the temporal\nmembrane integration of a layer in an SNN and secondly divide the\ncompute onto multiple GPUs. They provide a template how to implement\nit in common ML frameworks such as JAX. Finally, they show that their\nimplementation outperforms other toolboxes.\nSoundness:\n1 poor\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe implementation seems to outperform current toolboxes in terms of runtime.  The authors show that this layer-first\napproach gives a considerable speedup on GPUs due to reduced memory movement.\nWeaknesses:\nWhile better implementation of simulating SNNs using GPUs has its merits,\nthe task is merely a software\nengineering task. The paper does not add any value in terms of novel\ninsights. This is in particular true since the \"temporal\nparallization\" argumentation is indeed a misnomer as the temporal dimension is\nnot\ncomputed in parallel, but instead time of one layer is simply handled\nwithin one GPU kernel (but still computed sequentially if I understand it correctly).\nIf one wanted to design a custom CUDA kernel and would assume that\nonly feed-forward layers are allowed, this would be just the standard\napproach to do, I don't see any innovative aspects here. In\nparticular, equation 3 is just a re-writing (inserting) of\nx\n(\nt\n\u2212\n1\n,\nn\n)\n,\nthere is no \"transformation\" I can see. Note that\nv\ni\n(\nt\n,\nn\n)\nstill\nis a function of previous times,\nv\ni\n(\nt\n\u2212\n1\n,\nn\n)\n. All what is done is to\ncompute all time steps per layer first before sending the full output\nspike train to the next layer. This will obviously not work for\nrecurrent SNNs.\nAlso, the authors do not even provide their own optimized CUDA kernel\n(which would have more merit), but instead rely on generic toolboxes\nlike JAX. The code listings do not provide any details of the\nimplementation and are more like a tutorial how to use it.\nOverall, while the implementation might be useful as it improves the\nruntime of SNNs compared to the (apparently very non-optimized)\nstandard SNN packages, the paper does not provide any new scientific\ninsights. It is also not discussed that the approach works only for\nfeed-forward SNNs. Moreover, the presentation of \"temporal\nparallelization\" is not correct (as it just points to a fused\nsequential CUDA-kernel). Finally, the layer-first approach (fusing\nkernels to reducing memory operations) and dividing the compute for\nmultiple GPUs are rather standard practices in GPU programming in\ngeneral and not novel enough for a research oriented conference\ncontribution in my opinion.\nQuestions:\nIn Eq 3:\nx\ni\n(\nt\n,\nn\n)\nshould be\nx\ni\n(\nt\n,\nn\n\u2212\n1\n)\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9476 by Reviewer 4kmC", "Subheading": "Official ReviewbyReviewer 4kmC31 Oct 2023, 04:49 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper trys to use a temporal parallelization method to accelerate the propagation dynamics of SNNs on GPUs. The feature it claims is a cross-timestamp acceleration of LIF model. With the Leaky Integrate- and-Fire model as a test case, the CUDA-based implementation achieved 5\u00d7 to 40\u00d7 acceleration on the A100 GPU.\nSoundness:\n2 fair\nPresentation:\n1 poor\nContribution:\n2 fair\nStrengths:\nThe author proposed temporal parallelisation method tailored for universal SNN units on single and multiple GPUs. It supports both CUDA and JAX frameworks.\nWeaknesses:\nThe motivation behind this paper lacks clarity. Spiking Neural Networks (SNNs) are not typically intended for deployment on GPUs, meaning that a GPU is not the most suitable platform for SNN deployment. Without a demonstration of the clear benefits of utilizing GPUs for SNNs deployment as opposed to other platforms, the paper's underlying motivation remains unconvincing.\nHow does this paper leverage GPU to implement true spiking mechanism? It is not clear or discussed. Is it only considering simulating the mechanism of the Leaky-Integrate-and-Fire model behavior? Plus, there\u2019s no true spiking signals in GPU, addressing the temporal information is not really Spiking implementation. This paper doesn\u2019t clarify the basic concept. \n(In some sense, parallelizing temporal information is possible, but there\u2019s conversion between spiking temporal information and the muti-bit digital temporal information for GPU? Then what\u2019s the conversion cost?)\nAlthough this paper is based on the computational model of LIF, but it does not clearly describe how training and inference is done, respectively. Training an SNN is hard, and it is not discussed at all in this paper, so it\u2019s only about inference, or even, the simulation of inference?\nLast but not least, most importantly, this paper does not provide any AI-model based results, such as accuracy, performance, respective speed-up, etc, let alone thorough analysis based on the comparison of results. The only result is a table based on a single-layer toy model? For multi-GPU, where is Fig. 4, seems this paper is incomplete?\nQuestions:\nHow SNN neuron spiking behaviour described in Eq.1 and Eq. 2 reflected in GPU?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9476 by Reviewer VQqA", "Subheading": "Official ReviewbyReviewer VQqA29 Oct 2023, 22:50 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper presents an SNN-based acceleration strategy with parallelized temporal computation that supports both single and multiple GPUs.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nLargely improved SNN inference speed compared to the previous implementation.\nCompatibility with both single and multi-GPU processing.\nWeaknesses:\nW1:\nFigure 4 is missing.\nW2:\nThe biggest bottleneck of this work is that the accuracy benchmarking is completely missing in the paper. I understand the inference speed-up is very important in SNN, but I cannot see the reason why the paper chose not to report the accuracy. It is important to verify the proposed implementation with different SNN model architectures. E.g.. ResNet vs. VGG.\nW3:\nIt seems like the implementation can only accelerate the inference rather than training, which I think is not powerful enough.\nQuestions:\nPlease refer to the Weakness.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9476 by Reviewer yZKs", "Subheading": "Official ReviewbyReviewer yZKs26 Oct 2023, 22:41 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposes a temporal parallelization method for SNNs that can accelerate SNNs on both single or multi GPUs with up to 40x acceleration.\nSoundness:\n3 good\nPresentation:\n1 poor\nContribution:\n2 fair\nStrengths:\nThe training of deep SNNs requires much more time and memory consumption. Thus, it is meaningful to explore the acceleration of simulating SNNs on GPUs.\nWeaknesses:\nThe details of the proposed method are not described clearly in this paper. To make matters worse, the Supplementary Material is the same as the main paper.\nQuestions:\nIn Figure 3, how the propagation of the spiking neuron layer is paralleled? I assume that V[t] is still computed in serial. For an input sequence with length T, the time complexity is still O(T).\nIn section 3.2, the authors claim that the SNN accelerated by pipeline in multiple GPUs may have a faster speed than using a single GPU. However, I am afraid that the communication time between GPUs will be the bottleneck. According to my experience, the communication time is much longer than any other time. Thus, the pipeline method is seldom used in training, and the Distributed Data Parallel is the mainstream.\nIn Table 1, the time of SpikingJelly with or without CuPy does not have much difference, which is against my experience.\nWhere is Figure 4?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\nCode Of Conduct:\nYes"}]}, "6AtXCnHCFy": {"paper_info": {"Keywords": "Fault diagnosis, Deep learning, CNN, Domain Generalization, Load-domain Domain Generalization", "TL;DR": "we introduce a novel domain generalization, Load-Domain domain generalization,and for which we propose Feature Shift Network.", "Abstract": "Conventional deep learning methods for fault detection often assume that the training and the testing sets share the same fault pattern spaces and domain spaces. However, some fault patterns are rare, and many real-world faults have not appeared in the training set. As a result, it\u2019s hard for the trained model to achieve desirable performance on the testing set. \nIn this paper, we introduce a novel domain generalization, Load-Domain (LD) domain generalization, which is based on the analysis of the CWRU bearing dataset and its domain division method. For this scenario, we propose a feature shift model called FSN (Feature Shift Network). In the bearing dataset, domains are divided based on different operating conditions which have specific loads, so it\u2019s equivalent to load-based domain division. Moreover, the domain label corresponds to the actual load magnitude, making it unique as it contains physical information, which can boost detection accuracy on unknown domain beyond the training set. According to the knowledge above , FSN is trained for feature shift on adjacent source domains, and finally shifts target domain features into adjacent source domain feature space to achieve the purpose of domain generalization.\nExtensive experiments on CWRU demonstrate that FSN is better than the existed models in the LD domain generalization case. Furthermore, we have another test on MNIST, which also shows FSN can achieve the best performance.", "Primary Area": "transfer learning, meta learning, and lifelong learning", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9472", "PDF Url": "https://openreview.net/pdf?id=6AtXCnHCFy"}, "review_info": [{"Heading": "Official Review of Submission9472 by Reviewer 2Hv6", "Subheading": "Official ReviewbyReviewer 2Hv608 Nov 2023, 11:33 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper targets a domain generalization problem for fault diagnosis of the bearing dataset and proposes a new model, called the feature shift network (FSN), to adjust the features between source and target domains. The numerical experiment using CWRU and MNIST datasets is conducted to evaluate the effectiveness of the proposed FSN.\nSoundness:\n2 fair\nPresentation:\n1 poor\nContribution:\n1 poor\nStrengths:\nThe motivation to exploit the additional information of the problem by assuming the specific task of fault diagnosis in bearing datasets is good.\nThe domain generalization problem treated in this paper is important.\nWeaknesses:\nThe paper is not well written and has a lot of unclear points. For example, the detailed setting, such as loss function and calculation of each model component, is omitted. It is hard to understand the technical novelty and advantages of the proposed method.\nI cannot find the formal definition of the load-domain (LD) generalization problem treated in this paper.\nIn Table 1, the performance gain of the proposed FSN variants is marginal.\nThe motivation of the evaluation using the MNIST dataset is unclear.\nQuestions:\nWhat does the \"Relation\" mean in Table 1?\nHow does the proposed FSN exploit the physical meaning of domain labels for model training?\nIs the target domain data accessible in the load-domain generalization? In general, the target domain data is not accessible in domain generalization.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9472 by Reviewer 4bLC", "Subheading": "Official ReviewbyReviewer 4bLC04 Nov 2023, 02:39 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nTraditional deep learning methods for fault detection usually assume that the training set and the test set share the same fault mode space and domain space. Based on the analysis of CWRU bearing data set and its domain division method, this manuscript proposes a Feature Shift model called FSN (Feature Shift Network) to improve the detection accuracy of unknown domain, which can divide domains according to different operating conditions with specific loads, and take advantage of the physical significance of domain labels.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\n1.This manuscript proposes the idea of \"exploitability\" of domain-related information, and it may be a point worth exploring further.\n2.This manuscript is written in a standard and clear hierarchy,  and the structure is easy to follow.\nWeaknesses:\n1.In this manuscript, a parameter with physical significance is used as a domain label, and then the related features of the domain are used to assist classification. But now that labels have physical meaning, what happens if they are input directly to the network with other data? We didn't see the related comparison experiments. So it is not  convincing.\n2.Few comparison experiments are conducted.\n3.The detailed design of the model, including the loss function, is not explained in sufficient detail.\nQuestions:\n1.Why label a domain with a piece of information that can be numerically and physically meaningful, and how is that different from feeding it directly into a neural network?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9472 by Reviewer NDUe", "Subheading": "Official ReviewbyReviewer NDUe02 Nov 2023, 10:43 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper aims to address the multi-source domain generalization problem by proposing a feature shift network (FSN). Distinct from traditional domain generalization, the problem of interest has additional information of inter-domain linear relations in the form of domain labels. By taking advantage of such information explicitly in the proposed model, superior performance can be achieved.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\n--The paper attempts to addresses the domain generalisation problem derived from a real-world application. The problem itself is somewhat novel and has not been extensively studied and hence solving such a problem is of great significance.\nWeaknesses:\n--The introduction section lacks essential information of problem definition, description of methods and brief experimental results. This makes it less readable to the readers. For example, it is not clear what the \"Load-Domain\" means and how the feature shift model handles the problem.\n--The section of related work is not well organized. More focus should have been put on the most closely related works (i.e. domain generalization in fault detection problems) rather than a broad review of fault detection methods. In addition, the relations between existing works and this work should also be discussed.\n--In table 1, Multi FSN does not perform the best as Multi DANN has a result of 83.3.\n--The authors fail to compare with SOTA domain generalization methods.\n--There exist language issues/typos/notation inconsistency in the manuscript. E.g., \"P(1), P(2), ..., P^{(K)}\"; \"edge distribution\" should be \"marginal distribution\"; \"1 data of source domain a+1 and 9 data...\"; \"This thesis DANN domain adaptive network...\"; \"the ERM said empirical ...\";\nQuestions:\nWhat is the loss function of the network?\nWhat is the input (images?) and the output of the model?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9472 by Reviewer xHmt", "Subheading": "Official ReviewbyReviewer xHmt01 Nov 2023, 14:15 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper introduces a special domain generalization scenario termed Load-Domain domain generalization and proposes a new model, the Feature Shift Network (FSN), tailored for this scenario. The authors conduct experiments on the CWRU bearing dataset and the MNIST dataset, comparing FSN with classical fault diagnosis methods and other domain generalization methods.\nSoundness:\n2 fair\nPresentation:\n1 poor\nContribution:\n2 fair\nStrengths:\nThe experimental results show good performance of FSN in certain scenarios and hence its potential for practical applications in fault diagnosis and domain generalization.\nWeaknesses:\nThe writing of the paper should be substantially improved. It reads like bad machine translation and has a lot of grammatical and terminological errors.\nThe theoretical foundation of the FSN model could be explained in more detail. The model architecture shown in Fig. 3 requires further clarification and motivation.\nExperiments are only conducted on two datasets, with limited baseline methods for comparison. It is unclear how the proposed method performs on other datasets that also have \"linear\" domain labels.\nThe accuracy values do not have confidence intervals.\nQuestions:\nSee above.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9472 by Reviewer u8DP", "Subheading": "Official ReviewbyReviewer u8DP01 Nov 2023, 03:54 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper introduces a novel approach called Load-Domain (LD) domain generalization for fault detection in situations where the training and testing sets have different fault pattern spaces and domain spaces. The authors propose a feature shift model called FSN (Feature Shift Network) specifically designed for LD domain generalization. The model is trained on adjacent source domains to learn feature shifts and then applies these shifts to target domain features, enabling generalization beyond the training set. The approach is validated through extensive experiments on the CWRU bearing dataset and the Rotation MNIST dataset, demonstrating superior performance compared to existing models in LD domain generalization scenarios.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe topic of Domain Generalization is highly relevant and of significant interest to the research community. Furthermore, the paper addresses the important aspect of leveraging domain information during the training process, which has gained increased attention in recent times.\nThe paper is well-written and easily comprehensible, effectively conveying its ideas and findings to the readers.\nWeaknesses:\nThe LD domain generalization problem setting addressed in the paper is acknowledged as a highly specialized case, which limits its broader contribution to the field.\nThe experimental results presented in the paper lack persuasiveness. Firstly, the dataset used is small and may not accurately represent real-world scenarios. Additionally, the chosen baselines are outdated, primarily predating 2018, despite the emergence of numerous domain generalization methods since then. It is recommended to refer to recent surveys for a comprehensive overview of the latest approaches, e.g., [1].\nWhile the main idea of the paper is generally understandable, there are instances where sentences may create misunderstandings, and insufficient definitions of the problem and task settings are found throughout the paper. Notably, the captions for Figure 1 may inaccurately describe Figure 1(b), potentially causing confusion among readers.\n[1] Domain Generalization: A Survey\nQuestions:\nPlease review the weakness section, and kindly correct any misunderstandings that may exist in my assessment.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9472 by Reviewer nQ8N", "Subheading": "Official ReviewbyReviewer nQ8N30 Oct 2023, 00:03 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nIn this paper, the authors introduce a new Load-Domain (LD) domain generalization setting, where the domain label corresponds to actual load magnitude. To serve the scenario, the authors propose a feature shift model (FSN) to learn feature mapping between adjacent domains according to the physical meaning in domain labels. Experiments are carried out on CWRU bearing dataset and rotated MINST datasets to showcase the performance.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe idea of employing the physical meaning of domain labels to achieve generalization on consecutive domains makes sense to me.\nThe new setting of LD domain generalization also seems suitable for the benchmark of CWRU dataset, where the physical meaning of domain labels can be clearly defined.\nWeaknesses:\nThe paper\u2019s writing and formatting are not good enough, making it sometimes hard to read and understand. There exist many formatting errors (e.g., the use of spaces in Sections 1 and 2, the math symbol consistency in Section 4.1: P(1) and P^{(K)}). The caption of Figure 1 is duplicated. The abbreviation in Paragraph 3, Section 2 might be RAP. In the second line of paragraph in section \u201cDistribution Law Conjecture\u201d, F_p should be the the mapping relation between features, not labels.\nBesides the ones from 1., the Experiment part (e.g., paragraph 4 in \u201cClassical Model Contrast\u201d part) is hard to understand. Bad formatting and discontinuous sentences make it unreadable to me.\nKey references are absent. In the first paragraph of Section 4.2, methods aligning the three types of distributions should be cited to justify the categorization. The compared methods in \u201cComparison with Classical Models\u201d should also be referred to, and the Li et al. (2018) reference in this paragraph is wrongly cited, this paper should be \u201cDeep Domain Generalization via Conditional Invariant Adversarial Networks\u201d.\nAs to the experimental results, the metric of Table 1 is not clearly stated. Also, the experimental details like backbone choice, learning rate, optimization schedules are not provided.\nThe setting is still too limited. The current method and experiments only focus on generalizing from highly relevant and sequential source domains to a target domain close to the last seen source domain. The performance of the proposed method should be further evaluated on broader settings where the relationship of source and target domains is not fixed. Also, the setting seems much relevant to that of Continuous Domain Generalization, which should be discussed.\nQuestions:\nApart from those in weakness, the authors are encouraged to experiment on larger domain generalization benchmarks. Moreover, the compared methods are not recent enough, therefore more experiments should be added to provide a more comprehensive comparison.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9472 by Reviewer gaFV", "Subheading": "Official ReviewbyReviewer gaFV28 Oct 2023, 01:11 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper proposes a feature shift network (FSN) for a new domain generalization task called load-domain (LD) generalization based on analyzing the CWRU-bearing dataset. The key idea is to leverage domain labels to shift target features into adjacent source domains. The method is evaluated on the CWRU and rotated MNIST datasets by comparing them to existing domain generalization techniques.\nSoundness:\n1 poor\nPresentation:\n2 fair\nContribution:\n1 poor\nStrengths:\nThe application of domain generalization on the problem of fault detection is novel.\nWeaknesses:\nThe paper lacks novelty as feature shift is explored in prior work. The theoretical analysis relies on unproven conjectures and lacks rigor. More comprehensive empirical evaluation on realistic datasets and comparisons to recent benchmarks are needed to demonstrate effectiveness.\nQuestions:\nThe novelty of the proposed feature shift network (FSN) is questionable given prior work on feature shift for domain generalization. The current paper does not sufficiently differentiate FSN from these existing methods.\nThe paper also lacks theoretical analysis regarding the generalization abilities of FSN.\nEmpirically, evaluation is limited to two small datasets, which cannot sufficiently demonstrate effectiveness and robustness for the load-domain generalization task. More extensive testing on diverse realistic datasets are needed.\nFlag For Ethics Review:\nNo ethics review needed.\nDetails Of Ethics Concerns:\nn/a\nRating:\n3: reject, not good enough\nConfidence:\n5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9472 by Reviewer hzsf", "Subheading": "Official ReviewbyReviewer hzsf25 Oct 2023, 15:12 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper addresses a challenge in deep learning-based fault detection, where real-world faults may not always appear in the training set. This limitation makes it difficult for conventional models to generalize well to unseen fault patterns. To address this, the authors introduce a domain generalization method, Load-Domain (LD) domain generalization, specifically designed based on the CWRU bearing dataset. In this method, the domains are divided based on different operating conditions that have specific loads, which correspond to the actual load magnitude. This inclusion of physical information helps enhance the model's accuracy for unknown domains. The authors propose the Feature Shift Network (FSN), which is trained to shift features between adjacent source domains and the target domain for better generalization. The effectiveness of FSN is shown through experiments on both the CWRU bearing dataset and the MNIST dataset, where FSN outperforms existing methods.\nSoundness:\n2 fair\nPresentation:\n4 excellent\nContribution:\n2 fair\nStrengths:\nThe authors tackle a crucial issue in fault detection where conventional models may not perform well on unseen fault patterns.\nThis novel domain generalization approach, based on real physical properties (load magnitude), can potentially be more representative and robust than abstract or purely data-driven domain divisions.\nThe model's applicability on both the CWRU bearing dataset and the MNIST dataset suggests it is versatile and not limited to one type of data.\nThe authors compare their model with classical fault diagnosis methods, showing its superiority in specific scenarios.\nThe paper appears to have a well-structured format, with sections dedicated to reviewing the current state of research, introducing their novel domain generalization method, and discussing experimental results.\nWeaknesses:\nThe assumption that domain label corresponds to actual load magnitude might not hold for all real-world scenarios. It may be beneficial to test scenarios where this is not the case.\nIntroducing domain-specific information like load magnitude could risk overfitting to specific domain characteristics. The generalization capability of the model in truly unseen domains is a concern.\nQuestions:\nHow does the Load-Domain (LD) domain generalization approach compare with other domain generalization methods that don't rely on physical information like load magnitude?\nAre there potential scenarios where the assumption of domain labels corresponding to actual load magnitude may not hold? How would FSN perform under such conditions?\nFlag For Ethics Review:\nNo ethics review needed.\nDetails Of Ethics Concerns:\nN/A\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}]}, "9ceadCJY4B": {"paper_info": {"Primary Area": "applications to neuroscience & cognitive science", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Large Language Models, Uncertainty, Evaluation, In-Context Learning, Alignment, Multi-round dialogue, Robustness", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "TL;DR": "This research reveals that Large Language Models like ChatGPT exhibit a decline in reliability when confronted with disturbances like follow-up questions, even if initial answers are correct.", "Abstract": "With the emergence of generative conversational large language models (LLMs) like ChatGPT, serving as virtual assistants in various fields, the stability and reliability of their responses have become crucial. However, during usage, it has been observed that these models tend to waver in their judgements when confronted with follow-up questions from users expressing skepticism or disagreement. In this work, we draw inspiration from questioning strategies in education and propose a \\textsc{Follow-up Questioning Mechanism} along with two evaluation metrics to assess the judgement consistency of LLMs before and after exposure to disturbances. We evaluate the judgement consistency of ChatGPT, PaLM2-Bison, and Vicuna-13B under this mechanism across eight reasoning benchmarks. Empirical results show that even when the initial answers are correct, judgement consistency sharply decreases when LLMs face disturbances such as questioning, negation, or misleading. Additionally, we study these models' judgement consistency under various settings (sampling temperature and prompts) to validate this issue further, observing the impact of prompt tone and conducting an in-depth error analysis for deeper behavioral insights. Furthermore, we also explore several prompting methods to mitigate this issue and demonstrate their effectiveness.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "Supplementary Material": "pdf", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9468", "PDF Url": "https://openreview.net/pdf?id=9ceadCJY4B"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9468 by Area Chair AYgM", "Subheading": "Meta ReviewbyArea Chair AYgM12 Dec 2023, 15:11 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThis paper draws inspiration from questioning strategies in education and proposes to use follow-up questions that express disagreements/doubts to challenge an LLM's response. The reviewers think that the paper is well-written and the experiments are comprehensive. However, the remaining weakness after rebuttal is the lack of novelty, compared with existing work such as Wang et al. 2023a. Although the authors added one sentence in the revised version, \"Despite some studies on the reliability of LLMs (Radhakrishnan et al., 2023; Wang et al., 2023a; Turpin et al., 2023), our mechanism is closer to the interactions that ordinary users might have with LLMs in real life and features a more comprehensive scenario setup, compared to their more academically oriented settings or methodologies\", I find it to be unsatisfactory by just saying existing work uses \"more academically oriented settings or methodologies\". A more detailed discussion on what existing work has done and how current work's contribution is significant given existing work is needed. Given this, I would recommend rejecting the paper, but would not mind if the paper gets accepted.\nJustification For Why Not Higher Score:\nGiven existing work mentioned above, the novelty and discoveries of this paper seem not significant. It mainly verifies the behaviors of LLMs reported in previous papers with more follow-up strategies (i.e., different prompts).\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "Response Summary", "Subheading": "Official CommentbyAuthors23 Nov 2023, 04:58Everyone", "Content": "Comment:\nWe thank the reviewers for their valuable suggestions and constructive comments. Following the reviewers' suggestions, we have revised our manuscript and submitted a new version. In the following, We summarize the primary responses and indicate the corresponding modifications in the paper. The revised parts in our paper are highlighted in blue color for easier review.\n\n- We discussed the novelty of our work and compared it with related works (from Reviewer AVRR weakness1) (refer to Related Work).\n- We added an evaluation of the latest and more capable models (from Reviewer bqxm weakness1 and question1) (refer to Appendix 3.4).\n- We introduced two new interference scenarios and assessed changes in judgement consistency under these new scenario disturbances (from Reviewer bqxm weakness2) (refer to Appendix 6).\n- We elaborated on potential causes and possible mitigation methods for issues identified in our work (from Reviewer AVRR weakness2) (refer to Conclusion and Appendix 10).\n- We discussed future research room and potential directions in this area (from Reviewer bqxm weakness1).\n- We explained how the evaluation mechanisms and mitigation methods proposed in our work can be integrated with real-world applications (from Reviewer bqxm weakness3 and question2).\n- We provided detailed explanations for aspects that confused the reviewers (from Reviewer 3h1U question1 and question2).\n- We added a table of contents in our paper to make the appendix more intuitive (refer to page14)."}, {"Heading": "Official Review of Submission9468 by Reviewer AVRR", "Subheading": "Official ReviewbyReviewer AVRR15 Nov 2023, 11:40 (modified: 15 Nov 2023, 14:50)EveryoneRevisions", "Content": "Summary:\nThis paper explores testing the judgment consistency of conversational LLMs (e.g., ChatGPT) by using follow-up questions that express disagreements/doubts and challenge the model's response. Across a range of reasoning benchmarks, the authors find that modern conversational LLMs (e.g., ChatGPT, PaLM2-Bison, Vicuna-13B) are vulnerable to such disturbances, changing their beliefs into wrong answers for a large portion of examples where they can generate correct initial solutions. The authors also experimented with different settings including sampling temperature and prompt choices, and found that despite occasional improvements, such an issue largely remains.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\n- The paper is overall well-written and easy to follow.\n- The experiments are quite comprehensive, covering a wide range of reasoning tasks and LLMs. The findings are also consistent across different models and tasks, suggesting that what's found in this paper is a rather systematic issue of current (conversational) LLMs.\n- The analysis of the impact of different settings & alternative prompt designs on the model behavior could be interesting and valuable to the community.\nWeaknesses:\n- The overall novelty of this work is a bit limited given that prior work (many of which are also cited by the authors) has investigated the \"sycophantic\" behavior of LLMs, and the proposed methods in the paper are quite similar to the ones in prior work. For example, the paper by [Turpin et al.] which the authors seem to miss studies LLM's behavior when there exists bias in the context, where one of the settings is exactly about putting human user's belief (in a wrong answer) in the context, which is close to the type L (leading questions) prompt explored in this paper. Similar findings are also present in [Perez et al., 2022] as cited. [Wang et al., 2023a] as cited explores using another conversational LLM conditioned on a wrong solution to engage in a debate with the original LLM; the \"follow-up\" responses by the simulated user there also share many similarities with the ones proposed (expressing disagreement, doubt, different opinions, etc.).\n- The qualitative analysis misses some rather important details such as the proportion of each error category. While there are some discussions/insights about the issue in the paper, overall, as an analysis/evaluation type work, I feel the contribution could be strengthened if more fruitful thoughts/speculations about the underlying cause of the observed issues (and potential ways of mitigating them) are included.\n\n\n[Turpin et al.] Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting. arXiv-23.\nQuestions:\nNone\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer AVRR (1/3)", "Subheading": "Official CommentbyAuthors21 Nov 2023, 09:58 (modified: 21 Nov 2023, 10:11)EveryoneRevisions", "Content": "Comment:\nThank you for the valuable feedback!\n\n>Q1 (from weakness 1): The overall novelty of this work is a bit limited given that prior work (many of which are also cited by the authors) has investigated the \"sycophantic\" behavior of LLMs, and the proposed methods in the paper are quite similar to the ones in prior work. For example, the paper by [Turpin et al.] which the authors seem to miss studies LLM's behavior when there exists bias in the context, where one of the settings is exactly about putting human user's belief (in a wrong answer) in the context, which is close to the type L (leading questions) prompt explored in this paper. Similar findings are also present in [Perez et al., 2022] as cited. [Wang et al., 2023a] as cited explores using another conversational LLM conditioned on a wrong solution to engage in a debate with the original LLM; the \"follow-up\" responses by the simulated user there also share many similarities with the ones proposed (expressing disagreement, doubt, different opinions, etc.).\n\nA1: Sorry for the oversight. We will add [Turpin et al.] to the related work, and we appreciate your detailed and friendly reminder.\n\nThank you for your insightful comments. Although our work indeed intersects with several studies you mentioned regarding the reliability of large language models, it distinguishes itself in several key aspects:\n\n- **Novel Research Perspective.** Unlike [Wang et al., 2023a], which designs debate-like dialogues with invalid solutions for each sample, and [Turpin et al., 2023], which introduces bias features into model inputs for multiple-choice questions (like modifying the order of options), our Follow-up Questioning Mechanism is closer to scenarios that ordinary users might encounter in real-life use of LLMs. Furthermore, the simpler and more conversational follow-up questions in our mechanism are more general and in line with the habits of everyday users than the templates or methods designed for each sample in other approaches.\n\n- **Comprehensive Scenario Design.** As you mentioned, the research methods of the related work [Wang et al., 2023a][Turpin et al., 2023] only resemble one type of question in our proposed Follow-up Questioning Mechanism (leading questions), neglecting questioning and negation, common dialogue scenes in interactions between users and LLMs. Moreover, our experimental results show that questioning and negation also cause significant fluctuations in the judgement consistency of LLMs when facing interference.\n\n- **Beyond Sycophancy, Further Discoveries.** Our study not only corroborates the sycophantic behavior mentioned by [Perez et al., 2022] but also reveals a new finding: **the model may become cautious and neutral in the face of interference**, a behavior not extensively covered in previous studies. As analyzed in the Error Analysis (refer to pages 6 to 7), we categorized errors into four types through human observation, where error categories 2, 3, and 4 can be attributed to sycophancy. However, it's worth noting the existence of category 1 (Unable to answer), refer to Figure 5 in the paper. In cases of error category 1, the model opts for a cautious and neutral stance, avoiding direct answers. This behavior is crucial for understanding the practical usability of LLMs, as it reflects an attitude distinctly different from sycophancy when faced with challenges, negations, or misleading information, rather than just flattery.\n\nOur research aims to demonstrate through comprehensive analysis that conversational large language models show unreliable judgement when faced with disturbances like questioning, negation, and misleading. Although our study shares some thematic overlaps with the cited works, it contributes new perspectives and insights into the reliability and practical application of LLMs. Together with these insightful related studies, our work is vital for the future development and real-world deployment of LLMs.\n\nWe hope our clarification can address your concerns."}, {"Heading": "Response to Reviewer AVRR (2/3)", "Subheading": "Official CommentbyAuthors21 Nov 2023, 10:02 (modified: 21 Nov 2023, 10:10)EveryoneRevisions", "Content": "Comment:\n>Q2 (from weakness 2): The qualitative analysis misses some rather important details such as the proportion of each error category. While there are some discussions/insights about the issue in the paper, overall, as an analysis/evaluation type work, I feel the contribution could be strengthened if more fruitful thoughts/speculations about the underlying cause of the observed issues (and potential ways of mitigating them) are included.\n\nA2: Thank you for your valuable suggestions. In the qualitative analysis, we have presented the proportions of each error type in the form of bar charts (refer to Figure 5). To provide a more intuitive representation, we now present the proportions of each error type in tabular form.\n\nBased on the results of error analysis, we can categorize the model's behavior into two categories: sycophancy and caution. Error#2, Error#3, and Error#4 can be attributed to sycophancy behavior, while Error#1 represents the model's cautious and neutral stance, which is in stark contrast to sycophancy. By examining the proportions of different error types, we can observe that sycophancy behavior is the primary reason for the model's poor judgement consistency when facing skepticism, denial, or misleading input. However, caution and neutrality also contribute to fluctuations in the model's judgement consistency when dealing with interference to some extent.\n\n### The proportion of four types of errors on StrategyQA.\n\n|  **Model**  | **Error#1** | **Error#2** | **Error#3** | **Error#4** |\n|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|\n|   ChatGPT   |     12 %     |      /     |     88 %     |      /     |\n| PaLM2-Bison |      /     |      /     |     100 %    |      /     |\n|  Vicuna-13B |      8 %     |      /     |     92 %     |      /     |\n\n### The proportion of four types of errors on CoinFlip.\n\n|  **Model**  | **Error#1** | **Error#2** | **Error#3** | **Error#4** |\n|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|\n|   ChatGPT   |     86 %     |      /     |     14%     |      /     |\n| PaLM2-Bison |      /     |      /     |     100 %    |      /     |\n|  Vicuna-13B |      2 %     |     40 %     |     58 %     |      /     |\n\n### The proportion of four types of errors on MultiArith.\n\n|  **Model**  | **Error#1** | **Error#2** | **Error#3** | **Error#4** |\n|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|\n|   ChatGPT   |      /     |     54 %     |      2 %     |     44 %     |\n| PaLM2-Bison |     11 %     |      /     |     89 %     |      /     |\n|  Vicuna-13B |      /     |     62 %     |     18 %     |     20 %     |"}, {"Heading": "Response to Reviewer AVRR (3/3)", "Subheading": "Official CommentbyAuthors21 Nov 2023, 10:10Everyone", "Content": "Comment:\n>Q2 (from weakness 2): While there are some discussions/insights about the issue in the paper, overall, as an analysis/evaluation type work, I feel the contribution could be strengthened if more fruitful thoughts/speculations about the underlying cause of the observed issues (and potential ways of mitigating them) are included.\n\nThank you for your constructive suggestions! Here are our responses:\n\nWe believe that the potential reasons for the occurrence of this issue may primarily include the following:\n\n- **Misalignment of thought processes** (as mentioned in the first sentence of section 4 of our paper). When humans encounter questioning, negation, or disagreement, they typically rely on their own experiences and knowledge to reevaluate their perspectives, engaging in deeper contemplation of the issues at hand. In contrast, the model's response is solely based on the information it has seen in the training data, lacking genuine thought processes and only attempting to generate the most probable response for the given input.\n\n- **Limitations of training data and training process.** Large language models are typically trained on vast amounts of data, which may contain errors, biases, or incomplete information. This can lead to challenges when these models encounter real-world scenarios that differ from their training data. Specifically, if LLMs don't effectively learn to handle skepticism or disagreement during training (e.g., SFT or RLHF), they may struggle in similar real-life interactions. Additionally, the lack of exposure to dynamic, real conversational interactions during training could hinder their ability to navigate complex dialogue situations, such as those involving in-depth questioning or deep thought.\n\n- **Sycophancy and user-centric influence.** Through error analysis, we have found that sycophancy behavior is the primary cause of decreased judgement consistency in the model. This behavior is closely related to the model's preference learning during the training process, as larger models tend to generate answers that users want to hear. Furthermore, models designed for user interactions usually need to focus on user experience. Therefore, when confronted with skepticism or disagreement, the model often starts by expressing apologies and may even seek compromise to avoid potential conflicts.\n\n- **Limitations of the autoregressive model structure.** The model is likely to generate apologies or admit mistakes first due to sycophancy. Since the model relies on autoregressive methods when generating responses, it may make incorrect judgements in subsequent responses in order to maintain semantic consistency with the earlier apology, and it may even modify the original question to make responses sound plausible (refer to Error #2 in the error analysis).\n\nRegarding potential mitigation methods for this issue, we believe they include but are not limited to the following (from low to high cost):\n\n- **Alignment of thought processes.** We can design prompts to simulate the human thought process when facing interference, thus enhancing the model's judgement consistency. For example, as proposed in the paper, few-shot prompting mitigation method can align the model's \"thought process\" when dealing with interference with that of humans facing similar interference by designing demonstration examples.\n\n- **Trade-offs between stubbornness and sycophancy.** We can stimulate the model to simulate the emotional responses that a person with a specific character might have by designing the model with a certain personality. For instance, setting the system prompt as \"You are a highly confident, self-assured, and opinionated intelligent assistant.\" can enable the model to maintain its judgement when confronted with skepticism or disagreement, mitigating issues of poor judgement consistency. \n\n- **Emphasis on data quality and realistic interaction training.** We can rigorously purify our pre-training and supervised fine-tuning datasets, eliminating any incomplete, biased, or incorrect contents (despite the potentially higher costs). Additionally, we can collect dialogue data under scenarios of skepticism, negation, and misleading contexts. The collection methods can include manual annotation, distillation from more powerful models, or context distillatio using the model itself[1].  Furthermore,  we can collect preference data by gathering multiple responses in the face of distractions and then ranking them. This collected dialogue or preference data will be integrated with existing dialogue (or preference) datasets for training, strategically enhancing the model's resilience and effectiveness in responding to distractions such as  questioning, negation, and misinformation.\n\nThank you for your insightful comments. We hope our response can address your concerns.\n\n\u200b[1] Bai et al., Constitutional ai: Harmlessness from ai feedback."}, {"Heading": "Seeking Further Feedback", "Subheading": "Official CommentbyAuthors22 Nov 2023, 06:37Everyone", "Content": "Comment:\nDear Reviewer,\n\nI hope you're doing well. The discussion period is soon coming to an end. Thank you very much for your valuable feedback. We hope that we have addressed your concerns through careful comparison with other relevant works, along with additional analysis and discussion supplements. \n\nIf you still have any further reservations or suggestions, please don't hesitate to share them.  Your insights are invaluable to us, and we're keen to address any remaining issues.\n\nBest regards!\n\nAuthors"}]}, {"Heading": "Official Review of Submission9468 by Reviewer bqxm", "Subheading": "Official ReviewbyReviewer bqxm04 Nov 2023, 05:51 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe research addresses a critical concern in the use of generative conversational large language models (LLMs) like ChatGPT, focusing on their judgement consistency when faced with follow-up questions expressing skepticism or disagreement. Drawing inspiration from educational questioning strategies, the study proposes a FOLLOW-UP QUESTIONING MECHANISM and introduces evaluation metrics to assess LLMs' consistency before and after disturbances. The study evaluates ChatGPT, PaLM2-Bison, and Vicuna-13B across reasoning benchmarks, revealing a decline in judgement consistency even when initial answers are correct. The research explores the impact of disturbances, sampling temperature, and prompts, conducting an in-depth error analysis. Moreover, it introduces and evaluates various prompting methods to mitigate this issue, demonstrating their effectiveness.\nSoundness:\n3 good\nPresentation:\n4 excellent\nContribution:\n3 good\nStrengths:\n- **Comprehensive Evaluation**: The research evaluates multiple LLMs (ChatGPT, PaLM2-Bison, and Vicuna-13B) across eight reasoning benchmarks, ensuring a comprehensive analysis of their performance under different conditions.\n- **Thorough Analysis**: The study conducts a detailed analysis of disturbances, sampling temperature, prompts, and prompt tone, offering valuable insights into the factors affecting judgement consistency.\n- **Effective Solutions**: The research explores various prompting methods and demonstrates their effectiveness in mitigating the issue, suggesting practical solutions for enhancing LLMs' reliability.\nWeaknesses:\n- **Limited Scope of LLMs**: The study evaluates a specific set of LLMs (ChatGPT, PaLM2-Bison, and Vicuna-13B), potentially limiting the generalizability of the findings to other models in the rapidly evolving landscape of conversational AI.\n- **Scope of Disturbances**: While disturbances like questioning, negation, and misleading are considered, the study might benefit from exploring a wider range of disturbances to provide a more comprehensive understanding of LLMs' judgement consistency challenges.\n- **Lack of Real-World Application**: The research focuses on theoretical evaluation and proposed mechanisms; it would strengthen its impact by discussing practical implications and real-world applications of the proposed solutions.\nQuestions:\n- Considering the rapid advancements in AI technologies, how might the results differ when applied to newer or upcoming LLMs? Is there room for future research to address this limitation?\n- Can you provide insights into how the proposed mechanisms and solutions could be practically applied in real-world scenarios, especially in fields where LLMs are extensively used, such as customer support or healthcare?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer bqxm (1/6)", "Subheading": "Official CommentbyAuthors17 Nov 2023, 21:31 (modified: 17 Nov 2023, 21:45)EveryoneRevisions", "Content": "Comment:\nThank you for the insightful comment! We will address your concerns as follows:\n\n> Q1: Considering the rapid advancements in AI technologies, how might the results differ when applied to newer or upcoming LLMs? Is there room for future research to address this limitation?\n\nA1 (also response to weakness 1): \nYour concern is indeed very necessary. Considering the rapid development of large language models, the latest LLMs may have improvements in various aspects, and we also believe it is necessary to explore whether this issue remains universal on the latest LLMs. With limited computing resources, we have evaluated the judgement consistency of several of the latest and most capable closed-source and open-source models, such as `GPT-4-1106-preview`[1], `UltraLM-13B-v2.0`[2], `XwinLM-13B-v0.2`[3], and `Zephyr-7B-Beta`[4], on the benchmarks MultiArith, StrategyQA, and CoinFlip, as per the experimental setup in the paper. We report the experimental results below.\n\nThe experimental results show that even the most advanced LLMs generally exhibit noticeable fluctuations in judgement consistency when faced with user questioning, negation, or misleading inputs. Consequently, we posit that this challenge will persist in the realm of LLMs, even with the advent of newer, more advanced models in the future. This issue is universal across all LLMs and is currently underemphasized, which underscores the importance of our research. Given this context, it is unlikely that newly developed models will be able to fully address these challenges in the near term.\n\n\n*Note 1: We chose models based on [AplacaEval Leaderboard](https://tatsu-lab.github.io/alpaca_eval/) rankings and our computational resources we could afford.*\n\n*Note 2: Due to the costs associated with calling the GPT-4 API, we only sampled 100 samples from the test sets of each of the three datasets for evaluating the judgement consistency of GPT-4. For all other models, the number of samples used for evaluation strictly adhered to the evaluation settings outlined in our paper.*\n\n[1] https://openai.com/blog/new-models-and-developer-products-announced-at-devday\n\n[2] https://huggingface.co/openbmb/UltraLM-13b-v2.0\n\n[3] https://huggingface.co/Xwin-LM/Xwin-LM-13B-V0.2\n\n[4] https://huggingface.co/HuggingFaceH4/zephyr-7b-beta"}, {"Heading": "Response to Reviewer bqxm (2/6)", "Subheading": "Official CommentbyAuthors17 Nov 2023, 21:33 (modified: 17 Nov 2023, 21:58)EveryoneRevisions", "Content": "Comment:\n### The results of GPT-4-1106-preview.\n\n|   Dataset  | Closed-ended. |        |          |         | Open-ended. |        |         |         | Leading. |        |         |         |\n|:----------:|:-------------:|:------:|:--------:|:-------:|:-----------:|:------:|:-------:|:-------:|:--------:|:------:|:-------:|:-------:|\n|            |     before    |  after |    M.    | M. Rate |    before   |  after |    M.   | M. Rate |  before  |  after |    M.   | M. Rate |\n| MultiArith |     99.00     | 97.00  |  2.00 \u2193  |  2.02 %  |    99.00    | 96.00  |  3.00 \u2193 |  3.03 %  |  98.00   | 97.00  |  1.00 \u2193 |  1.02 %  |\n| StrategyQA |     77.00     | 53.00  | 24.00 \u2193  |  31.17 % |    80.00    | 37.00  | 43.00 \u2193 |  53.75 % |  79.00   | 53.00  | 26.00 \u2193 |  32.91 % |\n|  CoinFlip  |     53.00     | 35.00  |  18.00 \u2193 |  33.96 % |    51.00    | 13.00  | 38.00 \u2193 |  74.51 % |  53.00   | 21.00  | 32.00 \u2193 |  60.38 % |\n\n### The results of UltraLM-13B-v2.0.\n\n|   Dataset  | Closed-ended. |        |       |         | Open-ended. |        |        |         | Leading. |        |        |         |\n|:----------:|:-------------:|:------:|:-----:|:-------:|:-----------:|:------:|:------:|:-------:|:--------:|:------:|:------:|:-------:|\n|            |     before    |  after |   M.  | M. Rate |    before   |  after |   M.   | M. Rate |  before  |  after |   M.   | M. Rate |\n| MultiArith |     25.00     | 16.11  | 8.89 \u2193 |  35.56 % |    28.33    | 22.78  |  5.56 \u2193 |  19.61 % |  28.33   |  4.44  | 23.89 \u2193 |  84.31 % |\n| StrategyQA |     54.44     | 46.43  | 8.01 \u2193 |  14.71 % |    52.55    | 37.12  | 15.43 \u2193 |  29.36 % |  55.75   | 26.78  | 28.97 \u2193 |  51.96 % |\n|  CoinFlip  |     32.00     | 22.80  | 9.20 \u2193 |  28.75 % |    32.60    | 16.20  | 16.40 \u2193 |  50.31 % |  29.20   | 12.60  | 16.60 \u2193 |  56.85 % |\n\n### The results of XwinLM-13B-v0.2.\n\n|   Dataset  | Closed-ended. |        |        |         | Open-ended. |        |        |         | Leading. |       |        |         |\n|:----------:|:-------------:|:------:|:------:|:-------:|:-----------:|:------:|:------:|:-------:|:--------:|:-----:|:------:|:-------:|\n|            |     before    |  after |   M.   | M. Rate |    before   |  after |   M.   | M. Rate |  before  | after |   M.   | M. Rate |\n| MultiArith |     49.44     | 43.33  |  6.11 \u2193 |  12.36 % |    63.89    | 53.33  | 10.56 \u2193 |  16.52 % |  56.11   | 5.00  | 51.11 \u2193 |  91.09 % |\n| StrategyQA |     59.10     | 23.58  | 35.52 \u2193 |  60.10 % |    58.95    | 12.37  | 46.58 \u2193 |  79.01 % |  60.84   | 1.31  | 59.53 \u2193 |  97.85 % |\n|  CoinFlip  |     41.80     | 16.60  | 25.20 \u2193 |  60.29 % |    37.00    | 16.80  | 20.20 \u2193 |  54.59 % |  45.00   | 1.40  | 43.60 \u2193 |  96.89 % |\n\n### The results of Zephyr-7B-Beta.\n\n|   Dataset  | Closed-ended. |        |        |         | Open-ended. |        |        |         | Leading. |        |         |         |\n|:----------:|:-------------:|:------:|:------:|:-------:|:-----------:|:------:|:------:|:-------:|:--------:|:------:|:-------:|:-------:|\n|            |     before    |  after |   M.   | M. Rate |    before   |  after |   M.   | M. Rate |  before  |  after |    M.   | M. Rate |\n| MultiArith |     31.67     | 28.33  | 3.33 \u2193 |  10.53 % |    27.78    | 23.33  | 4.44 \u2193 |  16.00 % |  30.56   | 16.11  | 14.44 \u2193 |  47.27 % |\n| StrategyQA |     56.04     | 51.82  | 4.22 \u2193 |  7.53 %  |    54.73    | 48.03  | 6.70 \u2193 |  12.23 % |  57.06   | 46.58  | 10.48 \u2193 |  18.37 % |\n|  CoinFlip  |     21.80     | 14.40  | 7.40 \u2193 |  33.95 % |    21.40    | 17.20  | 4.20 \u2193 |  19.63 % |  20.60   |  7.60  | 13.00 \u2193 |  63.11 % |"}, {"Heading": "Response to Reviewer bqxm (3/6)", "Subheading": "Official CommentbyAuthors17 Nov 2023, 21:35 (modified: 17 Nov 2023, 21:44)EveryoneRevisions", "Content": "Comment:\n> Q1: Is there room for future research to address this limitation?\n\nBased on the latest evaluation results we have added above, it can be observed that the issue of fluctuating judgement consistency in models when subjected to user interference is still very significant, thus there is ample room for further research. We believe that some preliminary research directions for the future include:\n\n- From **evaluation** perspective, exploring more evaluation methods and metrics, such as designing prompts with other types of interference, can more comprehensively assess the judgement consistency of LLMs in various scenarios. \nIn addition, the impact of different base models, training strategies, and optimization algorithms on the model's judgement consistency issue can be evaluated and compared.\n\n- From **training or fine-tuning** perspective, on one hand, explore other training or fine-tuning strategies, such as adversarial training, reinforcement learning, etc., to improve the robustness of LLMs when facing interference; on the other hand, research how to combine our evaluation methods with existing model training and optimization techniques to enhance the judgement consistency of LLMs. Our work aims to draw attention to this issue through systematic and comprehensive evaluation, providing inspiration and assistance for future efforts in addressing this issue through model training or fine-tuning in the future.\n\n- From **alignment** perspective, explore how to alleviate the issue of LLMs tending to please and flatter users, thus improving judgement consistency, when facing questioning, negation, or disagreement from users, by alignment, such as aligning the model's thinking process after being disturbed with the human's thinking process after being disturbed."}, {"Heading": "Response to Reviewer bqxm (4/6)", "Subheading": "Official CommentbyAuthors17 Nov 2023, 21:37 (modified: 17 Nov 2023, 21:56)EveryoneRevisions", "Content": "Comment:\n> Q2: Can you provide insights into how the proposed mechanisms and solutions could be practically applied in real-world scenarios, especially in fields where LLMs are extensively used, such as customer support or healthcare?\n\nA2 (also response to weakness 3): Thank you for your constructive feedback. We agree that discussing how this mechanism can be integrated with practical applications can indeed help strengthen the impact of our research.\n\nCurrently, LLMs mainly appear as virtual assistants in real life. Considering that they may be questioned by users or have disagreements with users during the interaction process, we believe it is necessary to use the mechanism we proposed to evaluate the model's judgement consistency in the face of interference before they are officially put into use. If the judgement consistency is low, the mitigation methods in the paper can be considered to improve their judgement consistency in the face of interference to some extent. This can not only enhance the user experience and satisfaction but also improve the reliability of the model-generated content in some fields where virtual assistants participate in actual decision-making. \n\nHere are some potential impacts and applications of our proposed mechanism and mitigation methods in real-life scenarios:\n\n- **Customer Support**: LLMs are widely used as virtual bots in the customer support field, primarily for answering user questions, solving problems, and providing advice. In this process, users may question the bot's responses or disagree with the bot-generated answers. For this application scenario, the quality assurance and monitoring team of virtual bots can use our proposed mechanism to evaluate the judgement consistency of customer support virtual bots when facing user interference. After comprehensive and reliable analysis of the results, the development team can implicitly concatenate the mitigation methods from the paper as model input after the user's question to improve the judgement consistency of virtual bots when facing interference, thereby enhancing the quality and reliability of customer support services and increasing user satisfaction and trust.\n\n- **Healthcare**: LLMs can serve as virtual assistants in healthcare, assisting in areas such as diagnosis, medical image review, and drug development. For example, when an LLM serves as a virtual medical assistant in reviewing medical images and submitting the results to doctors for diagnosis, our proposed mechanism can be used to repeatedly evaluate the consistency of its judgements with different interference questions. If the consistency reaches a preset threshold, the judgement can be submitted as auxiliary material to the doctor; otherwise, we may reasonably suspect that the judgement's reliability is low, and the mitigation methods from the paper can be used to improve judgement consistency. If the consistency still fails to meet the preset threshold after applying the mitigation methods, the patient case can be marked to remind the doctor to exercise caution. It is important to note that the final judgement should be made by the doctor, and the judgements and recommendations provided by the virtual medical assistant serve as reference information to support the doctor.\n\nIt is important to note that although our proposed mechanism and mitigation methods can assess and improve the model's judgement consistency to some extent, considering the complexity of real-world situations and the high requirements for consistency in some application scenarios, more efforts are needed in model training strategies and more comprehensive evaluations in the future to completely solve this problem. \n\nThank you for your insightful suggestions, and we hope our response has been helpful to you."}, {"Heading": "Response to Reviewer bqxm (5/6)", "Subheading": "Official CommentbyAuthors17 Nov 2023, 21:41 (modified: 17 Nov 2023, 21:44)EveryoneRevisions", "Content": "Comment:\n> Q3 (from weakness 2): While disturbances like questioning, negation, and misleading are considered, the study might benefit from exploring a wider range of disturbances to provide a more comprehensive understanding of LLMs' judgement consistency challenges.\n\nA3: Thank you for your comprehensive and careful consideration. Exploring more different types of disturbances indeed helps us more thoroughly evaluate and understand the judgement consistency of LLMs in various scenarios. In addition to the three types of disturbance mentioned in the paper, we have designed two types of disturbance based on scenarios that may occur in actual teaching[5][6]: **emotional disturbance** and **irrelevant information disturbance**. \n\n- Emotional disturbance is expressing dissatisfaction with the model's response using sentences containing strong emotional implications. Specifically, we use \u201cI am very disappointed with your answer, please respond again.\u201d as a follow-up question.\n\n- Irrelevant information disturbance refers to confusing the model's judgement by adding some irrelevant information to the original question. We sample 500 samples from GSM-IC-2step[7] and GSM-IC-mstep[7] as the experimental dataset, and concatenate \"I have added some information to the question, please answer it again.\" with the new samples that have added irrelevant questions as follow-up questions.\n\nFollowing the setup in the paper, we evaluated the judgement consistency of `ChatGPT`, `PaLM2-Bison`, `Vicuna-13B`, `GPT-4-1106-preview`, `UltraLM-13B-v2.0`, `XwinLM-13B-v0.2`, and `Zephyr-7B-Beta` in these two new disturbance scenarios, and the experimental results are shown below.\n\nFrom the experimental results, it can be seen that whether it is the three types of follow-up questions proposed in the paper or the two new types of disturbance proposed, the model's judgement consistency is generally low when facing these disturbances. Adding new disturbance further verifies the universality of this issue.\n\n*Note 1: GSM-IC[7] is constructed based on the validation set of GSM8K by adding an irrelevant sentence to each sample, and is divided into two datasets, GSM-IC-2step and GSM-IC-mstep, according to whether the intermediate steps are more than 2 steps.*\n\n[5] Humphries S. Please teach me how to teach: The emotional impact of educational change. The emotional rollercoaster of language teaching, 2020.\n\n[6] Tofade et al. Best practice strategies for effective use of questions as a teaching tool. American journal of pharmaceutical education, 2013.\n\n[7] Shi et al. Large language models can be easily distracted by irrelevant context. International Conference on Machine Learning. PMLR, 2023."}, {"Heading": "Response to Reviewer bqxm (6/6)", "Subheading": "Official CommentbyAuthors17 Nov 2023, 21:43 (modified: 17 Nov 2023, 21:59)EveryoneRevisions", "Content": "Comment:\n### The results of ChatGPT, PaLM2-Bison, and Vicuna-13B under emotional disturbance.\n\n|   Dataset  | ChatGPT |        |        |         | PaLM2-Bison |        |        |         | Vicuna-13B |        |        |         |\n|:----------:|:-------:|:------:|:------:|:-------:|:-----------:|:------:|:------:|:-------:|:----------:|:------:|:------:|:-------:|\n|            |  before |  after |   M.   | M. Rate |    before   |  after |   M.   | M. Rate |   before   |  after |   M.   | M. Rate |\n| MultiArith |  97.22  | 94.44  |  2.78 \u2193 |  2.86 %  |    95.56    | 70.00  | 25.56 \u2193 |  26.74 % |   46.67    | 41.67  |  5.00 \u2193 |  10.71 % |\n| StrategyQA |  60.55  | 22.85  | 37.70 \u2193 |  62.26 % |    65.94    | 46.29  | 19.65 \u2193 |  29.80 % |   56.77    | 34.79  | 21.98 \u2193 |  38.72 % |\n|  CoinFlip  |  7.80   |  2.60  |  5.20 \u2193 |  66.67 % |    50.20    | 49.80  |  0.40 \u2193 |  0.80 %  |   46.20    |  7.80  | 38.40 \u2193 |  83.12 % |\n\n### The results of GPT-4-1106-preview, UltraLM-13B-v2.0, XwinLM-13B-v0.2, and Zephyr-7B-Beta under emotional disturbance.\n\n|   Dataset  |  GPT-4 |        |        |         | UltraLM-13B-v2.0 |        |        |         | XwinLM-13B-v0.2 |        |        |         | Zephyr-7B-Beta |        |       |         |\n|:----------:|:------:|:------:|:------:|:-------:|:----------------:|:------:|:------:|:-------:|:---------------:|:------:|:------:|:-------:|:--------------:|:------:|:-----:|:-------:|\n|            | before |  after |   M.   | M. Rate |      before      |  after |   M.   | M. Rate |      before     |  after |   M.   | M. Rate |     before     |  after |   M.  | M. Rate |\n| MultiArith | 97.00  | 96.00  |  1.00 \u2193 |  1.03 %  |      23.89       | 21.11  |  2.78 \u2193 |  11.63 % |      56.67      | 51.67  |  5.00 \u2193 |  8.82 %  |     35.00      | 32.78  | 2.22 \u2193 |  6.35 %  |\n| StrategyQA | 79.00  | 53.00  | 26.00 \u2193 |  32.91 % |      53.57       | 43.38  | 10.19 \u2193 |  19.02 % |      57.93      | 19.21  | 38.72 \u2193 |  66.83 % |     55.75      | 51.38  | 4.37 \u2193 |  7.83 %  |\n|  CoinFlip  | 53.00  | 14.00  | 39.00 \u2193 |  73.58 % |      35.20       | 22.60  | 12.60 \u2193 |  35.80 % |      39.80      | 17.40  | 22.40 \u2193 |  56.28 % |     19.00      | 13.80  | 5.20 \u2193 |  27.37 % |\n\n### The results of ChatGPT, PaLM2-Bison, and Vicuna-13B under irrelevant information disturbance.\n\n|    Dataset   | ChatGPT |        |        |         | PaLM2-Bison |        |        |         | Vicuna-13B |        |        |         |\n|:------------:|:-------:|:------:|:------:|:-------:|:-----------:|:------:|:------:|:-------:|:----------:|:------:|:------:|:-------:|\n|              |  before |  after |   M.   | M. Rate |    before   |  after |   M.   | M. Rate |   before   |  after |   M.   | M. Rate |\n| GSM-IC-2step |  89.40  | 66.40  | 23.00 \u2193 |  25.73 % |    85.20    | 59.00  | 26.20 \u2193 |  30.75 % |   36.80    | 18.20  | 18.60 \u2193 |  50.54 % |\n| GSM-IC-mstep |  90.40  | 66.00  | 24.40 \u2193 |  26.99 % |    79.80    | 43.00  | 36.80 \u2193 |  46.12 % |   24.40    |  9.40  | 15.00 \u2193 |  61.48 % |\n\n### The results of GPT-4-1106-preview, UltraLM-13B-v2.0, XwinLM-13B-v0.2, and Zephyr-7B-Beta under irrelevant information disturbance.\n\n|    Dataset   |  GPT-4 |        |       |         | UltraLM-13B-v2.0 |       |       |         | XwinLM-13B-v0.2 |        |        |         | Zephyr-7B-Beta |        |        |         |\n|:------------:|:------:|:------:|:-----:|:-------:|:----------------:|:-----:|:-----:|:-------:|:---------------:|:------:|:------:|:-------:|:--------------:|:------:|:------:|:-------:|\n|              | before |  after |   M.  | M. Rate |      before      | after |   M.  | M. Rate |      before     |  after |   M.   | M. Rate |     before     |  after |   M.   | M. Rate |\n| GSM-IC-2step | 90.32  | 88.71  | 1.61 \u2193 |  1.79 %  |      13.40       | 8.40  | 5.00 \u2193 |  37.31 % |      30.00      | 17.00  | 13.00 \u2193 |  43.33 % |     31.20      | 19.80  | 11.40 \u2193 |  36.54 % |\n| GSM-IC-mstep | 92.00  | 90.40  | 1.60 \u2193 |  1.74 %  |       3.40       | 1.80  | 1.60 \u2193 |  47.06 % |      22.40      |  8.60  | 13.80 \u2193 |  61.61 % |     12.00      |  8.20  |  3.80 \u2193 |  31.67 % |"}, {"Heading": "Seeking Further Feedback", "Subheading": "Official CommentbyAuthors22 Nov 2023, 06:31 (modified: 22 Nov 2023, 06:38)EveryoneRevisions", "Content": "Comment:\nDear Reviewer, \n\nI hope you're doing well. The discussion period is soon coming to an end. Thank you very much for your suggestions. We hope that we have addressed your concerns through the additional experimental results provided. \n\nIf you still have any further reservations or suggestions, please don't hesitate to share them.  Your insights are invaluable to us, and we're keen to address any remaining issues.\n\nBest regards!\n\nAuthors"}]}, {"Heading": "Official Review of Submission9468 by Reviewer 3h1U", "Subheading": "Official ReviewbyReviewer 3h1U03 Nov 2023, 01:54 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper investigates the problem of answer consistency in large language models (LLMs), especially when prompted with questioning, disagreement, or misleading input. The authors designed a follow-up questioning mechanism, inspired by questioning strategies in education, to experiment with LLMs. After an initial correct response, the authors attempted prompts of questioning, disagreement, or misleading input in two different ways, one of the three and all of the three in a sequential manner. The authors conducted experiments on ChatGPT, PaLM2-Bison and Vicuna-13B using four kinds of objective reasoning questions: arithmetic reasoning, commonsense reasoning, symbolic reasoning, and knowledge reasoning. They found that a significant decrease in judgement consistency occurred after the models were prompted with questioning, disagreement, or misleading input, both in isolation and in sequence. The authors also tried some mitigation methods, but there is still room for improvement\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\n- The paper is clearly written and easy to follow. \n- It addresses the critical issue of trustworthiness in large language models. \n- The well-designed experiments and mitigation approaches clearly demonstrate the problem of LLMs and draw attention to its importance.\nWeaknesses:\n- I do not see a major problem with the paper. While some people may prefer a paper that proposes a new model, this investigative paper could still be a valuable contribution to the field.\nQuestions:\n1. I didn't understand the second sentence in footnote 1.\n\n2. Modification Rate (M. Rate) was not clear to me.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer 3h1U", "Subheading": "Official CommentbyAuthors17 Nov 2023, 21:53Everyone", "Content": "Comment:\nThank you so much for your kind words! Your appreciation means a great deal to us. We will provide you with a detailed explanation of your concerns.\n\n> Q1: I didn't understand the second sentence in footnote 1.\n\nA1: We are sorry for the ambiguity. When generating a response, the model usually provides a lot of thought process, ending with an answer. However, there is currently no particularly effective way to automatically evaluate these intermediate thought processes, so we can only assess the model based on the final answer it provides. To enable automated evaluation, we instruct the model to output the final result in a specified format (i.e., Answer:). We hope our response is helpful to you.\n\n\n> Q2: Modification Rate (M. Rate) was not clear to me.\n\nA2: Sorry for the confusion. We hope to explain the concept of Modification Rate (M. Rate) through an example. Suppose there is an evaluation test set with 1000 samples, and the model answered 10 correctly in the initial question and answer. We continue to ask follow-up questions for these 10 samples, and after the follow-up question, the model only answered 5 samples correctly. So M. = 10/1000 - 5/1000 = 5%, and M. Rate = (10 - 5) / 10 = 50%.\n\nThe rationale for employing both M. and M. Rate to assess the judgement consistency of LLMs primarily stems from the fact that in scenarios where initial performance is poor, the potential for further decrease in model performance is constrained. Consequently, relying solely on M. might not provide an accurate reflection of the model's judgement consistency.\nFor example, in the above example, although the model's performance only decreased by 5% after follow-up question, 50% of the samples answered correctly in the first round were answered incorrectly in the second round, indicating that the model's judgement consistency is low. Therefore, considering these two indicators together can provide a more accurate and comprehensive reflection of the model's judgement consistency.\n\nThank you for your valuable feedback. We hope our response has resolved your confusion."}]}]}, "BXYZvcgVUv": {"paper_info": {"Primary Area": "learning on graphs and other geometries & topologies", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Deep learning, Point cloud analysis, 3D scene segmentation", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "When it comes to understanding 3D scenes, the capability to perform zero-shot comprehension on real-world objects is essential, as unseen objects, absent from the training data, frequently appear in natural scenes.  While prior works have proposed zero-shot scene segmentation approaches by aligning 3D point features with other data modalities, they often rely on paired image sets for 3D data. More importantly, they treat scene objects independently, thereby neglecting the rich relational information inherent in scenes. This relational information between objects plays a pivotal role in identifying unseen objects within a scene, transforming the zero-shot scene understanding problem into a question as 'which object is likely to be adjacent to object A and on top of object B?'. Toward this, we introduce a novel open-vocabulary 3D scene segmentation strategy, ORG, which embeds 3D scenes into a knowledge graph framework. Our framework constructs entity graphs among scene objects using a 2D segmentation foundation model and learns relational knowledge within this graph structure. By semantically aligning node embeddings with text embedding space, ORG performs zero-shot inference effectively while leveraging prior relational knowledge specific to a given scene.  Our method consistently outperforms existing zero-shot scene segmentation approaches on three 3D scene understanding datasets: S3DIS, ScanNetV2, and 3DSSG.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9467", "PDF Url": "https://openreview.net/pdf?id=BXYZvcgVUv"}, "review_info": []}, "gYcft1HIaU": {"paper_info": {"Primary Area": "generative models", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Large Language Model, Medical Large Language Model, Clinical Knowledge, Knowledge Graph", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "Large Language Models (LLMs) show promising potential in solving clinical problems. Current LLMs, including so-called medical LLMs, are reported to achieve excellent performance on certain medical evaluation benchmarks, such as medical question answering, medical exams, etc. However, such evaluations cannot assess whether LLMs have mastered sufficient, compressive, and necessary medical knowledge for solving real clinical problems, such as clinical diagnostic assistance. In this paper, we propose a framework to assess the mastery of LLMs in clinical knowledge. Firstly, we construct a large medical disease-based knowledge base, MedDisK, covering 10,632 common diseases across 18 clinical knowledge aspects, which are crucial for diagnosing and treating diseases. Built on that, we propose a MedDisK-based evaluation method MedDisKEval: We prompt LLMs to retrieve information related to these clinical knowledge aspects. Then, we evaluate an LLM's mastery of medical knowledge by measuring the similarity between the LLM-generated information and the content within our knowledge base. Our experimental findings reveal that over 50% of the clinical information generated by our evaluated LLMs is significantly inconsistent with the corresponding knowledge stored in our knowledge base. We further perform a significance analysis to compare the performance of medical LLMs with their backbone models, discovering that 5 out of 6 medical LLMs perform less effectively than their backbone models in over half of the clinical knowledge aspects. These observations demonstrate that existing LLMs have not mastered adequate knowledge for clinical practice. Our findings offer novel and constructive insights for the advancement of medical LLMs.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9466", "PDF Url": "https://openreview.net/pdf?id=gYcft1HIaU"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9466 by Area Chair VY8u", "Subheading": "Meta ReviewbyArea Chair VY8u06 Dec 2023, 15:08 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThis paper introduces a medical knowledge base intended to be use to probe the implicit medical \"knowledge\" in LLMs. Specifically, the authors introduce \"MedDisK\", which is constructed on the basis of ICD codes; the authors identify 10,632 prevalent diseases and enlist domain experts to provide definitions for key clinical aspects relevant to these. This is used a probe: The same information is elicited from LLMs, and the resultant output is evaluated first by experts, and then automatically as a function of correlative measures such as ROUGE. It should be noted that this evaluation is done in Chinese. \n\nThe results\u2014namely that for 50% of rows in MedDisK, LLMs are completely wrong\u2014are surprising (and interesting), if taken at face value. The dataset may be a useful resource for researchers in biomedical NLP going forward. That said, reviewers largely agreed that the setup here was not particularly well motivated; why are these \"clinical knowledge aspects\" critical? And does it matter if the models are capable of performing specific \"downstream\" tasks (e.g., extraction) well, as has been empirically demonstrated? The authors assert, but without much argument, that LLMs must \"master adequate knowledge\" to be used in this space, but this is not clear to me a priori. \n\nFurther, as pointed out by rk49, the procedure being used here may not be robust; for instance, some of the \"completely wrong\" LLM responses at least appear to be an issue with the prompt strategy. For example, responding \"ok, I see\" in Table 3 indicates a model has \"misunderstood\" the question; perhaps this is part of the analysis, but again it seems unlikely that one would explicitly probe for these attributes in practice (i.e., outside of an endeavor to probe implicit knowledge). \n\nStill, I do think critical analyses such as this are important, and I think if the authors can better motivate the intent of the MedDisK dataset and justify its construction (with an eye on downstream tasks especially), and provide further evidence concerning the validity of the measure being used here, this could be a nice contribution.\nJustification For Why Not Higher Score:\nWhile the proposed resource (and accompanying evaluation) is intriguing, the work would benefit from a more explicit framing of its aims (including making the case that LLMs ought to \"master\" this particular type of \"knowledge\" in order to be useful for medical language processing tasks), and further evidence supporting the validity of the automated assessment procedure used.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "Official Review of Submission9466 by Reviewer rk49", "Subheading": "Official ReviewbyReviewer rk4902 Nov 2023, 21:19 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper introduces a large-scale medical disease-based knowledge base MedDisK, covering 10,632 common diseases and 18 clinical knowledge to evaluate LLMs. The purpose of the dataset is to  (a) include common diseases (b) involve disease base knowledge and (c) ensure that the sourcing of the dataset is such that it remains publicly inaccessible to prevent leaks during testing.\t\n\nFirst filter common diseases (determined by clinical experts based on ICD10 databases and frequency in EHR)  resulting in 10,632 common diseases. Then  employ clinical experts to define 18 disease-based clinical knowledge aspects that are crucial to medical decision-making (diagnoses, examinations, treatments) for each of the diseases. They use this database to probe LLMs and evaluate the mastery of clinical knowledge. They show that their scoring measures are in high agreement with clinical experts' subjective evaluation.  \n\nUsing the evaluation metrics they show that existing LLMs have not mastered adequate knowledge for clinical practice (showing that over 50%  of the generated information is not consistent with their KB) and are not ready to be foundation models for clinical domain.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThe paper does a good job in communicating the ideas. I agree with the author's motivation that for the LLMs to be accepted as foundation models they need to have mastered adequate clinical knowledge. This is an important question and needs comprehensive evaluation.\nWeaknesses:\nThe paper could provide a more thorough justification for the introduction of the new medical dataset, especially in the context of existing evaluation datasets. The paper mentions that the existing evaluation datasets cover only some common diseases and lack extensive coverage of knowledge across various aspects of diseases. This reviewer feels that this needs to be substantiated with more thorough comparison. \n\nWhile it is surprising that most of the LLMs perform poorly (with over 50%) predicted to be completely wrong. The evaluation procedure used to arrive at this conclusion requires further elaboration.\n\nOverall I am not fully convinced that this dataset MedDisK  and the outlined evaluation procedure is robust for determining LLMs clinical knowledge yet. \n\nThis reviewer has listed all the concerning questions in detail below.\nQuestions:\nWhat is the source of the EHR resource used in the preliminary making of the dataset?\n\nThe authors state \u201cThe existing medical evaluation benchmarks are predominantly based on question-answering (QA) tasks. These benchmarks collect questions from diverse sources, including medical examinations, electronic health records, online resources, and expert crafting\u2026\u2026cover only some common diseases and lack extensive coverage of knowledge across various aspects of diseases. \u201d  Can you compare each of these resources the paper is referring to in this sentence with MedDisK in terms of coverage of common diseases, disease base knowledge and public availability?  How does this compare with other existing medical relational databases - MIMIC, i2b2, iBKH KG etc?\n\nI understand that the paper does interval sampling (10 examples from each interval) and engages clinical experts to provide a categorical standard - wrong, correct or partially correct.  And this resulted in the following standard (0-0.3 is wrong) and (0.3 to 0.8 is partially correct) and (0.8 to 1.0 is correct). How representative are these categories? Did the experts find that all the samples in 0.8 to 1.0 are correct and correspondingly all in 0-0.3 are wrong? Can you provide more representative examples or more thorough classification of the \u201cCompletely Wrong\u201d category?\t\n\nSince LLMs response is post-processed using the NER model I think the NER model's performance is extremely crucial to evaluation. How well does it perform in identifying medical entities? From the analysis conducted in Table 8, it appears that all the LLMs are underperforming in identifying symptoms, affected sites, etc., while they generally perform well in recognizing population ages involving numeric entities. \n\nWould it be considered a correct hit if the model predicts 'GI tract' instead of 'digestive system' in the examples from Table 3? What kind of standardization was performed in evaluating LLMs response with the experts output?\n\nWhat according to the authors are the limitations of the dataset and the evaluation procedure outlined here?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer rk49 --- Part 1", "Subheading": "Official CommentbyAuthors17 Nov 2023, 04:54 (modified: 17 Nov 2023, 05:38)EveryoneRevisions", "Content": "Comment:\n**1. About MedDisK construction**: Thank you for your thoughtful concerns on the construction of our proposed database. As we introduced in our paper, the ICD-10 diseases coding system includes around 27,000 diseases. First, we conducted a statistical analysis of the occurrence frequencies of these ICD-10 diseases based on ~**4 million** highly de-identified EHRs from > 100 hospitals across 5 cities. We then filtered out high-frequency diseases with an occurrence rate exceeding 1/10000 to build the disease knowledge base. This process results in 1,048 diseases. To expand MedDisK's coverage of diseases, we asked clinical experts to choose an additional 9,584 diseases from the remaining low-frequency cases based on their clinical significance. We have included additional construction details of MedDisK in appendix A1 of the revised paper.\n\n**2. MedDisKEval versus Other datasets**: We are sincerely grateful to your constructive comments on the comparison between MedDisKEval and other medical QA datasets. We choose a total of six medical QA datasets for comparison: MedQA, MedMCQA, MMLU (medical part), MedicationQA, LiveQA, and HealthSearchQA. We use an English medical NER tool called MedCAT [1] to extract diseases from the questions, options, and answers. In assessing disease-based knowledge coverage, we find it challenging to precisely quantify the number of knowledge aspects covered by the QA dataset, given its potential to encompass distinct types of knowledge for various diseases. For instance, these QA datasets may only examine symptoms of one disease and anatomical sites of another. Instead, we extract and analyze the occurrence of 7 common disease-knowledge-related entities within these datasets, including patient population (Popu), symptom (Symp.), body parts (Part.), body systems (Syst.) therapeutic procedure (Proc.), medication (Medi.), and departments (Dept.). The experimental results are presented below, respectively. We have also updated the results and analysis in appendix A2 of the revised paper.\n| Datasets           |      Type      | \\# diseases |    Publicly available?    |\n| ------------------ | :------------: | :---------: | :-----------------------: |\n| MedQA              |   QA dataset   |    1391     |            Yes            |\n| MedMCQA            |   QA dataset   |    3475     |            Yes            |\n| MMLU (medical)     |   QA dataset   |     383     |            Yes            |\n| MedicationQA       |   QA dataset   |     172     |            Yes            |\n| LiveQA             |   QA dataset   |     480     |            Yes            |\n| HealthSearchQA     |   QA dataset   |     262     |            Yes            |\n| **Total of Above** |  QA datasets   |    3907     |            Yes            |\n| **MedDisK (Ours)** | Knowledge base |    10632    | Yes (evaluation platform) |\n\n| Dataset         | \\#Popu. | \\#Symp. | \\#Part. | \\#Syst. | \\#Proc. | \\#Medi. | \\#Dept. |\n| --------------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- |\n| MedQA           | 197     | 377     | 574     | 15      | 429     | 62      | 36      |\n| MedMCQA         | 241     | 452     | 1245    | 33      | 811     | 56      | 54      |\n| MMLU  (medical) | 92      | 114     | 250     | 10      | 111     | 6       | 9       |\n| MedicationQA    | 27      | 64      | 52      | 7       | 70      | 67      | 3       |\n| LiveQA          | 75      | 108     | 141     | 9       | 166     | 14      | 19      |\n| HealthSearchQA  | 10      | 63      | 40      | 3       | 4       | 2       | 2       |\n| **Total  of Above** | 349     | 570     | 1362    | 34      | 997     | 183     | 83      |\n| **MedDisK  (Ours)** | 701     | 18737   | 1585    | 89      | 5097    | 3826    | 89      |\n\nThe experimental results show that MedDisK covers significantly more diseases and disease-knowledge-related entities than existing QA datasets, which may demonstrate our claim. For medical knowledge graphs like iBKH, they contain various biomedical entities (disease, gene, molecule, etc.) and their corresponding relations. Generally, these medical KGs cover only a limited number of disease-related relations; for instance, iBKH includes only 6 disease-related relations. In contrast, MedDisK is a disease-centered knowledge base that involves **18** essential knowledge types crucial for clinical practice, including diagnosis and treatment. Additionally, none of these medical knowledge graphs have been utilized in the construction of mainstream medical evaluation datasets yet, making them incomparable to our evaluation dataset. \n\n[1] Kraljevic Z, Searle T, Shek A, et al. Multi-domain clinical natural language processing with medcat: the medical concept annotation toolkit[J]. Artificial intelligence in medicine, 2021, 117: 102083."}, {"Heading": "Response to Reviewer rk49 --- Part 2", "Subheading": "Official CommentbyAuthors17 Nov 2023, 04:59Everyone", "Content": "Comment:\n**3. Results Categories**: Thank you for your insightful comments. We have provided more examples of \u201cCompletely Wrong\u201d, \u201cPartially Correct\u201d, and \u201cBasically Correct\u201d in Appendix K of the revised paper. Here are some representative examples of the \u201cCompletely Wrong\u201d category:\n|                         Disease                         |  Knowledge  Aspect  |                    Ground Truth                    |                    Original LLM  Response                    | Post-processed  Response |\n| :-----------------------------------------------------: | :-----------------: | :------------------------------------------------: | :----------------------------------------------------------: | :--------------------------: |\n| rheumatoid arthritis of the hand interphalangeal joints | patient  population | higher  prevalence in females; middle-age; elderly |                         Ok,  I see.                          |             None             |\n|                  wart of brass players                  |  anatomical  sites  |                        skin                        | The  anatomical site of wart of brass players includes the bone marrow. |         bone  marrow         |\n|                  temporal lobe abscess                  |     departments     |              eurology;  neurosurgery               | Departments  involved in temporal lobe abscess include: None |             None             |\n\nCurrently, our threshold-based grading strategy is not 100% accurate. However, according to the results of human evaluation on 150 samples, only a small fraction of responses in the \u201cBasically Correct\u201d category contain knowledge errors, and these responses typically had only one or two errors. Similarly, in the \u201cCompletely Wrong\u201d category, only a very small portion of responses include correct knowledge. Moreover, the consistency between our proposed automated grading and expert grading reached **0.837** (Table 2 in our paper), indicating a strong agreement between automated and clinical expert evaluation. Therefore, the categorization of results is generally reasonable. We are greatly appreciated your valuable comments, and we plan to enhance the accuracy of this categorization through a more complex post-processing module in the future.\n\n**4. NER model**: We would like to express gratitude for your insightful thoughts and valuable suggestions of our medical NER model. The Medical NER model is indeed a crucial module in our evaluation method. In fact, NER models also play important roles in various medical applications. The NER model we applied in our evaluation is a reliable and stable tool that has been applied in various medical scenarios, including assisted diagnosis, EHR-based semantic parsing, and assisted consultations. It is a BERT-based model that is constructed by first pretraining with MLM objective on 3.5 million unlabeled and highly de-identified EHRs from 7 hospitals, and finetuning on 200k labeled EHR segments following the method proposed in [1]. The model is able to identify a total of **116** types of important medical concepts, and it achieves a micro-f1 of **0.88** on a test set of 10k+ real-world EHRs containing 40k+ medical entities, and even achieves f1 scores exceeding 0.9 on several crucial medical entities (anatomical sites, symptoms, medication, etc.). We also update the details of this NER model in the appendix E of the revised paper.\n\nFor the performance of LLMs on different knowledge aspects, we conduct an extra analysis of LLMs performance across 18 knowledge aspects, where we find that LLMs perform distinctly on different aspects. We have updated the detailed performance of LLMs on various aspects in **Figure 9 and 10** of the revised paper, respectively. Table 8 (Table 14 in the revised paper) presents the relative performance of 6 medical LLMs compared with their base models. Positive values (in green) indicate the medical LLM outperforms its base model in the corresponding knowledge aspect, while negative values (in red) signify underperformance in the corresponding aspect.\n\n[1] Jianlin Su, Ahmed Murtadha, Shengfeng Pan, Jing Hou, Jun Sun, Wanwei Huang, Bo Wen, and Yunfeng Liu. Global pointer: Novel efficient span-based approach for named entity recognition. arXiv preprint arXiv:2208.03054, 2022."}, {"Heading": "Response to Reviewer rk49 --- Part 3", "Subheading": "Official CommentbyAuthors17 Nov 2023, 05:03Everyone", "Content": "Comment:\n**5. Standardization**: Thank you for your valuable comments. We have standardized LLMs\u2019 response on a few knowledge aspects (Departments, Ages) with our medical thesaurus, while we also observe that certain medical terms are fundamentally distinct despite their semantic similarities. For example, though \u201cGI tract\u201d and \u201cdigestive system\u201d are semantically similar, they are distinguished in the medical field, since the GI tract is a subset of the digestive system, focusing on the direct passage of food.\n\nIt is widely recognized that distinguishing whether two medical terms have entirely identical meanings is a challenge in the field of medical NLP. To address this, we employ multiple metrics in our evaluation. BLEU and ROUGE are strict metrics, emphasizing token-level consistency, while cosine similarity focuses on semantic-level consistency. Our experiments reveal that utilizing multiple metrics ensures the robustness and fairness of the evaluation results.\n\n**6. Limitations**: Thank you for your concern on the limitations of our evaluation. There are two main limitations of our evaluation dataset:\n\n1. Although MedDisK covers a significantly larger number of diseases and disease-based knowledge, it has not covered all the 27,000 diseases in ICD-10 yet. Currently, our evaluation approach cannot assess LLMs\u2019 mastery on some rare diseases in clinical practice.\n\n2. MedDisK focuses on knowledge aspects that are crucial for clinical practice, such as medication, examination, surgical procedures. It does not involve other biomedical knowledge, such as molecule structure and gene."}]}, {"Heading": "Official Review of Submission9466 by Reviewer tVw5", "Subheading": "Official ReviewbyReviewer tVw531 Oct 2023, 16:19 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nTo evaluate whether LLMs have mastered sufficient clinical knowledge, the authors first propose a large-scale medical disease-based knowledge base named MedDisK. They then develop MedDisKEval, a method that prompts LLMs to retrieve information on clinical knowledge aspects and measures the similarity between LLM-generated information and MedDisK. Results show that most of the current LLMs do not have sufficient clinical knowledge.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\n1. The motivation is clear, and it is interesting to know whether current LLMs have mastered sufficient domain knowledge to help in the medical domain.\n2. The authors conduct extensive experiments with 12 LLMs, which include general LLMs and medical LLMs.\n3. The authors provide sufficient examples of prompt instructions, knowledge aspects, and LLM responses, which make it easier for readers to grasp the basic idea of the paper.\nWeaknesses:\n1. One significant issue with this paper is that the authors may overstate the implications of their evaluation results. The experiments are exclusively conducted in Chinese. However, this critical detail is not adequately emphasized in the main paper, particularly in their conclusion that \"none of the evaluated LLMs have mastered sufficient knowledge to handle real clinical problems effectively.\" Based on their evaluation, the valid conclusion should be that LLMs do not possess adequate clinical knowledge **in the Chinese language**, and this finding cannot be generalized to other languages.\n\n2. In the second paragraph of the introduction, the authors claim that current QA-based medical evaluation datasets cannot evaluate whether LLMs have mastered sufficient medical knowledge because those datasets cover only some common diseases. It would be more robust if the authors could further justify this statement with some analysis (e.g. to quantitatively show the coverage of diseases in the existing benchmarks).\n\n3. For the proposed knowledge base MedDisK, it would be better for authors to include more details of the construction process. For example, how is the agreement among the clinical experts, is there any strategy used to tackle disagreement, and will this process introduce any additional human bias?\n\n4. In section 3.2.1, the authors \"employ a specialized NER model to identify and extract medical entities from the text\". However, the exact name and citation of the used NER model are missing, and it will be more convincing to include an analysis of the accuracy of the NER model as incorrectly recognized entities could impact the evaluation results of LLMs.\nQuestions:\n1. The authors claim that current QA-based medical evaluation datasets cover only some common diseases. However, in section 3.1 where the authors introduce their proposed knowledge base, it is said that \"We first select a subset from the ICD10 database according to whether the diseases are common in clinical (determined by clinical experts) and are statistically frequent in EHR (Electronic Health Record), resulting in 10,632 common diseases.\" I wonder why they also consider common diseases in their knowledge base?\n\n2. In the section of \"Disease-Knowledge-based Automated Scoring\", are there any better metrics to evaluate the similarity? The token-level BLEU-1 and ROUGE-1 cannot consider semantic meaning, and the M3E model is described as a sentence-level metric, whereas the evaluation in this context focuses on the meaning of individual tokens.\n\n3. In Table 3, one completely wrong example of LLM response is \"ok, I see\". Since the authors mention that they employ a specialized NER model to identify and extract medical entities from the text, I wonder why the NER model could extract such words from the responses.\n\n4. In section 4.2.2, the authors assign scores of 0, 5, and 10 to \u201dCompletely Wrong,\u201d \u201dPartially Correct,\u201d and \u201dBasically Correct,\u201d respectively, to calculate a total score. It's important to clarify how they arrived at the values \"0, 5, 10\" for this scoring system.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer tVw5 --- Part 1", "Subheading": "Official CommentbyAuthors17 Nov 2023, 05:10Everyone", "Content": "Comment:\n**1. Language issue**: Thank you for your thoughtful comments. It is worth noting that foundational medical knowledge is relatively stable and universally applicable, unaffected by specific languages, because it includes fundamental concepts in physiology, anatomy, pharmacology, and other fields. These concepts have similar meanings across different linguistic contexts. Moreover, according to current works on LLM evaluation, foundation models such as GPT-3.5-turbo, GPT-4 also achieves considerable performance on **Chinese** general [1] and medical [2] datasets. This is because current LLMs have already possessed strong multilingual capabilities, and language is no longer a key factor influencing their performance. \n\nWe have also conducted a preliminary experiment to study the influence of language in our evaluation. Specifically, we randomly extract 500 diseases in our knowledge base, and translate all the related content into English with GPT-4. We have asked clinical experts to validate the correctness of the translation. Subsequently, we evaluate GPT-3.5-turbo on this small-scale English dataset by replacing the NER model with an English medical NER model MedCAT [3] and M3E model with MPNet model [4]. We find that the total score of GPT-3.5-turbo on these 500 diseases is **4.56** in English and **4.07** in Chinese, indicating that language has limited influence to our proposed evaluation method.\n\n[1] Huang Y, Bai Y, Zhu Z, et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Advances in neural information processing systems, 2023.\n\n[2] Zhu W, Wang X, Zheng H, et al. PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain[J]. arXiv preprint arXiv:2310.14151, 2023.\n\n[3] Kraljevic Z, Searle T, Shek A, et al. Multi-domain clinical natural language processing with medcat: the medical concept annotation toolkit[J]. Artificial intelligence in medicine, 2021.\n\n[4] Song K, Tan X, Qin T, et al. Mpnet: Masked and permuted pre-training for language understanding[J]. Advances in Neural Information Processing Systems, 2020.\n\n**2. Coverage of existing benchmarks**: We are grateful for your constructive suggestions. We compare MedDisK with six well-known medical QA evaluation datasets: MedQA, MedMCQA, MMLU (medical part), MedicationQA, LiveQA, and HealthSearchQA. We estimate the coverage of diseases in theses benchmarks by leveraging an English medical NER model MedCAT. The results are presented below, and we have added this comparison in the appendix A2 of our revised paper as well:\n\n| Datasets           |      Type      | \\# diseases |    Publicly available?    |\n| ------------------ | :------------: | :---------: | :-----------------------: |\n| MedQA              |   QA dataset   |    1391     |            Yes            |\n| MedMCQA            |   QA dataset   |    3475     |            Yes            |\n| MMLU (medical)     |   QA dataset   |     383     |            Yes            |\n| MedicationQA       |   QA dataset   |     172     |            Yes            |\n| LiveQA             |   QA dataset   |     480     |            Yes            |\n| HealthSearchQA     |   QA dataset   |     262     |            Yes            |\n| **Total of Above** |  QA datasets   |    3907     |            Yes            |\n| **MedDisK (Ours)** | Knowledge base |    10632    | Yes (evaluation platform) |\n\nThe table above shows that the proposed MedDisK covers much more diseases (>10k) than existing medical QA datasets (~3k). The results suggest that MedDisKEval significantly surpasses existing benchmarks in terms of the disease knowledge coverage."}, {"Heading": "Response to Reviewer tVw5 --- Part 2", "Subheading": "Official CommentbyAuthors17 Nov 2023, 05:14Everyone", "Content": "Comment:\n**3. Construction details**: Thank you for your insightful concern of MedDisK construction process. The construction of MedDisK can be divided into two phases: disease selection and knowledge annotation. In the first phase, we analyzed the occurrence of ICD-10 diseases in ~**4 million** highly de-identified EHRs from over 100 hospitals across 5 cities, and extracted 1,048 high-frequency diseases. To broaden the coverage of our knowledge base, we engaged clinical experts to identify another 9,584 clinically significant diseases from the remaining low-frequency diseases. As a result, we select a total of 10,632 diseases.\n\nIn the knowledge annotation phase of MedDisK construction, each clinical expert is responsible for a specific type of diseases. The introduction of human bias in this process is minimal, given that medical knowledge is objective information extensively recorded in a vast amount of medical literature and books. To minimize the human bias introduced in labeling, we use a retrieval module to retrieve related information of the specific disease from medical books and literature. We then asked experts to proofread the retrieved information and craft each clinical knowledge aspect of the given disease. If an expert encountered uncertainty regarding certain content, they would report the issue, and a discussion among all experts would arrive at a consensus for the annotation result. To test the feasibility of this annotation approach, in the early stage of knowledge base construction, we engaged two experts to annotate the knowledge related to 20 diseases (approximately involving **1,000** disease-related knowledge points) using the human-machine collaborative method mentioned above. The results revealed a disagreement rate of less than 2%, demonstrating the reliability of our knowledge base construction method. We have updated these construction details of MedDisK in appendix A1 of the revised paper.\n\n**4. NER model**: We are grateful for your constructive and thoughtful suggestions on the introduction of our medical NER model. The utilization of NER model in our evaluation can filter out irrelevant content in LLM responses and improve the reliability of the evaluation. In our study, we employ a medical NER model that has been trained on numerous EHRs early and applied in various real-world medical scenarios, such as assisted consultations and diagnosis, EHR-based semantic parsing. Specifically, we first pre-trained a BERT-based model on 3.5 million highly de-identified EHRs from 7 hospitals with MLM objective. Then we finetuned the pre-trained model on a total of 200k labeled EHR segments following the method proposed in [1]. On a test set of 10k+ real-word EHRs involving 40k+ medical entities, our NER model achieves a micro-f1 score of **0.88** on a total of **116** types of medical entities, and the f1-score even surpasses 0.9 on some common medical entities, such as anatomical sites, symptoms, medication. We have provided more details of our NER model in the appendix E of the revised paper.\n\n[1] Jianlin Su, Ahmed Murtadha, Shengfeng Pan, Jing Hou, Jun Sun, Wanwei Huang, Bo Wen, and Yunfeng Liu. Global pointer: Novel efficient span-based approach for named entity recognition. arXiv preprint arXiv:2208.03054, 2022.\n\n**5. About Common disease**: Thank you for carefully reading our paper. The 10,632 \u201cCommon Diseases\u201d in our MedDisK refer to a subset relative to the total 27,000 ICD-10 diseases. We find in our preliminary study (Table 7 in the revised paper) that existing QA-based medical evaluation datasets cover limited diseases as well as not possess sufficient, systematic disease-centered clinical knowledge. Moreover, according to existing studies ([1], Table 8 in [2]), existing QA-based datasets suffer from data contamination that is widely existed in current LLMs. Motivated by these issues, we constructed a large-scale disease-centered knowledge base to evaluate whether LLMs master adequate clinical knowledge for real-world medical scenarios, and to indicate possible pathways for training medical foundation models. While MedDisK only contains \u201ccommon diseases\u201d from the ICD-10 disease base, it already includes a substantially greater number of diseases compared to existing QA datasets.\n\n[1] Zhou K, Zhu Y, Chen Z, et al. Don't Make Your LLM an Evaluation Benchmark Cheater[J]. arXiv preprint arXiv:2311.01964, 2023.\n\n[2] Wei T, Zhao L, Zhang L, et al. Skywork: A More Open Bilingual Foundation Model[J]. arXiv preprint arXiv:2310.19341, 2023."}, {"Heading": "Response to Reviewer tVw5 --- Part 3", "Subheading": "Official CommentbyAuthors17 Nov 2023, 05:17Everyone", "Content": "Comment:\n**6. Metrics**: Thank you for your constructive and insightful suggestion. Measuring the correctness and completeness of LLMs\u2019 response is considered as a challenge in NLP community. To ensure the fairness and robustness of our evaluation, we employ multiple metrics, including BLEU, ROUGE, and cosine similarity. BLEU and ROUGE are strict metrics that consider the token-level consistency, while cosine similarity focuses on semantic-level consistency. It is worth noting that LLMs\u2019 responses have already been parsed into segments for enumerated types of knowledge, and M3E is acted as an embedding model rather than a sentence encoder in this situation. We have tried other token-level semantic similarity metrics such as **BERTScore** [1] in our study and found that it achieves unsatisfied consistency with manual evaluation (see **Table 11** in the appendix of the revised paper). We hypothesize that the reason BERTScore do not work in our evaluation is because in the medical domain, slight differences in tokens between terms may lead to entirely different meanings. For example, \u201cacute renal failure\u201d and \u201cacute respiratory failure\u201d are completely different diseases, where there is only a slight difference in tokens.\n\n[1] Zhang T, Kishore V, Wu F, et al. BERTScore: Evaluating Text Generation with BERT[C]//International Conference on Learning Representations. 2019.\n\n**7. Table 3 content**: We sincerely appreciate your carefully reading. The LLM response provided in Table 3 is the original response of LLMs before sending into the NER model. Based on your valuable suggestions, we have revised our paper by adding more examples with both original and post-processed responses in **Table 16** of appendix K.\n\n**8. Scoring System**: Thank you. The scoring system is derived based on our observation of LLMs\u2019 response. We observed that in the responses of a certain LLM, some are unrelated to the disease, some partially overlap with our annotations, and the rest responses basically cover all the knowledge stored in our knowledge base. For the convenience of evaluation, we established a scoring system on a scale of 0 to 10, where 0 represents \u201ccompletely wrong\u201d, 5 indicates \u201cpartial correct\u201d, and 10 signifies \u201cbasically correct\u201d."}]}, {"Heading": "Official Review of Submission9466 by Reviewer h42w", "Subheading": "Official ReviewbyReviewer h42w31 Oct 2023, 14:39 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper evaluates the current performance of medical LLMs by creating a benchmark and testing existing said LLMs against it. This work creates MedDisK, a database designed to test the medical knowledge of LLMs on different \"clinical knowledge aspects\". These properties are not limited to those used just for diagnosis; example properties include patient population, treatment principles, departments (relevant medical departments), etc. This work also introduces MedDisKEval, a method that includes automated and clinical-expert-dependent steps to grade the performance of LLMs. Notably, the paper concludes that most current medical LLMs do not perform better than the base LLMs they are built upon.\nSoundness:\n2 fair\nPresentation:\n1 poor\nContribution:\n2 fair\nStrengths:\n* The development of a medical knowledge benchmark involved consulting 20 clinical experts over 10 months is good. This paper focuses largely on Chinese data/expert consult, but the presentation itself features relevant English translation.\n* Creating a clear evaluation method combining automated/expert consultation is also useful. \n* The conclusions of the evaluation point out specific flaws in existing medical LLMs; certain models evaluate different features poorly, for example. This provides a concrete criticism/evaluation of those methods that can be built upon.\nWeaknesses:\n* The creation of a medical LLM benchmark itself does not make fundamental improvements over existing benchmarks developed in medical LLM research. As an example, the Singhal et al. 2023a paper also tested modern LLMs with human evaluation (MultiMedQA). Creating another benchmark by itself is not a conceptually novel improvement, and this work did not sufficiently argue for its improvement above these existing models/evaluations.\n* This work does not go into as much detail about the representation of medical knowledge in LLMs, providing only a benchmark without technical insight of what the LLMs might be doing or how they encode medical information.\n* Using MedDisKEval seems expensive or possibly unreliable. Someone seeking to use this evaluation method may need to consult expert opinion themselves, just to calibrate the alignment scores. The motivation behind the linear combination of BLEU-1, ROUGE-1 and cosine sim is empirically driven and is not inherently convincing as a metric.\nQuestions:\n* The database MedDisK was constructed with \"clinical experts and machine assistance.\" Further clarification is required; what was the exact process of constructing the database, and how was machine assistance used? \n* This work focused on a set of LLMs that is somewhat disjoint from existing popular medical LLMs. For example, MedPaLM?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer h42w --- Part 1", "Subheading": "Official CommentbyAuthors17 Nov 2023, 05:21 (modified: 17 Nov 2023, 05:32)EveryoneRevisions", "Content": "Comment:\n**1. Novelty of MedDisKEval**: Thank you for your thoughtful concern on the novelty of our proposed LLM benchmark. The MultiMedQA dataset proposed by Singhal et al. is actually a set of medical QA datasets, some of them (MMLU, MedQA, MedMCQA, etc.) have already been proposed in other literature. These QA-based datasets are able to evaluate LLMs\u2019 medical capabilities to some extent, while they face limitations in evaluating clinical knowledge mastery. Firstly, existing QA-based evaluation sets have a limited coverage of diseases and disease-related knowledge, which are crucial for clinical practice. To clarify our claim, we have compared them with our proposed benchmark MedDisK in terms of the coverage of diseases, and 7 types of disease-knowledge-related entities: patient population (Popu), symptom (Symp.), body parts (Part.), body systems (Syst.) therapeutic procedure (Proc.), medication (Medi.), and departments (Dept.).We have found that these datasets cover limited number of diseases as well as disease-related knowledge: \n\n| Datasets           |      Type      | \\# diseases |    Publicly available?    |\n| ------------------ | :------------: | :---------: | :-----------------------: |\n| MedQA              |   QA dataset   |    1391     |            Yes            |\n| MedMCQA            |   QA dataset   |    3475     |            Yes            |\n| MMLU (medical)     |   QA dataset   |     383     |            Yes            |\n| MedicationQA       |   QA dataset   |     172     |            Yes            |\n| LiveQA             |   QA dataset   |     480     |            Yes            |\n| HealthSearchQA     |   QA dataset   |     262     |            Yes            |\n| **Total of Above** |  QA datasets   |    3907     |            Yes            |\n| **MedDisK (Ours)** | Knowledge base |    10632    | Yes (evaluation platform) |\n\n| Dataset         | \\#Popu. | \\#Symp. | \\#Part. | \\#Syst. | \\#Proc. | \\#Medi. | \\#Dept. |\n| --------------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- |\n| MedQA           | 197     | 377     | 574     | 15      | 429     | 62      | 36      |\n| MedMCQA         | 241     | 452     | 1245    | 33      | 811     | 56      | 54      |\n| MMLU  (medical) | 92      | 114     | 250     | 10      | 111     | 6       | 9       |\n| MedicationQA    | 27      | 64      | 52      | 7       | 70      | 67      | 3       |\n| LiveQA          | 75      | 108     | 141     | 9       | 166     | 14      | 19      |\n| HealthSearchQA  | 10      | 63      | 40      | 3       | 4       | 2       | 2       |\n| **Total  of Above** | 349     | 570     | 1362    | 34      | 997     | 183     | 83      |\n| **MedDisK  (Ours)** | 701     | 18737   | 1585    | 89      | 5097    | 3826    | 89      |\n\nThe comparison results indicate that the proposed benchmark covers a much broader range of disease knowledge compared to the previous QA datasets. We have also updated this comparison results in the appendix A2 of the revised paper. Moreover, several recent studies ([1], Table 8 in [2]) highlight that **data contamination** in existing LLMs adversely affects the fairness of contemporary open-source QA-based evaluation datasets. In contrast, our knowledge base is crafted by clinical experts and cannot be directly accessed on the Internet; thus, the proposed evaluation benchmark does not suffer from data contamination. An evaluation platform will be released to ensure the availability our proposed benchmark.\n\n[1] Zhou K, Zhu Y, Chen Z, et al. Don't Make Your LLM an Evaluation Benchmark Cheater[J]. arXiv preprint arXiv:2311.01964, 2023.\n\n[2] Wei T, Zhao L, Zhang L, et al. Skywork: A More Open Bilingual Foundation Model[J]. arXiv preprint arXiv:2310.19341, 2023.\n\n**2. Research value**: We are sincerely grateful for your insightful comments. For a considerable period, we have been researching how general LLMs can truly serve as foundational models in the medical domain and be applicable in real clinical scenarios. We have observed that current general/medical LLMs cannot be directly applied in real-world clinical applications, despite achieving considerable performance on specific medical tasks. The motivation of this work is to find out how far current LLMs are to the real medical foundation models in the aspect of clinical knowledge mastery. We humbly believe that the proposed evaluation benchmark could offer valuable insights to developers of medical LLMs, ultimately promoting the advancement of foundational models in the medical domain. We will further delve into how LLMs can better encode medical knowledge to be applicable in real-world clinical scenarios."}, {"Heading": "Response to Reviewer h42w --- Part 2", "Subheading": "Official CommentbyAuthors17 Nov 2023, 05:25Everyone", "Content": "Comment:\n**3. Cost and Reliability of MedDisKEval**: Thank you so much for your thoughtful concerns of the cost and reliability of the proposed evaluation benchmark. \n\nFor the cost of MedDisKEval, as we mentioned above, we will make an online evaluation platform freely accessible for any researchers and organizations. The platform provides participants a list of diseases, prompts we employ in this work, and several demonstrative examples. Participants can either directly test their LLMs with the provided prompts, or design prompts by themselves. Once the participants upload the responses of their LLMs on the platform, the evaluation script will be running automatically. The alignment standard is **preset** by our clinical experts in the evaluation script and does not require extra calibration by participants. The evaluation results, including the performance on various knowledge aspects, will be available for downloading once the evaluation process ends. A leaderboard will be available for researchers to compare the performance of their LLMs with others. The planning of this evaluation platform has been updated in the appendix L of our revised paper.\n\nFor the reliability of MedDisKEval, it is indeed challenging to find proper metrics for our evaluation, and we have not found a perfect metric for our evaluation yet. As a compromise, we utilized common text evaluation metrics, such as BLEU for machine translation, ROUGE for text summarization, and cosine similarity for text similarity calculation. BLEU and ROUGE primarily assess token-level consistency between LLMs' responses and the ground truth context, whereas cosine similarity focuses on semantic-level consistency. We have observed in experiments that token-level metrics face difficulties in dealing with synonyms of medical terms, while cosine similarity may ignore slight token-level difference that completely change the meaning of medical terms. Therefore, we combine these metrics together in our evaluation. It is worth noting that the combination of these metrics is **NOT** achieved through a straightforward linear combination. Instead, each score generated by a specific metric is initially categorized into three tiers using expert-defined thresholds. Considering the statistical distribution biases across metrics, we request experts to establish independent thresholds for each metric. Therefore, the grades derived from different metrics are somewhat comparable and can be combined by **soft voting** to better reflect the performance of LLMs. We have also found that the combined metric achieves high consistency (**0.837**) with human experts (see Table 2 in our paper), indicating the reliability of this metric."}, {"Heading": "Response to Reviewer h42w --- Part 3", "Subheading": "Official CommentbyAuthors17 Nov 2023, 05:28Everyone", "Content": "Comment:\n**4. Clarification of MedDisk construction**: We are grateful for your constructive concerns of MedDisK construction. The construction of MedDisK involves two phases: selection of diseases, and knowledge annotation. In the first phase, we conduct a statistical analysis with machine assistance on the occurrence of ICD-10 diseases in ~**4 million** highly de-identified electronic health records (EHRs) from over 100 hospitals across 5 cities. We selected diseases with a frequency exceeding 1/10000, resulting in 1,048 distinct diseases. We further requested experts to select low-frequency diseases that are important in clinical practice from the remaining, resulting in another 9,584 diseases. In the second phase, we employed a retrieve-and-proofread knowledge annotation method. We first exploit an information retrieval module that retrieves disease-related information from medical books and literature. Subsequently, we asked clinical experts to proofread the retrieved information, filtering out irrelevant content, and supplementing missing knowledge. We find that such human-machine collaboration is helpful for minimizing human bias introduced in annotation.\n\n**5. Evaluated LLMs**: Thank you for your valuable and constructive suggestions. MedPaLM is currently a closed-source LLM that is only available to a selected group of Google Cloud customers for limited testing. Meanwhile, we found that the medical LLMs we evaluated have claimed **considerable** performance on medical QA datasets ([1] [2] [3]), while they are not applicable in real-world clinical scenarios. Therefore, we constructed a disease-based knowledge base, covering over 10k diseases across 18 distinct clinical knowledge aspects, to evaluate the clinical knowledge mastery of current foundation models and medical LLMs. Based on our evaluation results, none of the existing LLMs have mastered adequate clinical knowledge for real clinical applications. In comparison, GPT-3.5-turbo generally exhibits a greater mastery of clinical knowledge than current medical LLMs. We will contact Google in the future to get access of their MedPaLM model and evaluate it on our benchmark.\n\n[1] Zhang H, Chen J, Jiang F, et al. HuatuoGPT, towards Taming Language Model to Be a Doctor[J]. arXiv preprint arXiv:2305.15075, 2023.\n\n[2] Wang H, Liu C, Xi N, et al. Huatuo: Tuning llama model with chinese medical knowledge[J]. arXiv preprint arXiv:2304.06975, 2023.\n\n[3] Yunxiang L, Zihan L, Kai Z, et al. Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge[J]. arXiv preprint arXiv:2303.14070, 2023."}]}]}, "10eQ4Cfh8p": {"paper_info": {"Supplementary Material": "zip", "Primary Area": "reinforcement learning", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Reinforcement Learning, Flexible Job Shop Schedule Problem, FJSP", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "TL;DR": "A Unified RL Paradigm to solve Flexible Job Shop Optimization", "Abstract": "We present an end-to-end reinforcement learning framework designed to address the Flexible Job Shop Problem (FJSP). Our approach consists of two primary components: a generative model that produces problem solutions stepwise, and a secondary model that continually refines these (partial) solutions. Importantly, we train both models concurrently, enabling each to be cognizant of the other's policy and make informed decisions. Extensive experimentation demonstrates that our model delivers better performance in shorter time on several public datasets comparing to baseline algorithms. Furthermore, we highlight the superior generalizability of our approach, as it maintains strong performance on large-scale instances even when trained on small-scale instances. It is worth noting that this training paradigm can be readily adapted to other combinatorial optimization problems, such as the traveling salesman problemand beyond.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9463", "PDF Url": "https://openreview.net/pdf?id=10eQ4Cfh8p"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9463 by Area Chair 5KP5", "Subheading": "Meta ReviewbyArea Chair 5KP502 Dec 2023, 14:07 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nSummary: The paper introduces a RL framework specifically designed for the Flexible Job Shop Problem (FJSP). It features an innovative approach using two models that work concurrently: one for generating solutions and another for improving them. This methodology is claimed to be significantly faster than existing metaheuristic methods and some previous reinforcement learning approaches, while also providing high-quality solutions.\nStrengths: The paper's strength lies in its novel dual-model approach to solving FJSP, which combines solution generation and improvement in a concurrent training framework.\nWeaknesses:\nLack of Empirical Justification: The paper makes broad claims about adaptability to other problems but lacks empirical evidence to support these claims.  The methodology seems specifically tailored for FJSP, with no clear evidence or explanation of its adaptability to other combinatorial optimization problems.\nMissing Ablations and Baselines: There is no detailed examination of the performance of the generative model in isolation, which could provide insights into the individual contribution of each model. The paper misses certain baselines in its performance evaluation. For instance, comparisons against some existing RL-based methods and metaheuristics are not adequately presented.\nClarity Issues: The paper has several writing and typographical errors, unclear descriptions, and missing references. Certain critical components, like the improvement step's applicability to other problems and the reasoning behind various embeddings, are not well explained.\nIncomplete Empirical Evaluations: Missing results  in comparison tables and Lack of standard deviations or multiple run results weaken the robustness of the results presented.\nJustification For Why Not Higher Score:\nAll reviewers voted to reject the paper and I agree with their assessment.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "Official Review of Submission9463 by Reviewer 9qF7", "Subheading": "Official ReviewbyReviewer 9qF731 Oct 2023, 22:58 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper introduces a reinforcement learning framework tailored for the Flexible Job Shop Problem (FJSP). The methodology leverages graph neural networks, allowing the model to handle FJSP instances of varying scales. The main novelty consists of simultaneous generation and improvement: a generative model sequentially produces solutions while an improvement model refines them. Both models are trained concurrently via reinforcement learning. The approach is at least an order of magnitude faster than metaheuristics and outperforms dispatching rules and some previous RL approaches in terms of solution quality.\nSoundness:\n1 poor\nPresentation:\n1 poor\nContribution:\n2 fair\nStrengths:\nThe tackled problem is important in several practical scheduling applications. Unlike previous approaches that either only generate solutions in one shot or only learn to improve, the proposed approach trains two models to generate and improve at the same time, which could potentially provide the \u201cbest of both worlds\u201d, i.e., speed of one-shot generation and solution quality of improvement methods. The proposed two-stage approach and training is novel for scheduling problems to the best of my knowledge.\nWeaknesses:\nMy biggest concern is that the proposed approach seems to be only applicable to a specific scheduling problem (FJSP) with no variation in terms of constraints. In the abstract, the authors state that:\nIt is worth noting that this training paradigm can be readily adapted to other combinatorial optimization problems, such as the traveling salesman problem and beyond.\nhowever, there is 1) no empirical evidence to justify the claim and 2) no explanation of\nhow\nthis can actually be done. For instance, how can the improvement step be applied to the traveling salesman problem (TSP), especially considering the reward function? In several combinatorial optimization problems, it is hard to define a step-wise reward function, such as in routing problems such as the TSP. Moreover, the specific design of Section 3 seems to be over-fitted to the FJSP, with new problems requiring a substantial restructuring of the model.\nAnother important point is that the proposed generation-improvement method is not well justified in terms of performance. There is no ablation study on just using the generator model without any improvement:\nFrom the design of our framework, it can be seen that the generative model and the improved model can run independently, which means that we can only use the generative model to generate a complete solution or use the improved model individually to improve any feasible initial solution generated by other methods (such as random generation or PDRs).\nbut there is no result about this; in Table 3 only the improvement method alone is shown with other models. How would the\ngenerate\nonly perform? Moreover, a natural question would arise, namely why authors decided to go for a potentially more burdensome generate+improve method (in which the generator may potentially be worse due to over-reliance on the improvement model), and not just a generator. In these regards, it would be interesting to see how the same model with the generation part only would do.\nThe experimental section seems to be lacking some baselines - for instance, Table 1 only compares against OR-Tools and dispatching rules, but not against RGA and 2SGA and the 2 DRL baselines. Also, OR-Tools is missing the solution time, so it is difficult to assess how the proposed approach compares in solution time (given that the quality is already worse than the OR-Tools metaheuristics). Finally, no standard deviation has been reported nor multiple runs.\nIn terms of the quality of the paper, there is room for improvement. Aside from several typos, the writing feels sloppy, and there are missing references (\n(ref)\nin the paper, mk[?] in Table 2 and more), so I would suggest some revision. More importantly, in Algorithm 1:\nStore Transition $\\tau_{t+1}^g :< \\mathcal{S}_t; ~ \\mathcal{S}_{t+1} > \\text{into} ~EP_I$\nI believe this should be, in fact, $EP_I$ , given that $EP_G$ is not being used here.\nFinally, no code has been provided to reproduce the results.\nQuestions:\nWhy did you decide to use DuelingDQN, and not actor-critic algorithms such as PPO [29] or policy gradient methods as done in MatNet [17]*?\nAs in the \u201cweaknesses\u201d section, how would the model perform if only the generator was trained? And what if we trained with generator+improve but only used the generator for the solution?\nWhat is the number of improvement iterations $n_t$, and how was it selected?\nHow would the proposed method fare in larger-scale instances? [29] studies scale up to $100 \\times 60$.\n*Note: MatNet [17] is cited in the manuscript but not referenced throughout the text. It may be useful to at least briefly introduce the differences with the proposed method in the related works.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9463 by Reviewer swqB", "Subheading": "Official ReviewbyReviewer swqB31 Oct 2023, 15:53 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposes an end-to-end RL framework to solve the Flexible Job-Shop Problem (FJSP). The framework consists of two major components: a generation model that produces an assignment of operations that updates the partial solution, and an improving model that refines the current partial solution. By repeating the generation and improving steps until the complete solution is found, the proposed framework finds a solution for FJSP.\nThe authors evaluate the proposed framework with various-sized FJSP instances, and it is shown to outperform compounded Priority Dispatching Rules (PDR) but underperform Meta-heuristics (e.g., OR-tools).\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n3 good\nStrengths:\nThe proposed framework suggests a novel perspective for solving FJSP. Unlike the majority of iterative improving approaches that often perform improvement steps from a complete solution, the proposed framework employs \"improving\" actions during solution construction.\nWeaknesses:\nThe current manuscript still has room for improvement, including a more detailed explanation of the training.\nThe performance evaluation of the proposed framework seems quite limited, especially as the baselines are overly simplified in Table 1.\nQuestions:\nIt seems the number of improvement iterations $n_t$ would play a crucial role within the proposed framework. Could the authors provide further details on how to decide $n_t$? In the current manuscript, it is simply mentioned as a hand-crafted function depending on the iteration index $t$.\nWhat is GIM in Table 3? From the context, I assume it is the proposed method, but the acronym is never introduced.\nWhat is \"Generate+improve\" in Table 3? Is it different from GIM?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9463 by Reviewer ZW2D", "Subheading": "Official ReviewbyReviewer ZW2D31 Oct 2023, 13:14 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposes an RL based scheduling methods for Flexible Job Shop Problem. The approach empolys two graph neural network models, a generative model and an improvement model, which collaboratively solve the problem. At each timestep, the generative model progressively constructs a partial solution by adding a new component into the existing partial solution, and the improvement model refines this partial solution for better performance. Both models are designed to leverage inductive biases from the problem and its current partial solution, e.g., neighbor nodes from different types of edge. The models are trained end-to-end using the reward signal for each model, in an alternating manner to stabilize the learning of two models.\nThe proposed algorithm is evaluated with two experiments, one for synthetic datasets and the other for public benchmarks, and it showed superiority over several heuristics and DRL-based methods in terms of solution quality. Also, though the method failed to outperform the meta-heuristic algorithms, it showed comparable result while spending much less time than the meta-heuristics.\nSoundness:\n2 fair\nPresentation:\n1 poor\nContribution:\n2 fair\nStrengths:\nThis work proposed a new RL-based framework for solving FJSP, which combines the construction and the improvement processes so that they can be trained in end-to-end manner.\nThe method utilizes different graph representation that corresponds to a single partial solution, providing each model with relevant information. This approach is both interesting and convincing.\nAblation study for the two distinct models provides a good empirical evidence for the proposed architecture.\nWeaknesses:\n[Methods and Experiments]\nThis paper doesn't provide a clear rationale or justification for the use of various embeddings. Also, there's no ablation study for these design choices.\nThe method is evaluated only two public benchmarks, whereas the DRL baseline [1] has been tested on a more extensive set of benchmarks. This raises concerns about the comprehensiveness of the evaluation and potentially limits the generalizability of the proposed method's performance.\nThe reported performance of DRL baseline [1] is based on the greedy selection, while the method of this paper leverages sampling for improvement steps. For fairer comparison, the results from both greedy and sampling decoding should be included. Note that sampling performance reported in [1] for v_la task is better than the proposed method, while consuming more computation time.\n[Writings]\nThis paper has significant defects with clarity. \nFirst of all, there are too many typos, wrong spacing and inconsistent notaions. Below are some of them:\npage 1: 'PRD' \u2192 'PDR'\npage 2: There are many wrong spacing in Sec. 2, e.g, 'O_i,which', 'operations,O_{ij}', or \"...end of production.These two ...\"\npage 3: There is a wrong figure reference, '(in figure)'\npage 4: 'avenger' \u2192 'average'\npage 4: GAT has no reference\npage 5: 'avitation' \u2192 'activation'\npage 5: 'M_{ij}' suddenly pops up, which supposedly typo of {A_I}_{ij}, and suddenly A_J is used, which is definitely a typo.\npage 7: In the Algorithm 1, 'EP_I' \u2192 'EP_G' for the transition of generative model.\npage 7: There are several '(ref)'s in Sec 5.1, which should have been replaced by appropriate reference.\npage 8: In the text they say they use Gurobi Solver, but they report OR-Tools in the table.\nThroughout the paper, the authors use abbreviations without declare it, e.g., DRL in page 3, GAT in page 4, and GIM in page 7 (Algorithm 1)\nMoreover, the models are not clearly described, which makes it hard to fully understand the algorithm.\nFor example, in GAT Module section in page 5, it is unclear whether W is shared among different u's or not.\nAlso, the reward for each model is not stated mathematically, which introduces an ambiguity.\n[1] Song, Wen, et al. \"Flexible job-shop scheduling via graph neural network and deep reinforcement learning.\" IEEE Transactions on Industrial Informatics 19.2 (2022)\nQuestions:\nHow long it takes for training?\nWhy the 40 x 10 result is missing for OR-Tools?\nHow can this work be extended to other scheduling or CO problems?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9463 by Reviewer kC5r", "Subheading": "Official ReviewbyReviewer kC5r22 Oct 2023, 10:06 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe author proposed a deep reinforcement learning model to address the FJSP problem. This approach involves the simultaneous application of construction heuristics and improvement heuristics, enabling it to achieve better performance in shorter time on several public datasets.\nSoundness:\n1 poor\nPresentation:\n1 poor\nContribution:\n2 fair\nStrengths:\nBased on the claim of paper, it seems good to use construction heuristic to construct a better partial solution and use improvement heuristic to improve the partial solution.\nWeaknesses:\nActions (in Section 3.1) are critical, but not defined clearly. I have no problems with actions for construction heuristics, but actions for improvement heuristics are not well defined. In Section 3.2 \u201cInsertion Position Embedding\u201d (P5), the definition of insertion position is undefined clearly, and why the number of choices is (n+m) for each operation. Besides, it is also unclear about why the total number of insertion positions is equal to n\u00d7(n+m). For these unclear descriptions, there is no clue to understand the proposed method. Note that in Section 3.2 \u201cPolicy Model\u201d (P6), there is no way to understand the description \u201cObviously, there are at most m different insertion schemes for each improvement decision.\u201d\nFigure 2 is confusing and unconvincing. For example, why is 31 moved to the position after 11, not before 11? If it can also be moved to that before 11, I don\u2019t see the strategy.\nThe representation of operations is inconsistent and thus makes it hard to understand how the Insert Position Embedding works (There are $O_{ij}, O_j, O_{j(i)}, O_i$ in the article).\nLack of test results for public benchmark dataset. With comparison to [29], you should also compare with la(edata) and la(rdata). And you may test on the dataset which is referenced by [29] to improve the reliability of your method.\nPresentation comments:\nLack of spaces in many places. E.g., \u201cBoth the generative model and the improvement model will use formula(4) to select the action to be executed in the current state st at step t on their respective feasible action sets.The advantage value function is fitted by a parametric MLP\u201d\nIn section 5.1, \u201c In addition, we also used (ref1),(ref1),\u201d, and \u201cmk [? ]\u201d in Table2. Please carefully check the content.\nIn Algorithm1, \u201cE%K\u201d => \u201ce%K\u201d, the second \u201c$EP_I$\u201d => \u201c$EP_G$\u201d, etc. There should be more that you need to find out for fixing.\nThere is no data in some places in the tables, such as 40x10 for OR-Tools in Table1 and mk[?] for UB* in Table2.\nQuestions:\nI am still wondering about your method for the Machine Process Queue Embedding:\nIs $M_{ij} = 1$ if $O_j$ is processed on Machine $i$, or $O_j$ \u201ccan be\u201d processed on Machine $i$? What is the concept of model designing (or why it is designed in this way)?\nIt\u2019s not clear that \u201cJob Sequence Embedding\u201d, if $O_{ij}$ ($j$-th operation of Job $i$) is processed then $A_J(J_i, O_{ij})$ = 1?\nFlag For Ethics Review:\nNo ethics review needed.\nDetails Of Ethics Concerns:\nN.A.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}]}, "NFqFA2vCQV": {"paper_info": {"Keywords": "video-language representation learning, transfer learning", "Abstract": "Driven by the data-centric AI paradigm, pre-trained image-text representations exhibit a subtle alignment of visual and textual concepts. In light of images being a subset of videos, recent work is dedicated to transferring pre-trained image-text representations into the video-language domain, attracting widespread attention. Nevertheless, these efforts employ training strategies such as full fine-tuning or post-pretraining, which do not necessarily constitute the most optimal approaches for transferring general pre-trained representations. In this paper, we resort to the increasingly popular parameter-efficient transfer learning (PETL) approach, proposing AdptIP, to adapt the pre-trained CLIP model into the field of video-language representation learning. AdaptIP devises a hierarchical cross-modal adaptation approach, focusing on intra-modal temporal modeling and inter-modal fine-grained alignment within the video-language domain. Additionally, the pre-trained CLIP backbone is frozen to maintain a common prior and ensure efficient model training. Comprehensive experiments on video-text retrieval, video question answering, and video captioning benchmarks highlight the versatility, superiority and efficiency of AdaptIP. Code will be available soon.", "Primary Area": "representation learning for computer vision, audio, language, and other modalities", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9460", "PDF Url": "https://openreview.net/pdf?id=NFqFA2vCQV"}, "review_info": []}, "BQvbL2sFQx": {"paper_info": {"Primary Area": "representation learning for computer vision, audio, language, and other modalities", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Shift equivariance, Shift invariance, Downsampling, Convolutional neural networks", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "The performance of convolutional neural networks (CNNs) are thought to be insensitive to image shifts. However, recent studies have revealed that downsampling layers in CNNs result in inconsistent outputs for shifted input images. In this\nstudy, we present an approach for performing downsampling that ensures absolute shift equivariance. By employing model-agnostic downsampling method that leverages origin selection functions obtained from coordinate-independent statistics of the feature map, we can achieve perfect shift equivariance, while still adhering to the conventional downsampling procedures. Our method allows CNNs to exhibit both improved accuracy and perfect shift invariance for image classification, while also achieving shift equivariance in semantic segmentation benchmarks. Furthermore, we introduce a methodology for achieving shift equivariance without the need for any additional training process. This is accomplished by transferring pretrained weights and replacing existing layers with shift-equivariant\ncounterparts. Additionaly, we show that fine-tuning of the modified CNNs leads superior performance compared to previously proposed models.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9459", "PDF Url": "https://openreview.net/pdf?id=BQvbL2sFQx"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9459 by Area Chair gTWr", "Subheading": "Meta ReviewbyArea Chair gTWr05 Dec 2023, 13:26 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThe paper proposes a model-agnostic up-/down-sampling method that is exactly equivariant to translations (shifts) for images leveraging \u201corigin alignment\u201d. \n\nThe reviewers agree that while the motivation of the paper is clear, the presentation needs significant improvement, particularly the algorithmic description. Most importantly, reviewers 1rzF and 1mgo raised serious concerns about the choice of benchmarks, metrics, and baselines. Additionally, some of the presented gains are marginal and statistically negligible due to the stochastic nature of the training.  Finally, while reviewer gzfx was enthusiastic, they did not participate in the discussion nor provide further evidence to support their upbeat view of the paper vis-\u00e0-vis the other reviewers' less favorable opinions.\n\nThe authors did not provide any feedback, thus none of the concerns raised by the reviewers were addressed. \n\nDespite the clear motivation, the paper remained unmodified during the rebuttal period, leaving the presentation lacking and many of the concerns regarding evaluation and benchmarking unresolved. I recommend to reject.\nJustification For Why Not Higher Score:\nThe paper seems to have several evaluation issues, and it requires a rather large overhaul.\nOne reviewer provided a high-score, but they didn't engage on the discussion not provide further evidence substantiating their positive view in light of the other reviewers' less favorable views.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "Official Review of Submission9459 by Reviewer gzfx", "Subheading": "Official ReviewbyReviewer gzfx10 Nov 2023, 21:10Everyone", "Content": "Summary:\nThe authors propose and implement MASS, a method for downsampling that is exactly equivariant to shifts in images, making CNN networks exactly equivariant (or invariant) with respect to this symmetry in their downstream tasks (which are typically image classification). The method is implemented in a \u201cmodel-agnostic way\u201d, and can be used to invariantize pre-trained CNNs. Its performance is demonstrated on standard data sets.\nSoundness:\n4 excellent\nPresentation:\n2 fair\nContribution:\n3 good\nStrengths:\nThe problem of imposing exact shift equivariance (in CNNs) is important in current literature and applications. The proposed solution is simple, robust, and easily applicable to pre-existing methods.\nWeaknesses:\nMain\n* The algorithmic/mathematical presentation should be clearer. Occasionally notation appears that has not been precisely defined (e.g. $S_o$ on pg. 4 or the use of the $y$ variable). I specifically find Figure 1 hard to understand. \n\n* A substantial effort is made to separate the proposed method from general polyphase sampling, but the exact reason behind the latter\u2019s \u201cperformance degradation\u201d should be explained more rigorously, as this is what would set it apart in applications.\n\nMinor\n* There are several typos, and all acronyms need to be defined upon first appearance.\nQuestions:\nRegarding the choice of selection rule:\n* In what way does the choice of function matter? Can it be determined adaptively, if the noise distribution is known?\n* And finally, is it always possible to find a good function regardless of the level of noise?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n8: accept, good paper\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9459 by Reviewer 1rzF", "Subheading": "Official ReviewbyReviewer 1rzF06 Nov 2023, 18:32 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper presents an approach to shift equivariant up- and downsampling in convolutional neural networks.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\n- The paper presents a very compelling motivation for shift equivariance based on industrial applications\n- The paper seems to provide the shift-equivariant scheme that is consistent by design and also compatible with multiple architectures\n- The proposed approach in principle could be used without fine-tuning if some accuracy loss is acceptable, unlike some other methods in the literature\nWeaknesses:\n- The description of theory is not at all clear, especially the part related to figures. Maybe if I read this paper 2 or 3 times, I will eventually understand what color schemes imply in Figures 1,2,4. But I want to be able to understand this from the first glance. Figures should serve the purpose of clarifying things and not making them more obscure. Without the explanation of what colors are supposed to signify and explain in the figures - it is impossible to quickly understand what they are supposed to clarify.\n- The compelling motivation for shift equivariance is not supported by problem specific datasets. All experiments are done on generic datasets. CIFAR-10 does not seem to fit the motivation at all with its 32x32 images. It seems like a misfit for the purpose of the paper. I expect that the industrial applications involve high-resolution imagery. If authors can provide results on high-resolution datasets, especially from the industrial domain this will make the results a lot more compelling. There is a recent dataset described here: https://arxiv.org/pdf/2303.06673.pdf. I am sure that more search will reveal more datasets like this. I remember encountering similar problems on kaggle.\n- Consistency metric defined in equation (4) does not make any sense to me. What does it measure, what is x and y? Is it pointwise pixel match, if so, why there is no summation over pixels? One of the closing brakets is missing.\n- Baselines used in experimental tables are not explained well. As a result, the experiments do not seem persuasive\n    - What is the reference for DDAC and LPF?\n    - Why LPS is not included in Tables 2,3?\n    - Why Table 3 does not contain same baselines as Table 2? It seems that a few of the baselines in Table 2 are very effective. It may well be that APS with enhancements presented in Table 3 might be as effective or better than MASS?\n- Results in Table 2 are marginal and statistically insignificant. To me, the value of this result is approaching 0, because most confidence intervals overlap. It does not make sense to use bold font to signify the best model, when the best model is not significantly different than another model.\n- From Tables 2-4, I do not see a decisive value of the proposed approach with respect to other approaches such as LPS-DDAC-3 or ASP-DDAC-3. Why do we need this approach, what is the value?\n- I am not sure I see value in using the pretrained version of any of the approaches discussed in the experimental section. What is the point, can you explain in detail the actual use case? When the networks are fine-tuned properly, many of them achieve very similar results.\n- Table 5 confirms previous concerns.\nQuestions:\n- page 2: missing reference. \"2016; ?), and group operations\"\n- page 6: typo \"Schemetic of MASS\" -> \"Schematic of MASS\"?\n- is your approach compatible with visual transformers?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9459 by Reviewer pcfU", "Subheading": "Official ReviewbyReviewer pcfU05 Nov 2023, 19:00 (modified: 03 Dec 2023, 13:58)EveryoneRevisions", "Content": "Summary:\nThe proposed work suggests a downsampling technique that is shift equivariant by equivariant origin alignment. This method is well suited for adapting to the existing layer and can utilize the weights of pre-trained models. Moreover, the proposed models outperformed other shift equivariant techniques without introducing more learnable parameters.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nThe proposed method achieves perfect shift equivariance and performs better in classification tasks, even without any learnable sampling parameters (compared to the LPS). It is also well-suited for reusing pre-trained weights.\nWeaknesses:\n1. The proposed method resembles building equivariant layers with canonical functions [a]. The proposed method can be seen as a special case of the mentioned work for shift equivarinace. This limits the contribution of the paper. \n\n2. The benefit of the proposed technique compared to the existing method (APS, LPS) is poorly described.\n\n3. The improvements are marginal.\n\n\na. Equivariance With Learned Canonicalization Functions\nQuestions:\n1. Figure 2 caption: \u201cOn the other hand, in the case of polyphase sampling, only data from specific regions is retained, which may result in potentially suboptimal representations. MASS-Max-pool combines the advantages of selecting suitable representatives and ensuring shift equivariance.\u201d\u2014 I do not entirely understand the statement. We can perform convolution with max filter followed by LPS. What is the extra benefit of MASS-Max-pool?\n2. Section 4.1 \u201c there exist only $s^2$ unique sampling origins represented as o \u2208 {(0, 0),(0, 1), ...,(s, s)}.\u201d \u2014 should it be \u201c{(0, 0),(0, 1), ...,(s-1, s-1)}.\u201d?\n3. While training from scratch as the MASS-Max-pool shifts the input to match the new calculated origin, does it likely introduce unwanted data augmentation? Especially if the pooling window is large.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9459 by Reviewer 1mgo", "Subheading": "Official ReviewbyReviewer 1mgo01 Nov 2023, 07:51 (modified: 29 Nov 2023, 01:50)EveryoneRevisions", "Content": "Summary:\nThis paper addresses the problem of how to preserve shift-equivariance property for convolutional neural networks. Specifically, the authors merely consider circular shift operation over the input image sample, and simply extend an existing method APS (adaptive polyphase sampling, proposed by Anadi Chaman and Ivan Dokmani\u0107 in their CVPR 2021 work) by incorporating a pre-defined selection function for determining the origin which can accurately fit the shift operation. Experimental validation is conducted on image classification and semantic segmentation tasks.\nSoundness:\n2 fair\nPresentation:\n1 poor\nContribution:\n2 fair\nStrengths:\n+ The problem, i.e., how to preserve shift-equivariance property for convolutional neural networks, is critical.\n\n+ The proposed method is simple and hand-crafted even though its implementation is not clear.\n\n+ Comparative experiments are conducted on both image classification (with CIAFR-10 and ImageNet-1K datasets) and semantic segmentation (with PASCAL VOC dataset) tasks.\nWeaknesses:\n- The method and presentation.\n\nIn this work, although the authors addresses a fundamental research problem, how to preserve shift-equivariance property for convolutional neural networks, the proposed method called MASS is rather incremental, lacking new tech insights. To the best of my knowledge, MASS is merely a simple modification of existing work APS (adaptive polyphase sampling) proposed by Anadi Chaman and Ivan Dokmani\u0107 in their CVPR 2021 paper. Specifically, the authors use a pre-defined selection function for determining the origin with ASP which can accurately fit circular shift operations over the input image sample.  Generally, I have not seen any insightful differences against APS. \n\nThe presentation of the method is poor: 1) usually no explanations for notations and terms appeared in formulas; 2) no explanations/details on the formulation of the proposed MASS; 3) rather coarse descriptions for Figure 1 and Figure 2; 4) some sub-figures are wrong, e.g., two sub-figures for MASS in channel 1 of Figure 1 are not consistent to the others.\n    \nThe writing of the paper is also poor. Please see my comments in \"Others\" part for details.\n\n- The limitations.\n\nThe authors did not discuss on the limitations of the proposed method.\n\n- The experiments.\n\nNote that the authors claim that the pre-defined selection function for determining the origin can accurately fit the shift operation. However, the authors did not provide any details on how to implement it in experiments. This makes experimental comparison confusing. \n\nComparison is limited to APS.\n\nExperiments are not convincing. On CIFAR-10 dataset, the proposed MASS brings very marginal gains to APS. On PASCAL VOC dataset, MASS performs worse than APS. However, on ImageNet-1K dataset, MASS is much better than APS. What are the root reasons? \n\nThere is no ablation to study how does the proposed method MASS work. \n\nHow about the performance of MASS under other shift operations to the input image sample instead of circular shift operations?\n\n- Others.\n\nThe writing can be improved significantly. There exist numerous typos, grammar errors and inaccurate descriptions throughout the whole paper. Here, I just list some example errors in the \"Related Works\" section:\n\n1. \"Cheng et al.,2016; ?\" -> an inaccurate citation;\n2. \"a lack of shift equivariance occur\" -> \"a lack of shift equivariance **occurs**\";\n3. \"While careful augmentation strategies substantailly improves\" -> \"While careful augmentation strategies **substantially improve**\";\n4. \"Another line of research is to apply anti-aliasing low-pass filter, which originate\" -> \"Another line of research is to apply anti-aliasing low-pass filter, which **originates**\";\n5. \"This anti-aliasing concepts are\" -> \"This anti-aliasing **concept is**\";\n6. \"The first absolute shift-invariant method for image classification tasks are proposed \" -> \"The first absolute shift-invariant method for image classification tasks **is** proposed\";\n7. \"While the selection of the polyphase components of APS is based on the l2 norm, learnable polyphase sampling (LPS) generalize to select\" -> \"While the selection of the polyphase components of APS is based on the l2 norm, learnable polyphase sampling (LPS) **is generalized** to select\";\n8. Many citations are not formal, even to APS.\n\n----------------------------------------------------- Post Rebuttal----------------------------------------------------------\n\nAs the authors did not provide any responses to my concerns, I downgrade my rating from \"borderline reject\" to \"reject\".\nQuestions:\nPlease refer to my detailed comments in \"Weaknesses\" for details.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Post rebuttal rating", "Subheading": "Official CommentbyReviewer 1mgo01 Dec 2023, 19:18 (modified: 15 Mar 2024, 00:27)EveryoneRevisions", "Content": "Comment:\nAs the authors did not provide any responses to my concerns, I downgrade my rating from \"borderline reject\" to \"reject\""}]}, {"Heading": "Official Review of Submission9459 by Reviewer QNhG", "Subheading": "Official ReviewbyReviewer QNhG31 Oct 2023, 16:00 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper presents a strategy to achieve perfect equivariance in convolutional neural networks. That is, the model produces exactly the same output when the input image is shifted horizontally or vertically. This is achieved by preserving statistics of the positions of the downsampling process in the pooling layers of convolutional networks. The results indicate that the method works perfectly for downsampling (classification) and upsampling (segmentation) operations without the need to re-train the models.\nSoundness:\n4 excellent\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\n* The problem is important and the paper is well motivated.\n* The solution is simple and generic, can be applied to any convolutional model.\n* The results are strong. 100% equivariance is achieved in all experiments, demonstrating the effectiveness of the proposed solution.\n* The approach works on downsampling and upsampling paths of CNNs. The evaluation includes image classification and semantic segmentation.\nWeaknesses:\nMain comments:\n\n* Equivariance is demonstrated only for inference (test) time. It is unclear how the method would facilitate equivariance during training. In other words, by implementing MASS in all pooling layers, what augmentations would be unnecessary when training a new model? The only experiments that involved training a model from scratch were conducted with the CIFAR dataset, but the augmentation procedure was not explained. More analysis of equivariance during training would be informative.\n* In general, the explanation of the method has a few gaps that could be better presented and clarified. For instance, the paper indicates that previous work ignores classical sampling theory, but how MASS uses classical sampling theory is not explained later. Also, it is unclear what the authors mean by \"MASS meticulously preserves the initial downsampling process\". The introduction indicates that MASS uses input data statistics to select the origin, but these statistics are not clearly defined later. The procedure could be more formally presented to avoid confusions.\n* The paper mentions that non-equivariant methods can display severe accuracy drops, but this does not seem to be reflected in the results. The consistency of other methods is usually above 80% and the classification rate remains high. If shifts are introduced randomly, the accuracy of a non-equivariant method can change every time. Reporting how the results change with the amount of shift introduced to break the classification of a model would be informative.\n\nOther comments:\n* It is unclear if the method results in computing or memory overhead (even if minimal, what additional operations / variables are added compared to regular pooling).\n* Some acronyms are not clearly defined, such as APS, LPF and DDAC. Table 2 uses them extensively without citations or explanations to what they refer to exactly.\n* Some minor typos: equivaiant, \"architectures.architectures\", experimants, inializing.\nQuestions:\n* Does the proposed method remove the need for using certain augmentations during training?\n* Are the accuracy results reported without shifts? The experimental procedure is unclear, please explain.\n* Can you add results of how shifting affects accuracy in shift-sensitive models?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}]}, "eR4W9tnJoZ": {"paper_info": {"Primary Area": "general machine learning (i.e., none of the above)", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "creative content, digital creatives, attention, personalization, content optimization, content generation, generative AI", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "TL;DR": "AI-based content-generation method based on the understanding that digital communication differs significantly from other media.", "Abstract": "Media platforms compete for users\u2019 attention. Their reach crucially depends on algorithmic real-time bidding and efficiency of hyper-personalized, rapidly generated, and user-optimized content. Attention is, although, a scare and fleeting quantity, often awarded less than 1 second per stimulus. Thus, the current strategy is to rely on the vast amount of user-generated data to mimic the content to the user. The underlying assumption is that this is sufficient incentive for attention. This strategy has evidently failed. As witnessed by the alarmingly low or short-lived successes of campaigns in recent times. This mismatch is exacerbated because most content consumed today is digital. Whereas strategies for digital content mimic our past understanding from mass-media. Hence, we formalize a new understanding of communication, specifically for the digital mediums. We prove that the digital medium needs a new understanding of communication protocols. To that end, we take a first principles approach to the new communication protocol: the neurological representations of communication, specifically, where the communication happens in less than 1 second per stimulus. First, we break down and elaborate on this neurological representation of decision-making. Next, we proffer use of our behavioural communication model for generation and optimization of content creatives. To that end, we elaborate methods for rapid, AI-generation content, increasing the efficiency of visual communication on digital media. Within this exploration we include themes of Hyperpersonalization and Search-engine optimization. Thus, we find that strategically produced content exhibits stronger associations to users\u2019 nonconscious needs, wants and goals, which elicits user attention and content-diversity significantly.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9458", "PDF Url": "https://openreview.net/pdf?id=eR4W9tnJoZ"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9458 by Area Chair poho", "Subheading": "Meta ReviewbyArea Chair poho08 Dec 2023, 02:14 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThis paper explores how ads generated with LLMs could lead to higher click-through rate. The strength of the paper includes the problem definition and  the simple approach. The limitations of the paper include 1) lack of technical novelty, 2) limited scalability due to manual prompting, 3) insufficient evaluation. All reviewers are in agreement that the paper needs much more work, and there is no response from the authors.\nJustification For Why Not Higher Score:\nAll reviewers recommend rejecting this paper.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "Official Review of Submission9458 by Reviewer ZAnW", "Subheading": "Official ReviewbyReviewer ZAnW30 Oct 2023, 23:21 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper proposes a method for rapid, AI-generated content, increasing the efficiency of visual communication on digital media. Within\nthis exploration the authors include themes of Hyperpersonalisation and Search-engine optimation.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nI have not found any strengths of this paper.\nWeaknesses:\nThe theme of this paper may not be closely related to the conference, as it is only an engineering specification and lacks theoretical explanation.\nThe presentation of the paper is chaotic, making it difficult to read.\nThe method mentioned in the paper, which utilizes ChatGTP to generate accurate prompts and generates high-quality digital advertisements using this prompts and a large text-to-image model, has been widely applied in the engineering field and therefore lacks innovation.\nQuestions:\nPlease refer to Weaknesses.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9458 by Reviewer 1HkZ", "Subheading": "Official ReviewbyReviewer 1HkZ30 Oct 2023, 09:30 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper mainly investigates whether generative AI can produce content to attract the user and explains the procedure of content generation from the neuroscience perspective.\nSoundness:\n1 poor\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThis paper introduces a communication protocol to explain how the brain has been triggered, the entire process is fluent and reasonable.\nThe content of the pre-research is sufficient.\nThe strategy of the prompt is meaningful.\nWeaknesses:\nThis work is too simple, just using the existing GenAI to produce the context and comparing it with the corresponding items.\nThe prompt is hand-crafted, and cannot be applied flexibly.\nThe number of samples in the experiment is too small, and the experiments should cover more scenarios.\nQuestions:\nHow do you confirm the prompt is reliable and the output of the GenAI is following the rules?\nSince it's an online experiment, why not invite more people?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n1: strong reject\nConfidence:\n5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9458 by Reviewer DN2b", "Subheading": "Official ReviewbyReviewer DN2b19 Oct 2023, 07:30 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper proposes a framework that leverages Generative AI to create Ad creatives that aim to increase the Click-through rate of advertisements. The framework and ad creative generation leverage four principles: 1) the evolutionary category need, 2) past memories and brand guidelines; 3) the strongest emotional memory, and 4) context with photographic details.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThe paper does a good job of motivating and explaining the problem, as well as providing all the necessary details and motivation to understand the necessary background. Also, the paper focuses on an interesting aspect of generative AI and how it can be used to generate ad creatives with the goal of increasing click-through rates that, over the years, have been declining. Overall, I think that this work has the potential to inform various interested stakeholders, including advertisers, policymakers, and social media operators. Also, I like the paper\u2019s approach that aims to leverage the power of Generative AI (particularly ChatGPT) to generate content based on principles obtained from the neuroscience field.\nWeaknesses:\nMy main concerns with the paper are related to the framework\u2019s evaluation. I believe that the evaluation is quite limited and simplistic, given that the sample of the recruited participants is biased (the overwhelming majority being from India) and the evaluation focuses on only two products. I suggest that the authors explain and motivate how they perform the user recruitment procedure and the reason why they selected the two products. Overall, given these limitations, it\u2019s unclear whether the paper\u2019s results are generalizable.\nAdditionally, the paper fails to explain how this study is different from previous efforts that aim to understand the use of neuromarketing methods without the use of Generative AI to create the ad creatives. The presented framework can also be applied by people to generate ad creatives, so its unclear if the novelty of this work lies in the formulation/use of the framework or the combination of the framework with Generative AI models like ChatGPT. I suggest to the authors to better contextualize their work and better explain the novelty of this work.\nAlso, the paper does not explain how the envisioned framework will be applied in practice. The paper\u2019s evaluation defines a set of prompts that are very specific to the products that are studied and generates creatives that are then subsequently used to compare the user perceptions vs. ad creatives that simply show the product with a white background. Overall, it\u2019s unclear on whether the envisioned framework can be applied in practice without great input and effort from experts that will guide the generation of the ad creatives.\nIn addition, there is a disconnection between the motivation of the work and the framework/evaluation. The framework does not account for user personalization, which is an important aspect when considering the ad ecosystem. So I am wondering how the paper is planning to incorporate user personalization in this framework and how Generative AI models can assist in this, especially when considering the privacy concerns that may arise from sharing user-specific data with companies that offer LLM solutions (e.g., OpenAI).\nTo summarize, I believe that this work is interesting and important, however, at this stage, I believe that the paper is not ready for publication. In addition to the above concerns, I would like to make the following suggestions to the authors (mainly minor issues):\nThere are a couple of references listed as Anonymous, when they are not Anonymous so I suggest fixing these issues.\nConsider not using pie charts for the evaluation results, given that it is one of the worst visualization methods.\nQuestions:\nHow did you recruit participants, and why most of them are from India? How can the recruitment approach affect the presented results?\nHow are the two products selected? Are these products popular in India, where most participants are from?\nHow is this study different from previous efforts studying the use of neuromarketing methods vs. plain advertisements like the ones shown to the participants (plain background with the product in the middle)? Is the novelty of the work the use of ChatGPT to generate the ad creatives?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9458 by Reviewer Bg7M", "Subheading": "Official ReviewbyReviewer Bg7M15 Oct 2023, 23:49 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper leverages the content generated by generative AI to enhance the critical last moments of decision-making. Grounded in the understanding that long-duration decisions are the cumulative result of numerous micro-decisions, the author dissects the final seconds of an e-commerce purchase into multiple stimuli. Subsequently, the author introduces a four-point prompt strategy, informed by the outcomes of this analysis. After that, to validate the efficacy of this prompt strategy, the author conducted a series of experiments.\nSoundness:\n1 poor\nPresentation:\n2 fair\nContribution:\n1 poor\nStrengths:\nThanks for your interests in ICLR! Overall, this is an interesting paper on a topic which is of interest to ICLR Conference. It offers valuable insights into the application of neuroscientifically designed content to enhance ad click-through rates. The paper astutely recognizes the promise of leveraging Generative AI for this purpose. Building upon this foundation, the author thoughtfully presents four distinct prompt strategies and supports them with well-structured experiments, thus substantiating the validity of their approach.\nWeaknesses:\nWhile the author presents a comprehensive theoretical framework and provides clear and detailed insights into the prompt strategies, there is room for improvement in the experimental validation of the proposed techniques. As outlined in the paper, the experiments are limited to a single product tested on a sample of 236 participants. Given the potential applicability of this technique to a wide range of products, the scope of experimentation appears somewhat narrow. Expanding the experiment set to encompass a more diverse array of products would strengthen the paper's claims.\nAdditionally, the manual design of prompts by the author may not be a scalable solution when considering the need for ad design across a vast array of products. Further exploration of automated or semi-automated prompt generation methods could enhance the paper's practicality and applicability in real-world scenarios.\nQuestions:\nHave you explored the possibility of automating the prompt generation process for various products?\nCould you provide insights into any supplementary experiments conducted to further validate the effectiveness of the prompt strategy outlined in your paper?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}]}, "jx6njBKH8E": {"paper_info": {"Supplementary Material": "zip", "Primary Area": "societal considerations including fairness, safety, privacy", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "large language model, training data extraction, fine-tuning, pseudo-labeling with membership, privacy", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "TL;DR": "We present a novel attack method that amplifies training data exposure in language models by fine-tuning them with pseudo-labeled memberships.", "Abstract": "Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization. This paper introduces a novel attack scenario wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the exposure of the original training data. This strategy differs from prior studies by aiming to intensify the LM's retention of its pre-training dataset. To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data. However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging. To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM. We subsequently fine-tune the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their membership probabilities. Our empirical findings indicate a remarkable outcome: LMs with over 1B parameters exhibit a four to eight-fold increase in training data exposure. We discuss potential mitigations and suggest future research directions.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9456", "PDF Url": "https://openreview.net/pdf?id=jx6njBKH8E"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9456 by Area Chair UhuM", "Subheading": "Meta ReviewbyArea Chair UhuM05 Dec 2023, 08:13 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThe authors propose a new type of setting, where the goal is to amplify training data exposure - e.g. make trainin data easier to extract with subsequent membership inference attacks. They do this by fine-tuning the data on pretraining dataset-like data, using LLM-generated-text detetors.\nStrength: most reiewers agree that this is a novel approach to an important problem.\nWeakness: the experimental validation seems questionable (JBPh), relies on LLM text detection methods that are often known to be unreliable (ZqEj)\nJustification For Why Not Higher Score:\ncore issues like experimental design and reliance on zero-shot detectors without careful testing and validation are problems that probably should be resolved.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "A kind reminder regarding our response", "Subheading": "Official CommentbyAuthors21 Nov 2023, 14:14Everyone", "Content": "Comment:\nDear Reviewer,\nAs the ICLR rebuttal period is approaching its end, we kindly remind you to review our submitted response. Your feedback is essential for finalizing our work.\nThank you for your attention.\nBest regards,\nThe Authors", "Replies": [{"Heading": "Remarks and Appeciation", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:05Everyone", "Content": "Comment:\nDear Reviewers,\nAs the rebuttal period draws to a close, we sincerely hope our response, crafted based on your constructive comments, aids in furthering a positive evaluation of our work. We are grateful for the opportunity to refine our submission with your insights.\nThank you for your thoughtful review and time.\nBest regards,\nThe Authors"}]}, {"Heading": "Official Review of Submission9456 by Reviewer VkQN", "Subheading": "Official ReviewbyReviewer VkQN01 Nov 2023, 05:27 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis work investigates how model fine-tuning may potentially make the models more vulnerable to leaking their pre-train dataset. The authors apply the machine-generated text with more like human-written to fine-tune the language models. Reinforcement learning with self-generation is employed to fine-tune the models. To demonstrate the effectiveness of their approach, the author conducts experiments on six datasets over 6 language models with different amounts of trainable parameters.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n3 good\nStrengths:\nThis work proposes a new perspective to make data extraction attacks on pre-training language models easier.\nThis study performs experiments across diverse datasets and various models, enhancing the generalizability of the empirical analysis.\nWeaknesses:\nWhile the author explores various models in the experiments, there is a noticeable lack of diversity in their architectures; all the studied models originate from the same architectural family.\nIt would be valuable if the authors could show some qualitative results, e.g., reconstructed text in the model fine-tuning with their approach and the standard approaches.\nThere is no model utility performance comparison between this work and the other work.\nQuestions:\nsee weakness.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer VkQN", "Subheading": "Official CommentbyAuthors14 Nov 2023, 10:15 (modified: 14 Nov 2023, 10:25)EveryoneRevisions", "Content": "Comment:\nThank you for acknowledging our main contributions:\na new perspective for training data extraction attacks.\nexperimenting across various datasets and models to enhance the generalizability.\n[W1] While the author explores various models in the experiments, there is a noticeable lack of diversity in their architectures; all the studied models originate from the same architectural family.\n[W1] In this paper, instead of demonstrating experimental results across various architectures (\ne.g.\n, GPT-Neo-X, LLaMA, and BLOOM), we focused on experiments with different model parameters within a single architecture, OPT. This decision was based on the following reasons:\nConsistency in Recent LM Architectures\n: We believe that showing the performance of our approach in different architectures would not significantly enhance the contribution of this study, as recent LMs generally use the Transformer decoder-based architecture. Following your valuable suggestions, we will mention the application to other structures like BERT and T5 as promising future work in our paper. We will gladly consider your advice for expansion in our future research.\nLimitation of Experimental Resources\n: Due to the limitations in experimental resources like GPU, storage space, and workforce (as mentioned in Section B.1), we conducted our experiments with the OPT family only, reducing the diversity of model architectures without compromising the validity of our approach.\n[W2] It would be valuable if the authors could show some qualitative results, e.g., reconstructed text in the model fine-tuning with their approach and the standard approaches.\n[W2] We understood your mention of 'standard approaches' as 'standard TDE attack' (\ni.e.\n, without fine-tuning the model). As we can add the qualitative results you mentioned in time for the final submission, we will enhance the final version by comparing the differences in reconstructed texts between LMs with and without fine-tuning.\n[W3] There is no model utility performance comparison between this work and the other work.\n[W3] As emphasized in Sections 3.1 and 6, the adversary possesses a replica of the target LM, thus eliminating the need to consider a decline in the model's utility, like validation perplexity. Also, as this is the first study amplifying LM exposure through adversarial fine-tuning, we used the TDE attack results on the reference LM as a baseline (in Table 2) instead of comparing performance with other works.\nThank you for the constructive comments. We hope these explanations resolve the concerns. Further questions or suggestions would also be appreciated."}]}, {"Heading": "Official Review of Submission9456 by Reviewer ZqEj", "Subheading": "Official ReviewbyReviewer ZqEj01 Nov 2023, 03:17 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper proposes a new attack strategy to increase the exposure of private training data from pre-trained language models. The main contributions are:\nThe paper introduces a novel scenario where an attacker fine-tunes a pre-trained language model with self-generated texts that are pseudo-labeled based on their machine-generated probabilities. The paper assumes that texts with lower machine-generated probabilities are more likely to contain training data.\nThe paper uses a zero-shot machine-generated text detection method (DetectGPT) to calculate the perturbation discrepancy of each generated text, and a reinforcement learning from human feedback method (RLHF) to fine-tune the target language model to favor texts with lower perturbation discrepancy.\nThe paper evaluates the proposed attack strategy on six versions of the OPT language model and shows that it can amplify the training data exposure by four to eight times compared to the reference models. The paper also analyzes the extracted samples and discusses potential mitigations and future research directions.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nOriginality: The paper introduces a novel attack scenario where an adversary fine-tunes a pre-trained language model to amplify the exposure of its training data. This strategy differs from prior studies by aiming to intensify the model\u2019s retention of its pre-training dataset. The paper also proposes a two-step approach to achieve this goal, involving pseudo-labeling based on machine-generated probabilities and reinforcement learning with self-generations. To the best of my knowledge, this is the first work to explore such an attack strategy and demonstrate its feasibility and effectiveness.\nQuality: The paper is well-written and provides sufficient technical details and empirical evidence to support its claims. The paper follows the standard structure of an ICLR submission and adheres to the formatting guidelines. The paper also discusses potential mitigations and countermeasures against the proposed attack, as well as open questions for future research. The paper uses appropriate references and citations to acknowledge previous work and situate its contribution in the literature.\nClarity: The paper is clear and easy to follow. The paper defines the threat model, the adversary\u2019s capabilities and objective, and the main steps of the attack strategy in a precise and coherent manner. The paper also explains the rationale and intuition behind each step of the attack, as well as the challenges and assumptions involved. The paper uses figures, tables, and equations to illustrate the key concepts and results. The paper also provides qualitative analysis of extracted samples and discusses the limitations and implications of the attack.\nSignificance: The paper addresses an important and timely problem of training data extraction attacks on neural language models, which pose serious privacy risks for both data owners and model users. The paper demonstrates that such attacks can be amplified by adversarial fine-tuning, which can increase the exposure of sensitive training data by up to eight times. The paper also provides insights into the factors that affect the vulnerability of language models to such attacks, such as model size, training dataset type, and perturbation function. The paper contributes to advancing the understanding and mitigation of privacy threats in language modeling.\nWeaknesses:\nThe paper does not specify how the adversary evaluates the effectiveness of the TDE attack, and what are the assumptions and limitations of the attack scenario. The paper also does not compare or contrast its attack strategy with existing TDE attacks in terms of feasibility, scalability, and practicality.\nThe paper relies on a single zero-shot machine-generated text detection method (DetectGPT) to pseudo-label the self-generated texts, without considering other possible methods or evaluating the robustness and reliability of DetectGPT. The paper also does not explain how the perturbation discrepancy correlates with the membership probability or the presence of training data in the generated texts. The paper does not account for the potential confounding factors or sources of bias in its experiments, such as the choice of prompts, sampling methods, hyperparameters, datasets, and evaluation metrics.\nThe paper does not discuss the ethical and social implications of its attack strategy. The paper proposes a novel form of TDE attack that can amplify the exposure of sensitive and private information from pre-trained LMs, but does not address the potential harms or risks that such an attack can pose to individuals, organizations, or society at large.\nQuestions:\nIn Figure 1, perturbed LM generations are divided into two classes: \"good answer\" and \"bad answer,\" based on the value of d(x). Was the threshold for d(x) chosen empirically?\nIn Table 1 for Epoch 1, the three values with the lowest test accuracy are highlighted. In contrast, for Epoch 2, the highlighted values represent the top-3 highest test accuracy. There are no highlights in Epoch 0 and Epoch 3. Should the highlighting approach be consistent, or was this variation done intentionally for a specific reason?\nFlag For Ethics Review:\nNo ethics review needed.\nDetails Of Ethics Concerns:\nNot applicable\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer ZqEj (1/3)", "Subheading": "Official CommentbyAuthors14 Nov 2023, 10:25 (modified: 14 Nov 2023, 10:57)EveryoneRevisions", "Content": "Comment:\nThank you for acknowledging our main contributions:\na novel attack scenario where an adversary fine-tunes a pre-trained language model to amplify the exposure of its training data.\nwell-written and provides sufficient technical details and empirical evidence to support its claims.\nclear and easy to follow.\nadvancing the understanding and mitigation of privacy threats in language modeling.\n[Q1] In Figure 1, perturbed LM generations are divided into two classes: \"good answer\" and \"bad answer,\" based on the value of d(x). Was the threshold for d(x) chosen empirically?\n[Q1] Instead of setting a separate threshold to distinguish between good and bad answers for d(x), we decide based on their relative magnitude. We paired 100,000 generations into 50,000 pairs and, within each pair, pseudo-labeled the generation with the lower and higher d(x) as the good and bad answer, respectively. Consequently, we end up with 50,000 good answers and 50,000 bad answers. Please refer to Section 4.1 for more details.\n[Q2] In Table 1 for Epoch 1, the three values with the lowest test accuracy are highlighted. In contrast, for Epoch 2, the highlighted values represent the top-3 highest test accuracy. There are no highlights in Epoch 0 and Epoch 3. Should the highlighting approach be consistent, or was this variation done intentionally for a specific reason?\n[Q2] In Table 1, we highlighted the epoch with the highest classification accuracy 'for each model' (i.e., the highest value in each row is displayed). In the final version, we will enhance readability by adding shading to the rows in the table to aid the reader's understanding.\n[W1-a] The paper does not specify how the adversary evaluates the effectiveness of the TDE attack, and [W1-b] what are the assumptions and limitations of the attack scenario. [W1-c] The paper also does not compare or contrast its attack strategy with existing TDE attacks in terms of feasibility, scalability, and practicality.\n[W1-a] The evaluation metric for the effectiveness of an adversary's TDE attack is the true positive (Section 5.2). Specifically, the adversary generates 100,000 texts containing precisely 256 tokens from the target LM and then reports the number of generations extracted from the training dataset. In this case, we consider a generation to be 'memorized (or extracted)' if exactly 50 consecutive tokens in the generation are present in the training data.\n[W1-b] As you pointed out, our research assumes a restricted white-box scenario. The restricted white-box scenario assumed in our study is often considered unrealistic in previous research [1] and thus has not been explored yet. However, this assumption is increasingly essential for the following reasons (details in Section 3.1):\nIncrease in Public LMs\n: Efforts towards open science are leading to an increase in publicly available LMs, which pose potential risks related to our assumption.\nRisk of LM Weight Leakage\n: Sophisticated black-box model extraction attacks make our assumption more realistic.\nCustomized LM API Services\n: Recently, services that support customizing back-end LMs by fine-tuning them with user datasets are emerging (e.g., OpenAI [3]), and our research could be a severe concern in such cases.\nSpecifically, item 3 is not mentioned in our paper, so we plan to enhance the content by adding this in the final version."}, {"Heading": "Response to Reviewer ZqEj (2/3)", "Subheading": "Official CommentbyAuthors14 Nov 2023, 10:26 (modified: 14 Nov 2023, 10:57)EveryoneRevisions", "Content": "Comment:\n[W1-c] As feasibility and practicality are already discussed in [W1-a] and Section 3.1, we will address the remaining concern\u2014scalability. In this paper, instead of demonstrating experimental results across various architectures (\ne.g.\n, GPT-Neo-X, LLaMA, and BLOOM), we focused on experiments with different model parameters within a single architecture, OPT. This decision was based on the following reasons:\nConsistency in Recent LM Architectures\n: We noted that most recent LMs are commonly based on the Transformer decoder architecture. Therefore, we judged that showing the applicability of our approach across various architectures would not significantly enhance the contribution of our research.\nLimited Experimental Resources\n: Due to constraints in resources such as GPUs, storage, and workforce (Section B.1), we conducted our experiments by reducing the diversity of model architectures within a range that did not compromise the validity of our approach.\nWe believed that due to the first reason, reporting the performance of our work using just the OPT family was sufficient. While the limitations mentioned in the second reason restricted the scope of this paper, we will note the possibility of applications to other architectures like BERT or T5 in our paper as promising future work.\n[W2-a] The paper relies on a single zero-shot machine-generated text detection method (DetectGPT) to pseudo-label the self-generated texts, without considering other possible methods or evaluating the robustness and reliability of DetectGPT. [W2-b] The paper also does not explain how the perturbation discrepancy correlates with the membership probability or the presence of training data in the generated texts. [W2-c] The paper does not account for the potential confounding factors or sources of bias in its experiments, such as the choice of prompts, sampling methods, hyperparameters, datasets, and evaluation metrics.\n[W2-a] The primary reason for adopting DetectGPT in this study is its state-of-the-art (SOTA) performance and its capability to calculate machine-generated probability in a zero-shot manner. To maximize the robustness and reliability of DetectGPT, we intentionally matched generations with significant perturbation discrepancies during the pairing process (Section 4.1). We believe this local optimal pseudo-labeling process significantly mitigates the issues you pointed out.\n[W2-b] In this study, we assume that the lower the perturbation discrepancy, the lower the machine-generated probability [2], and therefore, the higher the likelihood of the generations being human-written (Section 4.1). We will reexamine the coherence of this statement in the text and enhance it in the final version.\n[W2-c] We did not conduct a specific search for the optimal combination of hyperparameters, such as the sampling method, during our study (Section 5.1). The main reason is that the adversary can independently combine our adversarial fine-tuning approach with previous TDE attacks. We agree with your concern about not using all training data sets of the target LM for verification or potential bias due to the choice of evaluation metrics could be an issue. In the final version, we will add a 'Limitations' Section to address these concerns comprehensively.\n[W3] The paper does not discuss the ethical and social implications of its attack strategy. The paper proposes a novel form of TDE attack that can amplify the exposure of sensitive and private information from pre-trained LMs, but does not address the potential harms or risks that such an attack can pose to individuals, organizations, or society at large.\n[W3] As you mentioned, we are concerned about the potential harm and risk factors associated with the public disclosure of our research results. To address these issues, we plan to add an 'Ethics and Broader Impacts' Section in the final version. Briefly, the potential risks identified are as follows:\nUnfair Competitive Practices\n: If private training data is used in the training of an LM, leaking such information could be considered an act of unauthorized appropriation of data collected, refined, and stored with a significant investment of time and resources by a company. Furthermore, an adversary could attempt to deduce the training data preprocessing methods from the attributes and properties of the refined training dataset.\nPersonal Information Leakage\n: The vast training data used in recent LMs may include unfiltered or sensitive data in context [4]. If an adversary extracts such sensitive information from the target LM, it could lead to serious privacy breaches.\nThank you for the constructive comments. We hope these explanations resolve the concerns. Further questions or suggestions would also be appreciated."}, {"Heading": "Response to Reviewer ZqEj (3/3)", "Subheading": "Official CommentbyAuthors14 Nov 2023, 10:35 (modified: 14 Nov 2023, 10:48)EveryoneRevisions", "Content": "Comment:\n[1] Carlini, Nicholas, et al. \"Extracting training data from large language models.\"\n30th USENIX Security Symposium (USENIX Security 21)\n. 2021.\n[2] Mitchell, Eric, et al. \"Detectgpt: Zero-shot machine-generated text detection using probability curvature.\"\narXiv preprint arXiv:2301.11305\n(2023).\n[3]\nhttps://platform.openai.com/docs/guides/fine-tuning\n[4] Brown, Hannah, et al. \"What does it mean for a language model to preserve privacy?.\"\nProceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency\n. 2022."}]}, {"Heading": "Official Review of Submission9456 by Reviewer JBPh", "Subheading": "Official ReviewbyReviewer JBPh31 Oct 2023, 17:36 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper focuses on amplifying training data memorization in terms of the extraction attack performance. The goal is to put the target model in a state where it is more likely to regurgitate training data.\nMore specifically, the authors propose a fine-tuning method, using reinforcement learning, text generation and machine-text generation detection to condition the model such that it is likelier to regurgitate its training data. They do this with restricted whit-box access to the model, and no access to the training data. They attempt to achieve this by doing the following steps: 1) generating many samples from the model 2) using detectgpt to give scores on how likely each generation is to be human written, intuition being that human written text is more likely to have been pre-training data. 3) create pairs of training/generation data 4) training a reward model to distinguish between the generation and pseudo training data. 5) fine-tune the target model using the reward model. 6) taking samples from the new model and comparing tot the non-trained reference model.\nThe authors then test the performance of the proposed method by taking samples from the fine-tuned model and then measuring exact matches with training data and reporting the values. They compare these numbers to those of a non-fine tuned model. They also study the performance of the reward model separately.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThe approach/way of looking at the extraction problem is novel, prior work usually focuses on coming up with post-hoc extraction and not fine-tuning-based methods, where the decoding process is modified such that it incentivizes training data extraction. This paper however, tries to change the model so that its more likely to generate training data.\nThe problem is also an important problem, as current extraction methods are not very successful, most of them demonstrating low extraction rates.\nWeaknesses:\nLack of enough experiments and ablations to support the main claim of the paper, that the method amplifies memorization. See questions 1-3 below. This is my main concern with the approach, as the model might as well just be regurgitating the same set of n-grams, over and over and as the reported is not measured over deduplicate generations based on n-grams (it seems like the only deduplication performed is wrt to full matched strings with training data), nor is there a diversity metric reported. Intuitively, I would assume that the fine-tuning is going to get the model to collapse on the set of generations used for RM training/FT. I also wonder why the authors did not use a metric similar to BLEU.\nThe structure of the paper is hard to follow and its not really well written. Some of the results are not explained well, also the way the deduplication is performed is not fully clear.  See questions 3 and 4 below.\nSection 5.2 only shows how well a reward model can learn to differentiate between machine generated and human written text. It does not provide any evidence to support the claims of the paper regarding training data. It is simply an ablation. I am not sure what it is included as one of the first results. The fact that the reward model can differentiate between different texts does not necessarily translate to it being better at incentivizing the target model to regurgitate training data.\nQuestions:\nCan the authors disentangle how much of the extractions overlap with the generated text that they fine-tuned with, and how much of the extracted text is non-overlapping and actually a generation of the model that is due to the amplification. Right now the main remaining question is does this method actually reinforce memorizations or is it just overfitting to the pseudo labeled data? (this corresponds to weakness 1 from above)\nHow many of the generated samples after fine-tuning differ from the reference model generations? as mentioned in question 1, if the model is collapsing, the new generations that the fine-tuned model has would overlap a lot with the generations from the refenrece model. it would be interesting to see if that is the case, or if there are any entirely new generations.\nOne main problem with the results is that the generation deduplication is happening on a full string level, and not n-gram overlaps. Same as point 1, I think there is probably huge overlap, what is the diversity of generations? There are no ablations here. We need a lot more ablations on the experiments.\nsection 5.2 please elaborate on the duplicate token overlaps, and the intervals. I went over the text multiple times but did not realize what the point of that experiment is.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer JBPh (1/2)", "Subheading": "Official CommentbyAuthors14 Nov 2023, 10:29Everyone", "Content": "Comment:\nThank you for acknowledging our main contributions:\na novel approach to extraction attack.\na possibility of potential improvements to current extraction methods with low extraction rates.\n[Q1] Can the authors disentangle how much of the extractions overlap with the generated text that they fine-tuned with, and how much of the extracted text is non-overlapping and actually a generation of the model that is due to the amplification. Right now the main remaining question is does this method actually reinforce memorizations or is it just overfitting to the pseudo labeled data? (this corresponds to weakness 1 from above)\n[Q1] In the following table, we report the true positives after deduplicating some texts containing actual training data from reference and fine-tuned LMs. Here, 'duplication' refers to cases where 'two different generated texts are extracted from the same training data point.' As the adversary in Section 3.1's assumption does not consider the duplication of extracted texts (Section 3.1), we did not perform separate n-gram-based deduplication [3].\nOPT\n\u25cf\n\u25cb\nInc.\n125M\n56\n139\n$\\times$2.5\u2191\n350M\n128\n272\n$\\times$2.1\u2191\n1.3B\n74\n621\n$\\times$8.4\u2191\n2.7B\n96\n425\n$\\times$4.4\u2191\n6.7B\n130\n477\n$\\times$3.7\u2191\n13B\n165\n425\n$\\times$2.6\u2191\nWe observed that, even after deduplication, there is an amplification of 2.6-8.4 times in LMs with over 1B parameters. While some performance decreases in the 13B parameter model, our adversarial fine-tuning enhances TDE attack performance up to eight times. We will add these ablation studies in the final version.\n[Q2] How many of the generated samples after fine-tuning differ from the reference model generations? as mentioned in question 1, if the model is collapsing, the new generations that the fine-tuned model has would overlap a lot with the generations from the refenrece model. it would be interesting to see if that is the case, or if there are any entirely new generations.\n[Q2] We report in the following table how much overlap exists between samples extracted from the fine-tuned LM and those from the reference LM. Each extracted sample has not been deduplicated (i.e., the total number is the same as in Table 2), and each range represents the number of overlapping tokens.\nOPT\n[0,64)\n[64,128)\n[128,192)\n[192,256)\n256\nTotal\n% (<64)\n125M\n100\n46\n18\n5\n0\n169\n59.17\n350M\n158\n70\n38\n59\n0\n325\n48.62\n1.3B\n545\n83\n30\n117\n0\n775\n70.32\n2.7B\n505\n12\n0\n0\n0\n517\n97.68\n6.7B\n534\n11\n0\n0\n0\n545\n97.98\n13B\n704\n81\n1\n0\n0\n786\n89.57\nThese experimental results show that generations extracted from our fine-tuned LM usually do not overlap with those not fine-tuned. We set the 'overlap' threshold to 25% (\ni.e.\n, 64 tokens), and70.32% to 97.98% of extraction results from fine-tuned LMs with over 1B parameters are unique. Based on your feedback, we will add these ablation studies to the final version.\n[Q3] One main problem with the results is that the generation deduplication is happening on a full string level, and not n-gram overlaps. Same as point 1, I think there is probably huge overlap, what is the diversity of generations? There are no ablations here. We need a lot more ablations on the experiments.\n[Q3] To check the diversity of total generated texts (note, not extracted results) from both reference and fine-tuned LMs, we measured self-BLEU scores [1] and unique n-gram percentages [2]. First, the following table describes the stochastic self-BLEU scores.\nOPT\n\u25cf\n\u25cb\nDec. (%)\n125M\n0.33\n0.26\n20.74\n350M\n0.29\n0.27\n6.02\n1.3B\n0.28\n0.24\n15.80\n2.7B\n0.27\n0.24\n11.95\n6.7B\n0.27\n0.23\n15.22\n13B\n0.26\n0.24\n9.23\nOur fine-tuning strategy decreased self-BLEU scores by 6.02% to 20.74%, showing improved diversity across all parameter ranges. Next are the results for unique n-gram percentages for n=2, 3, 4.\nn=2\nn=3\nn=4\nOPT\n\u25cf\n\u25cb\nDiff.\n\u25cf\n\u25cb\nDiff.\n\u25cf\n\u25cb\nDiff.\n125M\n12.91\n18.69\n1.45\n40.31\n49.74\n1.23\n70.84\n78.03\n1.10\n350M\n15.43\n19.64\n1.27\n44.91\n50.22\n1.12\n74.62\n77.56\n1.04\n1.3B\n16.14\n26.95\n1.67\n46.64\n58.36\n1.25\n76.13\n80.74\n1.06\n2.7B\n17.31\n23.45\n1.35\n48.47\n55.59\n1.15\n77.40\n81.15\n1.05\n6.7B\n17.66\n22.37\n1.27\n49.06\n54.75\n1.12\n77.93\n81.60\n1.05\n13B\n18.40\n22.70\n1.23\n49.92\n55.31\n1.11\n78.45\n81.06\n1.03"}, {"Heading": "Response to Reviewer JBPh (2/2)", "Subheading": "Official CommentbyAuthors14 Nov 2023, 10:30 (modified: 14 Nov 2023, 10:40)EveryoneRevisions", "Content": "Comment:\nSimilarly, we confirmed the improvement in unique n-gram percentages in all settings. Like the previous two ablation studies, we will add the results of this ablation study on generations' diversity to the final version, further enhancing our paper.\n[Q4] Section 5.2 please elaborate on the duplicate token overlaps, and the intervals. I went over the text multiple times but did not realize what the point of that experiment is.\n[Q4] We will answer based on the overlap mentioned in Section 5.1. If our understanding is inaccurate, please let us know the specific position of a page, section, paragraph, or line so we can provide a more detailed response.\nFundamentally, the metric for the effectiveness of our TDE attack is the true positives among 100,000 generated texts (Section 5.1). To determine whether a generation is part of the training data, we need to calculate how much it overlaps with a training data sample (in the training dataset) of the target LM. Since each generation has precisely 256 tokens, the unit of overlap is tokens (\ni.e.\n, not word or byte). Inspired by previous research, we set the number of overlapping tokens at 50 [3]. In summary, if 50 consecutive tokens appearing in a generation of the target LM exist at any location in the training dataset, we consider the generation to have been 'extracted' from the training dataset.\nWe further segment the 'duplicate intervals' of generated texts with [50, 256] token overlaps to dive deeper into our result. We divide the true positive samples into five duplicate intervals as follows (Table 2): [50, 64), [64, 128), [128, 192), [192, 256), {256}.\n[W1] Lack of enough experiments and ablations to support the main claim of the paper, that the method amplifies memorization. See questions 1-3 below. This is my main concern with the approach, as the model might as well just be regurgitating the same set of n-grams, over and over and as the reported is not measured over deduplicate generations based on n-grams (it seems like the only deduplication performed is wrt to full matched strings with training data), nor is there a diversity metric reported. Intuitively, I would assume that the fine-tuning is going to get the model to collapse on the set of generations used for RM training/FT. I also wonder why the authors did not use a metric similar to BLEU.\n[W1] Through your questions 1 to 3, we conducted additional ablation studies to demonstrate that our strategy amplifies memorization. Specifically, we confirmed that (1) our fine-tuning does not compromise the diversity of LM's generated texts (Q3), and (2) even with deduplication of texts extracted from the same training data position, the amplification remains still effective (Q1). These results suggest that our fine-tuning is less likely to lead to LM collapse.\n[W2] The structure of the paper is hard to follow and its not really well written. Some of the results are not explained well, also the way the deduplication is performed is not fully clear. See questions 3 and 4 below.\n[W2] We will add extensive ablation studies to the experiment and clarify the meaning of 'duplicate token overlaps,' improving the paper's structure.\n[W3] Section 5.2 only shows how well a reward model can learn to differentiate between machine generated and human written text. It does not provide any evidence to support the claims of the paper regarding training data. It is simply an ablation. I am not sure what it is included as one of the first results. The fact that the reward model can differentiate between different texts does not necessarily translate to it being better at incentivizing the target model to regurgitate training data.\n[W3] We agree with your opinion that the experiment in Section 5.2 (Table 1) does not support the paper's primary claim and is somewhat close to an ablation study. We acknowledge that the experiment observing whether the reward model can distinguish samples that seem 'more/less machine-generated' and whether such distinguishability varies with the scale of the target LM has a relatively trivial contribution. We plan to improve the structure of the forthcoming paper for an enhanced presentation.\nThank you for the constructive comments. We hope these explanations resolve the concerns. Further questions or suggestions would also be appreciated.\n[1] Zhu, Yaoming, et al. \"Texygen: A benchmarking platform for text generation models.\"\nThe 41st international ACM SIGIR conference on research & development in information retrieval\n. 2018.\n[2] Wang, Alex, and Kyunghyun Cho. \"BERT has a mouth, and it must speak: BERT as a Markov random field language model.\"\narXiv preprint arXiv:1902.04094\n(2019).\n[3] Carlini, Nicholas, et al. \"Extracting training data from large language models.\"\n30th USENIX Security Symposium (USENIX Security 21)\n. 2021."}, {"Heading": "Official Comment by Reviewer JBPh", "Subheading": "Official CommentbyReviewer JBPh21 Nov 2023, 23:10Everyone", "Content": "Comment:\nI thank the authors for their response. I have one follow up question, 64 is a bit too high of number of tokens, this paper (\nhttp://arxiv.org/abs/2111.09509\n) shows that generations become novel/unique with n>10 for n-grams. I would be curious to see this table for n=10, or at least 20. I am still not convinced of the diversity of the text and that it is not regenerating the fine-tuning data, because the self-bleu number increases are also not fully convincing."}, {"Heading": "Response to Reviewer JBPh", "Subheading": "Official CommentbyAuthors22 Nov 2023, 17:46Everyone", "Content": "Comment:\nThank you for your follow-up question. We have measured the proportion of novel/unique n-grams in the generations from the fine-tuned LM, referencing the paper (in Figure 1) you raise (\ni.e.\n, the count of n-grams in the generations that did not appear in the fine-tuning dataset in Table 5 of our manuscript).\nOPT\nTotal 10-grams\nUnique 10-grams\nProportion (\u2191)\n125M\n24,334,096\n24,306,061\n0.9988\n350M\n24,129,396\n24,096,257\n0.9986\n1.3B\n23,572,674\n23,548,392\n0.9990\n2.7B\n24,175,634\n24,148,747\n0.9989\n6.7B\n24,428,210\n24,404,788\n0.9990\n13B\n24,156,194\n24,125,084\n0.9987\nSimilarly to the paper (in Figure 1) you raise, the generations from the fine-tuned LM contained over 99% of the 10-gram set being novel/unique. We believe these results serve as an indicator of the diversity in the generations of the fine-tuned LM. We will cite the prior work and include this ablation study in the final version of our manuscript. We hope these explanations resolve the concerns."}]}, {"Heading": "Official Review of Submission9456 by Reviewer cSHP", "Subheading": "Official ReviewbyReviewer cSHP30 Oct 2023, 16:47 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper presents a novel attack strategy aimed at increasing the vulnerability of pre-trained language models to training data extraction attacks. By adversarially fine-tuning the LMs, the authors claim to amplify the exposure of sensitive pre-training data. They propose the use of pseudo-labels to help fine-tune the model in a way that favors text likely to have originated from the pre-training dataset. Their experiments suggest that this approach can lead to a significant increase in training data exposure, particularly in large models with over 1 billion parameters.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nGenerally, the paper is well-written and easy to follow.\nThe paper introduces a unique and unexplored attack vector that goes beyond the traditional post-hoc data extraction methods.\nGiven the widespread use of large LLMs, the paper addresses a timely and significant issue of data privacy.\nWeaknesses:\nThe paper assumes the availability of restricted white-box capabilities, which may not always be the case in real-world scenarios.\nAlthough the author provided extensive empirical study results, I'm still curious about the underlying mechanism behind the attack. Could the author elucidate how the proposed adversarial fine-tuning method effectively amplifies data exposure? It might be helpful to use a naive linear classification task as an illustrative example.\nQuestions:\nHow would the efficacy of this adversarial fine-tuning approach change if some form of differentially private training was already applied to the pre-trained language model? Would the attack still be as effective, or would it require significant modifications?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer cSHP", "Subheading": "Official CommentbyAuthors14 Nov 2023, 10:31 (modified: 14 Nov 2023, 10:39)EveryoneRevisions", "Content": "Comment:\nThank you for acknowledging our main contributions:\nwell-written and easy to follow.\nintroducing a unique and unexplored attack vector.\naddressing a timely and significant issue of data privacy.\n[Q1] How would the efficacy of this adversarial fine-tuning approach change if some form of differentially private training was already applied to the pre-trained language model? Would the attack still be as effective, or would it require significant modifications?\n[Q1] Unlike previous approaches that aim to leak more effective training data from LMs, our method primarily targets making the LM itself more vulnerable. From this perspective, we anticipate that our approach would still be effective\u2014\ni.e.\n, amplifying the risk of leakage several times\u2014even in LMs with well-defined defense strategies like differentially private training applied (Section 6).\n[W1] The paper assumes the availability of restricted white-box capabilities, which may not always be the case in real-world scenarios.\n[W1] The restricted white-box scenario assumed in our study is often considered unrealistic in previous research [1] and thus has not been explored yet. However, this assumption is increasingly essential for the following reasons (details in Section 3.1):\nIncrease in Public LMs\n: Efforts towards open science are leading to an increase in publicly available LMs, which pose potential risks related to our assumption.\nRisk of LM Weight Leakage\n: Sophisticated black-box model extraction attacks make our assumption more realistic.\nCustomized LM API Services\n: Recently, services that support customizing back-end LMs by fine-tuning them with user datasets are emerging (e.g., OpenAI [2]), and our research could be a severe concern in such cases.\nSpecifically, item 3 is not mentioned in our paper, so we plan to enhance the content by adding this in the final version.\n[W2] Although the author provided extensive empirical study results, I'm still curious about the underlying mechanism behind the attack. Could the author elucidate how the proposed adversarial fine-tuning method effectively amplifies data exposure? It might be helpful to use a naive linear classification task as an illustrative example.\n[W2] Due to difficulties in explaining our approach using a linear classifier as an example, we will provide an explanation along with an example of an adversarial attack on ResNet to describe the primary mechanism of our attack.\nThe primary mechanism of our approach that effectively amplifies exposure involves increasing the probability of the LM generating responses that are expected to contain more (partial) training data by rewarding such responses more highly. Our approach is akin to performing 'gradient ascent' on a ResNet model to make it misclassify more adversarial examples, thereby becoming increasingly vulnerable to adversarial attacks.\nTarget Model\nRisk\nEvaluation Metric\nResult\nSocial Impact\nResNet (example)\nMisclassification\nAccuracy\nVulnerable ResNet to Adversarial Attacks\nLow\nOPT (ours)\nTraining Data Exposure\nMachine-generated Probability (i.e., perturbation discrepancy)\nVulnerable OPT to Training Data Extraction Attacks\nHigh (due to privacy risks)\nThank you for the constructive comments. We hope these explanations resolve the concerns. Further questions or suggestions would also be appreciated.\n[1] Carlini, Nicholas, et al. \"Extracting training data from large language models.\"\n30th USENIX Security Symposium (USENIX Security 21)\n. 2021.\n[2]\nhttps://platform.openai.com/docs/guides/fine-tuning"}, {"Heading": "Official Comment by Reviewer cSHP", "Subheading": "Official CommentbyReviewer cSHP21 Nov 2023, 22:14Everyone", "Content": "Comment:\nThank you, author(s), for your response. Regarding Q1, I believe it would be beneficial to include some experiments to substantiate your argument. As for Q2, I appreciate your willingness to enhance the details in support of your white-box settings. Concerning Q3, I understand your point about the ResNet example, and while it may not be the most suitable illustration, it's acceptable if a linear classifier explanation is unavailable. I will maintain my original score."}, {"Heading": "Response to Reviewer cSHP", "Subheading": "Official CommentbyAuthors22 Nov 2023, 17:46Everyone", "Content": "Comment:\nThank you for your response. Regarding the answer to Q1, we believe that experiments in robust LMs applied with possible mitigations (\ne.g.\n, differentially private (DP) training) can demonstrate the effectiveness of our approach, thereby enhancing its contributions. While we positively consider these aspects in our subsequent work, we would like to point out the difficulty of this DP training from scratch due to limited experimental resources. If you could advise us about any public-available defense models we might be missing (possibly named something like DP-GPT), it would be greatly helpful in setting our future direction. Regarding the answers to Q2 and Q3, we once again express our appreciation for your constructive comments."}]}]}, "rGvDRT4Z60": {"paper_info": {"Primary Area": "societal considerations including fairness, safety, privacy", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "fairness, privacy, pate, pareto frontier", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "Deploying machine learning (ML) models often requires both fairness and privacy guarantees. In this work, we study the challenges of integrating group fairness interventions into the Private Aggregation of Teacher Ensemble (PATE) framework. We show that in the joint fairness-privacy setting, the placement of the fairness intervention before, or after PATE\u2019s noisy aggregation mechanism (which ensures its differential privacy guarantees) leads to excessive fairness violations, or inefficient privacy budgeting, respectively. With this in mind, we present FairPATE which adds a rejection mechanism due to fairness violations. Through careful adjustment of PATE\u2019s privacy accounting, we match the DP-SGD-based state-of-the-art privacy-fairness-accuracy trade-offs (Lowy et al., 2023) in demographic parity, and improve on them for equality of odds with 2% lower disparity at similar accuracy levels and privacy budgets. We also evaluate FairPATE in the setting where exact fairness guarantees need to be enforced by refusing to provide algorithmic decisions at inference-time (for instance, in a human-in-the-loop setting) thus trading off fairness with coverage. Based on our FairPATE, we provide, for the first time, empirical Pareto frontiers for fairness, privacy, accuracy, and coverage on a range of privacy and fairness benchmark datasets.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "Supplementary Material": "zip", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9455", "PDF Url": "https://openreview.net/pdf?id=rGvDRT4Z60"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9455 by Area Chair q9mW", "Subheading": "Meta ReviewbyArea Chair q9mW10 Dec 2023, 18:22 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThe submission introduces FairPATE, an enhancement of the PATE framework that integrates fairness constraints. FairPate ensures more accurate differential privacy (DP) accounting by considering the potential for query rejections based on fairness. Numerical results demonstrate favorable privacy-fairness-accuracy trade-offs, outperforming prior methods in terms of fairness and accuracy for a given privacy budget.\nOverall, the paper is clearly written, and the results are promising. However, the reviewers highlighted several limitations that must be addressed before acceptance.\nReviewer Mex5 highlighted the need to consider fairness when rejecting queries in privacy models, noting that ignoring fairness can be both consequential in practice and, in experiments, impact the Pareto frontier's observed gains. They also suggested the importance of including human judgments in the fairness calculus, even when a model refuses to answer a query due to fairness concerns. Such guidelines could improve the positive impact of FairPate.\nReviewer XkhF noted a disconnection with prior work. I read the authors' comments in this regard, and note their effort in addressing this issue. However, the reviewer's concerns remained post-rebuttal.\nFinally, reviewer p5Rv noted the small improvements of FairPATE over baselines and raised concerns about the diversity and clarity of experiments, with several benchmarks relegated to the appendix without thorough discussion. The reviewer also challenged comparisons with Loewy et al.'s algorithm, arguing that the former benefits from public unlabelled data while the latter adheres to stricter privacy constraints. They also raised issues with comparisons to related work.\nOverall, this is a promising paper. However, given the many issues raised by the reviewers that persisted after the extensive discussion, I encourage the authors to revise their manuscript and re-submit.\nJustification For Why Not Higher Score:\nSee the issues raised above, most related to concerns regarding prior work, presentation of results, and implications of the rejection mechanism in fairness in practice.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "Thank you for your feedback. Please consider our rebuttal responses.", "Subheading": "Official CommentbyAuthors21 Nov 2023, 18:33Everyone", "Content": "Comment:\nDear reviewers,\nThank you for your valuable feedback. We believe it has improved our submission.  Given that the author interaction window is coming to an end, we would like to kindly ask you to consider the rebuttal responses and let us know if they address your concerns and, if they do, to consider raising your scores.\nIt would be our pleasure to answer any further questions."}, {"Heading": "General Author Response", "Subheading": "Official CommentbyAuthors18 Nov 2023, 04:40 (modified: 23 Nov 2023, 05:53)EveryoneRevisions", "Content": "Comment:\nWe thank the reviewers wholeheartedly for their detailed comments and constructive feedback. We are glad that all reviewer found the paper to be well-written and easy to follow, and that they appreciated the simplicity of our joint fairness-privacy rejection mechanism of FairPATE. One reviewer noted the potential of extending this mechanism to other [trustworthiness] contexts, while another appreciated our discussion of interventions point in PATE.\nNew Manuscript and Color Codings.\nWe have applied reviewer's comments within the manuscript and have highlighted newly added parts in \"blue\" and fixed typos in \"red\". In order to fit the new manuscript within 9 pages we have also shortened certain sections of the paper which are marked in \"teal.\"\nWe will shortly address a common reviewer's question regarding comparison to prior work and provide a table of comparisons for methods to which our work has conceptual similarities but which are ultimately unsuitable for a fair empirical comparison. This is due to a difference in privacy notion, or lack of privacy accounting for privacy leakage of the fairness intervention.\nContribution & Guidance on Choice of Framework\nThe contribution of our algorithm is not limited to performance  improvements over our main baseline Lowy et al. 2023 (although we also demonstrate those). As we discuss in Sections 6, prior work has largely adopted a limited privacy notion for joint private-and-fair learning; namely that of\nDP w.r.t. sensitive attributes Jagielski et al. 2019\n. While this notion has its particular applications (e.g. for example to train ethnically-fair models that also do not reveal the ethnicity of the individual), it does not preclude catastrophic failures of privacy that more general notions of differential privacy protects against\u2014in the previous example, for an ethnic individual's medical features to be leaked as a result of release of the aforementioned model.\nThe main contribution of our work is to enable practical and $(\\varepsilon,\\delta)$ -DP training (i.e., with respect to all features and not just the sensitive ones) in a PATE-type framework; while Lowy et al. 2023 is the first work to enable this using a DP-SGD-type framework. We note that there are benefits and draw-backs to using either PATE and DP-SGD frameworks which by extension makes both FairPATE and DP-FERMI viable in their own regard. We have highlighted some of these trade-offs in Section 7.\nFurthermore, we have added an extended related works in Section I in the appendix, and have added the following table and paragraph on guidance for choice of framework to Section I.2.\nMethod\n$(\\epsilon, \\delta)$\n-DP\nDP w.r.t. Sensitive Attributes\nPrivacy Accounting for Fairness\nPublic Data\nFairness Intervention Space\nFairPATE\n$\\checkmark$\n-\n$\\checkmark$\nUnlabeled\nSamples\nDP-FERMI\n$\\checkmark$\n-\n$\\checkmark$\n-\nWeights\nSF-PATE\n-\n$\\checkmark$\n$\\checkmark$\nUnlabeled\nWeights\nJagielski et al. 2019\n-\n$\\checkmark$\n$\\checkmark$\n-\nWeights\nMozannar et al. 2020\n-\n$\\checkmark$\n$\\checkmark$\n-\nWeights\nDP-IS SGD\n$\\checkmark$\n-\n$\\checkmark$\n-\nSamples\nZhang et al. 2021\n$\\checkmark$\n-\n\u2717\n-\nOpt. Hyperparams\nChoice of Framework.\nThe appropriate choice of privacy notion is application-dependent. However, a $(\\epsilon, \\delta)$ -DP is a much stronger guarantee and it is by far the most prevalent notion of privacy. If interpretability is of concern, sample-space interventions are more appropriate since reasoning about the membership (or probability of membership) of a certain sample is easier than reasoning about high-dimensional gradients in the weight space. Finally, all PATE-based frameworks assume access to an unlabeled public dataset due to its semi-supervised nature while DP-SGD-type methods do not require such dataset. This need can be addressed by using a subset of the training data, throwing out its labels, and employing it in place of the unlabeled dataset in PATE-based models. This is how the original PATE papers were evaluated, and we have evaluated FairPATE similarly.\nIn practice, however, the unlabeled dataset can come from a public dataset which is different than the sensitive training set. From a privacy accounting perspective only, the use of such public data is similar to the use of public data to pre-train DP-SGD models: in both cases, privacy cost, by definition, is not being accumulated for the public data. The difference is that pre-training requires labels, while the PATE's public dataset need not be labeled.\nWe are happy to further engage with reviewers as part of the discussion phase and hope that the reviewers consider raising their scores.\nEdits\nThis comment has been edited for clarity after interactions with reviewer p5Rv. We thank the reviewer for their comment.", "Replies": [{"Heading": "Official Comment by Reviewer p5Rv", "Subheading": "Official CommentbyReviewer p5Rv18 Nov 2023, 04:46Everyone", "Content": "Comment:\nDear Authors,\nThanks for the response. Just to clarify when you say\n'''Finally, all PATE-based frameworks assume access to an unlabeled public dataset on which privacy accounting does not take place. This is akin to pre-training DP-SGD models on public data before any privacy accounting occurs\u2014although this requires labels while PATE-based models do not.'''\nDo you mean PATE uses the same kind of data, but without labels, as DP-SGD normally uses for pre-training ?\nFrom my understanding, the most popular examples of DP-SGD using pre-training is ImageNet pre-trained for private fine-tuning on CIFAR or JFT pre-training for fine-tuning on ImageNet and similar for NLP tasks.\nIs your public data from a very different distribution compared to the test set ? That's not the impression I got from reading the draft."}, {"Heading": "Thank you for the question", "Subheading": "Official CommentbyAuthors18 Nov 2023, 11:45Everyone", "Content": "Comment:\nIn evaluating PATE-type models against DP-SGD-type models, it is customary to use a split of the training data, throw out its labels and use it to query the aforementioned teacher ensemble. This ensures that PATE does not observe more information than their DP-SGD counterparts; and it is how the two original PATE papers were evaluated.\nSo the answer to the highlighted question, in the context of our evaluations, is no. We use a subset of the training data.\nOur prior comment was meant as an analogy for the privacy accounting only: pre-training data is not accounted for in privacy budgeting of DP-SGD; just as the unlabeled data is not accounted for in privacy budgeting of PATE. In other regards (distribution, etc.) the pre-training data, and the unlabeled dataset are not similar."}, {"Heading": "Official Comment by Reviewer p5Rv", "Subheading": "Official CommentbyReviewer p5Rv22 Nov 2023, 06:48Everyone", "Content": "Comment:\nThanks for the clarification. I think the way the above (original comment) statement is made is misleading and I would suggest that the authors update it to reflect what is mentioned in the clarification."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors22 Nov 2023, 11:19Everyone", "Content": "Comment:\nWe have applied your suggestion and edited the response for clarity on OpenReview and also in the paper. Note that we will update the latter on OpenReview after we have also applied other reviewers' additional comments.\nThank you for interacting with us!"}]}, {"Heading": "Official Review of Submission9455 by Reviewer XkhF", "Subheading": "Official ReviewbyReviewer XkhF06 Nov 2023, 06:11 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe proposes a framework to integrate fairness into PATE. The proposed method is a simple adaptation of PATE which incorporates fairness constraints into the model's query rejection mechanism.\nSoundness:\n3 good\nPresentation:\n4 excellent\nContribution:\n2 fair\nStrengths:\nThe paper tackles a highly relevant issue in ML, addressing both theoretical and practical implications of fairness and privacy.\nThe proposed framework is a simple adaptation of the existing PATE. Simplicity is a plus in my book.\nWeaknesses:\nFairness is \"enforced\" by adding parity constraint within the aggregator which acts as a rejection sampling step on the basis of fairness. The general idea was proposed in several other works in the past, including the ones cited by the authors and against which the authors compare. It is thus difficult to understand what is new in the proposed framework. An explicit mention would help.\nA discussion on when to use the proposed framework in contrast to other frameworks is absent (see also my questions below).\nThe experimental analysis should be improved. Some figures are misleading, e.g., same colors used for different algorithms (see questions below).\nQuestions:\nI can't judge what is the impact of IPP (the rejection step added at inference time) on the overall results. Can you provide some ablation study showing how the framework performs on various datasets with different majority/minority distributions with and without IPP?\nHow does the framework work in case of some distribution shift? This is especially important in the context of my question above.\nFor the other datasets reported in the appendix the trends shown reverts, e.g., DP-Fermi produces better tradeoffs than FairPATE. Can you discuss why? What feature of the dataset makes this possible?\nFig. 2 and 3 use orange colors for two different algorithms (Tran et al (Fig 2) and Jagielski et al. (Fig 3)). The authors should report all algorithms in all figures or justify their absence.\nWhy Tran et and Jagielski et al. are not reported for the UTK-dataset experiment?\nPaper\nLearning with Impartiality to Walk on the Pareto Frontier of Fairness, Privacy, and Utility\ndiscusses a similar topic (although the contributions from this work are different) and it could be added to your Related work section.\nMinor comments:\nA lot of the cited papers have appeared in conferences. But the authors cite their arxiv version. I suggest to update the references accordingly.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Weaknesses", "Subheading": "Official CommentbyAuthors18 Nov 2023, 05:00 (modified: 18 Nov 2023, 05:01)EveryoneRevisions", "Content": "Comment:\nWe thank the reviewer for their feedback. Below we address each point raised in \"Weaknesses\" section.\nFairness is \"enforced\" by adding parity constraint within the aggregator which acts as a rejection sampling step on the basis of fairness. The general idea was proposed in several other works in the past, including the ones cited by the authors and against which the authors compare. It is thus difficult to understand what is new in the proposed framework. An explicit mention would help.\nThe reviewer is correct that our work enforces fairness on the level of sample selection. However, this is done within a differentially private learning setup. The only prior work that we are aware of that has a sampling step for fairness within a private-learning setup is Kulynych et. al 2021 but importantly that work does not consider the privacy cost of the fairness importance sampling it performs while our work does.\nWe have added an extended related works section in Section I in the appendix with a more detailed comparison to Kulynych et. al 2021.\nA discussion on when to use the proposed framework in contrast to other frameworks is absent (see also my questions below).\nWe have addressed the reviewer's concern. Please see our General Response.\nThe experimental analysis should be improved. Some figures are misleading, e.g., same colors used for different algorithms (see questions below).\nWe have re-generated the figures in question (3 and 4) and updated the manuscript accordingly."}, {"Heading": "Response to Questions", "Subheading": "Official CommentbyAuthors18 Nov 2023, 05:15Everyone", "Content": "Comment:\nWe kindly note that in the following, where appropriate, we have grouped the reviewer's questions together:\nI can't judge what is the impact of IPP (the rejection step added at inference time) on the overall results. Can you provide some ablation study showing how the framework performs on various datasets with different majority/minority distributions with and without IPP?\nHow does the framework work in case of some distribution shift? This is especially important in the context of my question above.\nWe have run additional experiments on FairPATE+IPP and have added the results Section E.2 in the appendix of the updated manuscript. We observe that under class-imbalance for every percentage of coverage degradation, the fairness-privacy curve improves consistently. For the balanced case, we observe that the disparity levels are much higher. This is because random (non-stratified) sampling is not conducive to demographic parity.  IPP's overall behavior is similar as before but the curves are much closer to each other and the rejection rates are higher due to high disparity levels of the initial models caused by random sampling.\nFor the other datasets reported in the appendix the trends shown reverts, e.g., DP-Fermi produces better tradeoffs than FairPATE. Can you discuss why? What feature of the dataset makes this possible?\nWe thank the reviewer for the insightful question. Adult is a label-imbalanced dataset and we believe this is the reason its results stand out. Let us elaborate on this point: FairPATE achieves better fairness by pre-processing (a sample-level mitigation). As a result, if the dataset is imbalanced; especially if it is conditionally imbalanced, that is  $\\mathbb{P}[Y = 1 \\mid Z = 0] - \\mathbb{P}[Y = 1 \\mid Z = 1] > \\gamma,$ then fairness of a model trained on such data would also be bounded from below: $\\mathbb{P}[\\hat{Y} = 1 \\mid Z = 0] - \\mathbb{P}[\\hat{Y} = 1 \\mid Z = 1] > \\alpha > 0.$  This follows from Shamsabadi et al. 2023 Theorem 1, if we assume that the ground truth labels $Y$ are coming from a data oracle and the predicted labels $\\hat{Y}$ are from the \"surrogate model\". Since the oracle model is not fair; then the actual model trained on those will similarly not be fair (in the demographic parity sense).\nDP-FERMI on the other hand has a model-level mitigation with a fairness regularization term scaled with $\\lambda \\in \\mathbb{R}^+$ . For a large enough $\\lambda$ , DP-FERMI can close the fairness gap on a particular dataset. However, this comes at a cost to generalization. Prior works in algorithmic fairness have noted this trade-off under the notion of \"stability\" and \"generalization of fairness\" (see for instance, Huang and Vishnoi 2020). We note that in Section D in the appendix, we also present a weight-space mechanism  (namely, model fair-tuning) and show it to be similarly effective in reducing the residual fairness gap of a FairPATE student model.\nFig. 2 and 3 use orange colors for two different algorithms (Tran et al (Fig 2) and Jagielski et al. (Fig 3)). The authors should report all algorithms in all figures or justify their absence.\nWe have regenerated and replaced Figure 3 to ensure distinct colors. We apologize for the confusion.\nAs to the reason why Figure 3 does not include Tran 21 et al, we have already made an explicit note in the empirical section that we report the same  baseline results as reported by Lowy et al. 2023 (in 2nd line of SOTA Baseline Comparisons) including that of Tran et al 2021.\nWhy Tran et and Jagielski et al. are not reported for the UTK-dataset experiment?\nBoth methods are evaluated on tabular data and are not suitable for deep learning models. In particular, Lowy et al. 2023 report that they have attempted this to no avail:\nWe observed that the baselines were very unstable while training and mostly gave degenerate results(predicting a single output irrespective of the input). By contrast, our method was able to obtain stable and meaningful tradeoff curves.\nGiven that DP-FERMI reports better fairness-accuracy trade-offs at every privacy level compared to these baselines; we found it sufficient to test FairPATE against the new state-of-the-art (DP-FERMI); and report the other baselines for which we had reported data.\nMinor comments:\nA lot of the cited papers have appeared in conferences. But the authors cite their arxiv version. I suggest to update the references accordingly.\nWe thank the reviewer for their careful reading of the paper. We have replaced the arxiv papers with conference counterparts.\nWe once again thank the reviewer for their detailed feedback and kindly ask them to consider raising their score if we have addressed their concerns successfully. We are happy to answer further questions."}, {"Heading": "Response", "Subheading": "Official CommentbyReviewer XkhF22 Nov 2023, 07:43Everyone", "Content": "Comment:\nThank you for your replies. They are appreciated. \nI suggest to make it much more explicit the point discussed here regarding distribution shift and trends regarding different datasets, as the current results showcased (if one would read the main section only) may be misleading or at least report only a portion of the story.\nI also strongly suggest the authors to include the evaluation of Mozannar et al. al and Tran et al. in all of the experiments (including those based on CNNs)! \nIn particular, notice that the claim by Lowey et al. is erroneous and both algorithms are reported on the UTK datasets in their original papers:\nTran and Fioretto, On the Fairness Impacts of Private Ensembles Models\nSee figure 7 (right)\nTran et al, SF-PATE: Scalable, Fair, and Private Aggregation of Teacher Ensembles\nSee figure 2(c)"}, {"Heading": "Thank you for your time and suggestions!", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:28Everyone", "Content": "Comment:\nWe thank the reviewer for engaging with us. We appreciate the time and effort you have dedicated to reviewing our work.\nThank you for your replies. They are appreciated. I suggest to make it much more explicit the point discussed here regarding distribution shift and trends regarding different datasets, as the current results showcased (if one would read the main section only) may be misleading or at least report only a portion of the story.\nWe note that\nwe had already mentioned the particular strengths of our methods very prominently in the abstract and the introduction\nas well as the experimental section.\nOur method does better under Equality of Odds.\nUnder Demographic Parity it is a toss-up between the two methods as we had originally mentioned just below figures 2 and 3 in paragraph SOTA Baseline Comparisons.\nWe nevertheless understand that the placement of one of the figures in the appendix had made comparisons difficult, so\nwe have brought up the demographic parity results on Adult in Figure 2\n. We have also regenerated the figure with better-tuned parameters which gives tighter confidence intervals.\nI also strongly suggest the authors to include the evaluation of Mozannar et al. al and Tran et al. in all of the experiments (including those based on CNNs)! In particular, notice that the claim by Lowey et al. is erroneous and both algorithms are reported on the UTK datasets in their original papers:\nTran and Fioretto, On the Fairness Impacts of Private Ensembles Models\nSee figure 7 (right)\nTran et al, SF-PATE: Scalable, Fair, and Private Aggregation of Teacher Ensembles\nSee figure 2(c)\nThe official repository of Mozannar et al. 2020\nsolves a constrained linear program\nwhich is inappropriate for vision datasets. If the reviewer is aware of any other implementations, we would be grateful if they shared it with us. Also, as far as we know there is no published code base for any of the two Tran papers mentioned. If the reviewer is aware of such code bases, we would be grateful if they shared it as a part of the meta-review.\nWe note that \"On the Fairness Impacts of Private Ensembles Models\" adopt\nexcessive risk\nas their fairness metric \"which is defined as the difference between the private and non-private risk functions.\" This is a very different type of fairness metric that is incompatible with the group fairness metrics that we, Lowy et al, or even the other Tran paper proposed here (SF-PATE) adopts. Furthermore, as we elaborated in the general response (and the table), SF-PATE adopts DP-w.r.t. Sensitive Attributes instead of the general $(\\epsilon, \\delta)$ -DP. Therefore, given the incompatible definitions of fairness and privacy, we do not believe these are good baselines for our methods."}, {"Heading": "Response", "Subheading": "Official CommentbyReviewer XkhF02 Dec 2023, 20:22 (modified: 15 Mar 2024, 00:25)EveryoneRevisions", "Content": "Comment:\nDear authors, thanks a lot for your response. It clarified some of my doubts."}]}, {"Heading": "Official Review of Submission9455 by Reviewer Mex5", "Subheading": "Official ReviewbyReviewer Mex505 Nov 2023, 21:30 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe work considers the inclusion of fairness constraints into a method for differentially private\n(DP) training (or data generation from private data) based on transfer learning. The paper argues\nthat in this \"PATE\" approach which accounts of privacy concerns using DP, there is only one\nsensible place to incorporate fairness using an intervention (i.e. adjusting what/whether) data\nproceeds to subsequent PATE steps. This step is the point after the transfer from an ensemble of\nteachers is made. The paper puts a mechanism there that will reject some queries/instances if they\nresult in violations of fairness which is a function of all of the prior decisions of the\nmechanism. The work evaluates this approach relative to 2 other DP-based systems that incorporate\nfairness showing mostly preferable trade-offs between fairness, accuracy, and privacy; though this\nbenefit is small.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n1 poor\nStrengths:\nFairly well written and easy to follow.\nThe points of intervention discussions give a nice overview of the PATE approach and ways in\nwhich additional mechanisms can be independently injected. Note, however, the independent\nintervention assumption is a weakness below.\nRejection for fairness does give additional options for achieving fairness though this too comes\nwith a weakness below.\nWeaknesses:\nThe implications of rejecting for fairness are not considered. Rejection for privacy has\nimplications in terms of privacy budget and likewise rejections for fairness come with\nimplications and ignoring them might be responsible for the observed gains on the Pareto\nfrontier. Consider the noted rejection example:\n\"If at inference-time a decision cannot be made without violating a pre-specified fairness\n   metric, then the model can refuse to answer, at which point that decision could be relegated\n   to a human judge\"\nThe important implication here is that there will still be a judgement; it is just that the model\nwill not be making it. Regardless of whether the result of the human judgement will produce fair\nor unfair overall statistics (that consider ultimate judgement whether by model or human), those\ndecisions need to be incorporated into subsequent fairness calculus. Even if a query is rejected\ndue to privacy, and if a decision is made for it subsequently, it would need to be accounted for\nin subsequent fairness decisions.\nSuggestion: incorporate ultimate decisions, whether by model or human, into the rejection\nmechanism; i.e. update counts m(z, k) based on human decisions. Given that humans might put the\ngroup counts into already violating territory, it may be necessary to rewrite Line 7 of Algorithm\n1 to check whether the fairness criterion is improving or not due to the decision and allow\nqueries that improve statistics even though those statistics already violate \u03b3 threshold.\nHandling rejection in experiments will also need to be done but unsure what the best approach\nthere would be. Perhaps a random human decision maker?\nIn arguments for intervention points, assumptions are made which preclude solutions. They assume\nthe intervention need to be made independent of other mechanisms in PATE. That is, they cannot\nconsider information internal to decision making that is not described by Figure 1 like\nindividual teacher outputs. This leaves the possibility that some fairness methods might be able\nto integrated with PATE in a closer manner than the options described. One example is that they\nmight include the teacher outputs instead of operating on the overall predicted class like\nAlgorithm 1 assumes presently. C3 in particular suggests that some interventions will not account\nfor privacy budget correctly due to special circumstances and suggests at Point 4, they can be\nbudgeting can be handled correctly. Nothing is stopping a design from refunding privacy budget if\na query is rejected subsequently to an intervention point.\nSuggestion: rephrase arguments for why some intervention points are bad to make sure they don't\nalso make assumptions about how the interventions are made and whether they can interact with\nprivacy budget.\nResults in the Pareto frontier show small improvements, no improvements, and in some cases worse\nresults than prior baselines.\nSuggestion: Include more experimental samples in the results to make sure the statistical\nvalidity of any improvement claims is good. This may require larger datasets. Related, the\nexperiments show error bars but how they are derived is not explained.\nComparisons against methods in which rejection due to fairness is not an option may not be fair.\nSuggestion: either integrate suggestion regarding accounting for rejection above, or incorporate\nsome form of rejection (or simulate it) in the existing methods being compared to. It may be that\nthe best methodology is not FairPATE but some existing baselines if adjusted to include fairness\nrejection option.\nSmaller things:\nRejection rate is not shown in any experiments. One could view a misclassification as a\nrejection, however. Please include rejection rates or view them as misclassifications in the\nresults.\nThe distribution whose fairness need to be protected is left to be guessed by the reader. For\nprivacy, it is more clear that it is the private data that is sensitive and thus privacy\nbudgeting is done when accessing that private data as opposed to the public data. For fairness,\nthe impact on individuals in the private dataset seems to be non-existent as the decisions for\nthem are never made, released, or implemented in some downstream outcome. I presume, then, it is\nthe fairness needs to be respected on the public data.\nAlgorithm 1 and several points throughout the work hint at this. However, there is also the\nconsideration of intervention points 1,2,3 which seem odd as they points seen before any\nindividual for whom fairness is considered is seen. That is, fairness about public individuals\ncannot be made there, independent of any other issues such as privacy budgeting. Further, Theorem\n1 discusses a demographic parity pre-processor which achieves demographic parity on private data\nwhich I presume is irrelevant.\nThe statement\n\"PATE relies on unlabeled public data, which lacks the ground truth labels Y\"\nis a bit confusing unless one has already understood that fairness is with respect to public\ndata. PATE also relies on private labeled data to create the teachers.\nThe Privacy Analysis paragraph could be greatly simplified to just the last sentence regarding\npost-processing.\nSmallest things:\nDouble \"violations\" near \"violations of demographic disparity violations\".\nThe statement \"DP that only protects privacy of a given sensitive feature\" might be\nmischaracterizing DP. It is not focused on features or even data but rather the impact of\nindividuals\non visible results.\nQuestions:\nQuestion A: Is reasonable to ignore downstream decisions from queries rejected due to fairness\n  (i.e. contrary to my suggestion in the weaknesses above)?\nQuestion B: C1 makes a point that adding privacy after fairness may break fairness. What about in\n  expectation? Were one to view the demographic statistics defining fairness measures in\n  expectations, wouldn't they remain fair?\nQuestion C: Theorem 1 makes a statement about a pre-processor inducing privacy parameter\n  degradation but FairPATE (or PATE) appears to fit the definition of a pre-processor. If the point\n  of the Theorem is to argue against pre-processors, isn't it also arguing against PATE/FairPATE?\n  Unrelated, what is \"ordering defined over the input space X\" and why is it necessary?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Weaknesses", "Subheading": "Official CommentbyAuthors18 Nov 2023, 05:28Everyone", "Content": "Comment:\nWe thank the reviewer for their feedback. Below we address each point raised in \"Weaknesses\" section. We kindly note that in the following, we have grouped the reviewer's comments that addressed a common concern:\nThe implications of rejecting for fairness are not considered. Rejection for privacy has implications in terms of privacy budget and likewise rejections for fairness come with implications and ignoring them might be responsible for the observed gains on the Pareto frontier. Consider the noted rejection example: ...\nOur understanding of the reviewer's remark here is that in a human-in-the-loop setting, the mistakes of the human would ultimately be mistakes of the end-to-end system; and therefore, measuring the efficacy of the system should consider the human error as well. While we agree with the reviewer, we believe that measuring the end-to-end error of the human-in-the-loop system should be the topic of a future study.\nWe have added the following to our discussion in Section 7:\nWe note that in a human-in-the-loop system, the mistakes of the human would ultimately be mistakes of the end-to-end system; and therefore, measuring the efficacy of the system should consider the human error as well. We have shown that FairPATE and IPP enable such applications but whether such a system is the appropriate choice for a given application is out of the scope of the current study.\nIn arguments for intervention points, assumptions are made which preclude solutions. They assume the intervention need to be made independent of other mechanisms in PATE. That is, they cannot consider information internal to decision making that is not described by Figure 1 like individual teacher outputs. This leaves the possibility that some fairness methods might be able to integrated with PATE in a closer manner than the options described. One example is that they might include the teacher outputs instead of operating on the overall predicted class like Algorithm 1 assumes presently. C3 in particular suggests that some interventions will not account for privacy budget correctly due to special circumstances and suggests at Point 4, they can be budgeting can be handled correctly. Nothing is stopping a design from refunding privacy budget if a query is rejected subsequently to an intervention point.\nRegarding the suggested alternative scheme, we would like to to ensure that we understand the premise correctly: is the reviewer suggesting that we run PATE, accept and reject queries according to standard PATE analysis, and then apply the fairness intervention and reject possibly some more queries; and come back to give back some of the privacy budget?\nIf the answer is yes, then we note that standard PATE privacy analysis in Papernot et al. 2016 (summarized  in Section B in the appendix) is based on the probability of answering a certain query. To recoup the \"unused\" privacy budget one needs to find the difference between the privacy budget if the sample would have been answered and the privacy budget if the sample wouldn't have been answered. Note that the this is exactly what FairPATE already performs. Therefore, the suggested scheme is functionally equivalent to FairPATE but presents additional complexity.\nResults in the Pareto frontier show small improvements, no improvements, and in some cases worse results than prior baselines.\nSuggestion: Include more experimental samples in the results to make sure the statistical validity of any improvement claims is good. This may require larger datasets. Related, the experiments show error bars but how they are derived is not explained.\nWe have included more results in the revised manuscript. Please see our responses to\nReviewers XkhF and p5Rv.\nComparisons against methods in which rejection due to fairness is not an option may not be fair.\nSuggestion: either integrate suggestion regarding accounting for rejection above, or incorporate some form of rejection (or simulate it) in the existing methods being compared to. It may be that the best methodology is not FairPATE but some existing baselines if adjusted to include fairness rejection option.\nThe reviewer is correct that the IPP algorithm is an inference-time algorithm, therefore, its use is not limited to a FairPATE model.\nWe have run additional experiments on other models with the IPP;  and included the results in Section E.1 in the appendix. We find that FairPATE with IPP outperforms baselines (with IPP) in most regions fairness-privacy settings in both accuracy and coverage. See Figure 11 for a 2d Pareto surface, or Figure 12 for a corresponding 3d plot which showcases the accuracy performance improvements better."}, {"Heading": "Response to Weaknesses (Part 2)", "Subheading": "Official CommentbyAuthors18 Nov 2023, 05:30Everyone", "Content": "Comment:\nSmaller things:\nRejection rate is not shown in any experiments. One could view a misclassification as a rejection, however. Please include rejection rates or view them as misclassifications in the results.\nWe do report\ncoverage\nwhich is 1-rejection rate for the results that use the IPP. See Figure 5 where coverage is marked by the\ncolor\n; similarly the Pareto frontiers in the appendix all include coverage encoded in color.\nThe distribution whose fairness need to be protected is left to be guessed by the reader. For privacy, it is more clear that it is the private data that is sensitive and thus privacy budgeting is done when accessing that private data as opposed to the public data. For fairness, the impact on individuals in the private dataset seems to be non-existent as the decisions for them are never made, released, or implemented in some downstream outcome. I presume, then, it is the fairness needs to be respected on the public data.\nAlgorithm 1 and several points throughout the work hint at this. However, there is also the consideration of intervention points 1,2,3 which seem odd as they points seen before any individual for whom fairness is considered is seen. That is, fairness about public individuals cannot be made there, independent of any other issues such as privacy budgeting. Further, Theorem 1 discusses a demographic parity pre-processor which achieves demographic parity on private data which I presume is irrelevant.\nThe statement \"PATE relies on unlabeled public data, which lacks the ground truth labels Y\" is a bit confusing unless one has already understood that fairness is with respect to public data. PATE also relies on private labeled data to create the teachers.\nA fairness intervention and a privacy intervention have different goals w.r.t. the data distribution they target. In private learning, the goal is to protect the privacy of the sensitive training data only. In fair learning, the end goal is to ensure the algorithmic decisions (at inference-time) are fair. Within the context of FairPATE; from a private learning perspective, PATE (and FairPATE by extension) protect private teacher data (see Figure 1.a) and not the public unlabeld data. From a fair learning perspective, a FairPATE model is just like any other fairness-aware model is trained such that it achieves fairness on training data,\nwith the expectation that that behavior generalizes to inference-time.\nThese are standard assumptions which is why we have refrained from repeating them.\nSmallest things:\nDouble \"violations\" near \"violations of demographic disparity violations\".\nThe statement \"DP that only protects privacy of a given sensitive feature\" might be mischaracterizing DP. It is not focused on features or even data but rather the impact of\nindividuals\non visible results.\nWe thank the reviewer for their keen eye. We have fixed the typo and revised the sentence to\n..DP that only protects privacy of individuals with respect to particular sensitive features..."}, {"Heading": "Response to Questions", "Subheading": "Official CommentbyAuthors18 Nov 2023, 05:34Everyone", "Content": "Comment:\nQuestion A: Is reasonable to ignore downstream decisions from queries rejected due to fairness (i.e. contrary to my suggestion in the weaknesses above)?\nWe believe the alternative algorithm suggested is functionally similar to FairPATE. Please see our answer above.\nQuestion B: C1 makes a point that adding privacy after fairness may break fairness. What about in expectation? Were one to view the demographic statistics defining fairness measures in expectations, wouldn't they remain fair?\nThe aforementioned fairness constraints are rate constraints which are in and of themselves expected values. If the constraints remain effective post DP noising; it means that we have retained at least group membership information. This is inconsistent with the standard neighboring relationship that our standard privacy definition adopts (i.e., one-sample difference between two datasets) therefore it is a weaker privacy notion. We conjecture that this weaker privacy notion would be closer to the DP w.r.t. sensitive attributes (which defines the group membership), but that it is possibly even weaker because prior theoretical work with DP w.r.t. sensitive attributes (namely, Mozannar et al. 2020) have already shown that a second post-processing step is necessary to ensure the fairness constraint is satisfied post DP-noising.\nQuestion C: Theorem 1 makes a statement about a pre-processor inducing privacy parameter degradation but FairPATE (or PATE) appears to fit the definition of a pre-processor. If the point of the Theorem is to argue against pre-processors, isn't it also arguing against PATE/FairPATE? Unrelated, what is \"ordering defined over the input space X\" and why is it necessary?\nTheorem 1 does not apply to FairPATE because it does not pre-process the training data as described in theorem 1. FairPATE is a pre-processor from the point of view of the student which only sees public queries. An algorithm that pre-processes the teacher data would indeed fit Theorem 1 because it sees private sensitive data (this is intervention point 1 in Figure 1.a).\nThe assumption on the ordering is a technicality that simplifies the the proof.  In practice, one can almost always assume such an ordering exists. An example of such ordering would be to order images based on their pixel values in some specified order of height, width and channel starting by checking the first pixel, then the second pixel, and so on.\nWe once again thank the reviewer for their detailed feedback and kindly ask them to consider raising their score if we have addressed their concerns successfully. We are happy to answer further questions."}, {"Heading": "Official Comment by Reviewer Mex5", "Subheading": "Official CommentbyReviewer Mex504 Dec 2023, 22:22 (modified: 15 Mar 2024, 00:25)EveryoneRevisions", "Content": "Comment:\nThank you for the clarifications.\nWith regards to:\n\"From a fair learning perspective, a FairPATE model is just like any other fairness-aware model\n is trained such that it achieves fairness on training data, with the expectation that that\n behavior generalizes to inference-time. These are standard assumptions which is why we have\n refrained from repeating them.\"\nSome discussion there is warranted despite \"standard assumptions\". FairPATE (Algorithm 2 and\nAlgorithm 1) enforces fairness on the inference-time instances and I don't think this requires an\nassumption of being distributed similarly to training-time data. The approach does not resemble\nfairness-aware model training but rather an inference-time enforcement mechanism. The approach's\nDP protection of inference-time instances is only coincidental and not the goal while fairness\nw.r.t training-time instances is irrelevant as it is in any fairness training procedure\n(decisions for training instances have already been made in the past as indicated by their\nlabel). Is it accurate to say that \"FairPATE addresses inference data fairness and training data\nprivacy\"?\nWith regards to discussions about comparisons against baselines that do not have the option to\n\"reject due to fairness violation\"; I don't see a suggestion of solution for this work and given\nhow close the results already were, my concerns regarding this fairness and results are not\naddressed enough to warrant an upgrade in score large enough to the next threshold value."}]}, {"Heading": "Official Review of Submission9455 by Reviewer p5Rv", "Subheading": "Official ReviewbyReviewer p5Rv05 Nov 2023, 08:23 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper discusses the problem of interactions between fairness, privacy, and accuracy constraints for PATE (Private Aggregation of Teacher Ensembles) type algorithms for differentially private learning. PATE style of algorithms first uses the private data (partitioned into several small partitions) to train base classifiers (teachers). Then, the algorithm uses these teachers to label some public data in a privacy preserving way. In particular, it cleverly chooses which points it can label without sacrificing too much privacy. Then another classifier (student) is trained on this newly privately labelled dataset, which is then released to the user. In this paper, this labelling step is used to also incorporate privacy constraints. Finally, the paper uses empirical evidence to suggest that their algorithm achieves a better privacy fairness accuracy trade-off than Loewy et. al. 2023.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThe paper is written quite clearly and is easily readable. The arguments of the authors come out clearly without ambiguity and the reader can easily follow the train-of-thought. I appreciated that very much.\nI also found the main algorithmic idea of this paper quite nice. The idea of that the algorithm chooses which points to label not only on the bassi of the privacy constraint but also the fairness constraint is quite neat and could be useful in other contexts. I appreciated this.\nWeaknesses:\nDespite the interesting idea of the paper, I am unable to support this paper for acceptance. The four main reasons are as follows (in decreasing order of severity).\nW1\nSignificance: The paper aims to convey the significance of its contribution through experimental results (and I don't think there is anything wrong with it and in general support extensive empirical results), but the experiments present a rather bleak picture of the advantages of FairPATE.\nMinimal improvements\nMost importantly, there are rarely any results where the improvements of PATE over baselines is larger than 1%. For example in 1. Credit card dataset and 2. parkinson's dataset, there results differ by less than $1%$.\nSeveral examples of underperformance\nSeveral examples also show that FairPATE performs significantly worse than competitor. Examples include Demographic Parity for Adult dataset and UTK Face ($\\epsilon=5$).\nMisleading regarding diversity of experiments\nIn the introduction, the paper sells itself regarding performing a wide range of experiments but results on nearly half of these datasets are hidden away in the appendix without any mention in the main text and without comparisons. Experiments on\nCheXpert, CelebA, FairFace, and Retired-Adults\nare not available in the main text and it is very hard to interpret the results presented for this in the appendix.  In fact, there are\nno results of FAIR-PATE on CheXpert\nin the paper\nPlease show comparison on these for demographic parity and equalized odds (similar to Figure 2,3 with comparison to Loewe et. al.'s algorithm) on these four datasets. In addition, there are results in the literature that run DP algorithms on CelebA and CheXpert. Comparisons should be made to them to show what is the sacrifice being made compared to state-of-the-art.\n__ W2__\nWrong Conclusion from Theorem 2\nI did not go through the detail of Theorem 1 but I assume it is correct and follows from Group privacy arguments. However, the sentence above Theorem 1 (Point C2) claims that the result proves that  \" pre-processing ... will necesarrily degrade the guarantee of the .... private learning mechanism\". How ever this is not what the Theorem shows. The theorem only proves a privacy guarantee of $M\\odot P_{\\text{pre}}$ not that this is the tightest privacy guarantee possible for the composed mechanism. Am I missing something ?\nW3\nAbstaining from prediction for fairness reasons\nThe introduction justifies this as\n\"if a decision cannot be made without violating a pre-specified fairness metric, then the model can refuse to answer at which point the decision can be relegated to a human judge\"\n. If I have understood this correctly, this is a flawed argument. For example, consider the situation where there are group of data points from the majority group, on which if the algorithm predicts correctly the fairness will be violated and hence the algorithm abstains. Nevertheless, the prediction is easy for this group and the judge nevertheless predicts on them correctly and the overall fairness is still violated. Isn't this pointless then to relegate this to the judge ? Intuitively, it appears that any problem can be made fair this way by simply refusing to predict on certain majority groups thereby artificially boosting the fairness of the algorithm. The correct fairness solution should aim to improve performance on the minority group instead.\nW4\nUnfair Comparisons\nFair-PATE  and Loewy et. al's algorithm do not work under the same restrictions of differential privacy. Specifically, Fair-PATE uses public unlabelled data and Loewy et. al doesn't and we know that (even a small amount of) public data can severely help in improving accuracy of DP models (perhaps fairness too). Hence, it appears to me that Loewy et. al. uses a significantly stricter notion of privacy and achieves nearly comparable result and in some cases even better (for tabular datasets, there are no comparison on vision datasets with Loewy et. al.).\nQuestions:\nMotivation\nI understand that there are several works showing that privacy incurs a sacrifice in fairness. However, one possible approach to this problem is simply improving the accuracy and perhaps the resultant accuracy also leads to high fairness. In fact Berrada et. al. (2023) makes this argument with some experiments in their paper. I did not see an argument in this paper why this approach is not expected to solve all problems. Why should there be a trade-off between privacy and fairness ? Is it known in the literature and are there theoretical studies arguing about the necessity of a trade-off ?\nMissing baselines\nWhy is there no comparison with the algorithm of Zhang et. al. (2021), Kulynuych et. al. (2022) and Berrada et. al. (2023) ? They are mentioned in one or two sentences at the very end of the paper but without any comprehensive empirical comparison.\n**FairDP-SGD\" What is FairDP-SGD ? It doesn't seem to have been defined anywhere ? Is it an existing work ? Where is FAIR-PATE's result on CheXpert ?\nIn addition to this, please also also address\nW1,W2,W3,W4\n.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Weaknesses (Part 1)", "Subheading": "Official CommentbyAuthors18 Nov 2023, 05:48Everyone", "Content": "Comment:\nWe thank the reviewer for their feedback. Below we address each point raised in \"Weaknesses\" section.\nSeveral examples of underperformance\nSeveral examples also show that FairPATE performs significantly worse than competitor. Examples include Demographic Parity for Adult dataset and UTK Face ().\nFor the concerns about significance, please see our General Response.\nWe have noted clearly in the main paper that for the Adult dataset, under Demographic Parity DP-Fermi performs better than FairPATE for smaller epsilon values (Figure 10). However that trend is not consistent over different datasets (on UTKFace, FairPATE does better for lower epsilons and can even go lower in terms of achievable classification error\u2014see Figure 6). We provide a discussion on the cause of these performance differentials in response to\nReviewer XkhF.\nMisleading regarding diversity of experiments\nIn the introduction, the paper sells itself regarding performing a wide range of experiments but results on nearly half of these datasets are hidden away in the appendix without any mention in the main text and without comparisons. Experiments on\u00a0CheXpert, CelebA, FairFace, and Retired-Adults\u00a0are not available in the main text and it is very hard to interpret the results presented for this in the appendix. In fact, there areno results of FAIR-PATE on CheXpert\u00a0in the paper\nThe CheXpert results were already present in the paper (in the last figure of the appendix) but was unfortunately mislabeled. We apologize for the error.  The figure's label has been corrected in the updated manuscript.\nWe currently have results of FairPATE on CheXpert, CelebA and FairFace in Section H in the appendix.  Unfortunately given the space constraints we cannot add all the aforementioned results in the main text.\nPlease show comparison on these for demographic parity and equalized odds (similar to Figure 2,3 with comparison to Loewe et. al.'s algorithm) on these four datasets. In addition, there are results in the literature that run DP algorithms on CelebA and CheXpert. Comparisons should be made to them to show what is the sacrifice being made compared to state-of-the-art.\nDP-FERMI is an expensive algorithm to run. On CelebA, it requires 33 hours to train one model for 200 epochs as specified in the paper without parameter tuning on an NVIDIA A100; while the model has at least 4 hyper-parameters to tune. Nevertheless, we are currently in the process on running DP-FERMI on CelebA, and we will report the results as we acquire them in the coming days.\nW2 Wrong Conclusion from Theorem 2\nI did not go through the detail of Theorem 1 but I assume it is correct and follows from Group privacy arguments. However, the sentence above Theorem 1 (Point C2) claims that the result proves that \" pre-processing ... will necesarrily degrade the guarantee of the .... private learning mechanism\". How ever this is not what the Theorem shows. The theorem only proves a privacy guarantee of\u00a0\u00a0 $M \\odot P_{\\mathrm{pre}}$ not that this is the tightest privacy guarantee possible for the composed mechanism. Am I missing something ?\nWe are not sure we follow the reviewer's comments here. Does the reviewer take issue with the phrasing of the claim \" pre-processing ... will necesarrily degrade the guarantee of the .... private learning mechanism\"?\nIf yes, then we apologize for the confusion. We can adjust the claim as\npre-processing ... will necessarily degrade the guarantee of the .... composed preprocessor and private learning mechanism\nOf course, the private learning algorithm does not degrade on its own; but via the inclusion of a fairness pre-processor, we have shown that the joint fair and private mechanism will have degraded privacy guarantee which is always an upper bound on the privacy leakage\nWe are also unsure of what the reviewer mean by the \"tightest privacy guarantee possible for the composed mechanism.\" Do they mean that another $\\mathcal{P}_\\text{pre}$ could possibly reduce the privacy cost? If yes, then given that the pre-processor is simply ensuring the fairness definition holds (via a filter that class balances the dataset conditioned on the sensitive attribute) can the reviewer recommend another pre-processor?"}, {"Heading": "Response to Weaknesses (Part 2)", "Subheading": "Official CommentbyAuthors18 Nov 2023, 05:57Everyone", "Content": "Comment:\nW3\nAbstaining from prediction for fairness reasons\nThe introduction justifies this as\n\"if a decision cannot be made without violating a pre-specified fairness metric, then the model can refuse to answer at which point the decision can be relegated to a human judge\"\n. If I have understood this correctly, this is a flawed argument...\nWe think there has been a misunderstanding regarding a) purpose of the reject option for fairness, and b) the inference-time nature of the IPP algorithm that implements it. What the reject-option enables is for a human (e.g. a trained judge) to take over the decision making in case of a fairness constraint violation.\nThe presumption here is that a trained judge is better placed to make impactful decisions than the algorithm.\nTherefore, the human judge can choose to accept violations of the fairness constraint on a case-by-case basis. We do not believe the human judge should be placed under the same constraints as the algorithm given the vast difference in training, experience and accountability faced by the human judge versus the algorithm.\nConsider the situation where there are group of data points from the majority group, on which if the algorithm predicts correctly the fairness will be violated and hence the algorithm abstains\nPlease note that IPP is an\ninference-time\nalgorithm. It is incorrect to say that the \"algorithm predicts correctly that the fairness will be violated\" because there is no prediction involved in this process. For instance, using Demographic Parity, the fairness metric requires that the rates of acceptance be similar for different subgroups. Since IPP is releasing decisions, it can keep an exact record of the decisions released (this is what $m(z,k)$ counter keeps in Algorithm 1). Therefore, the rates calculated based on this record is exact and so are the detected violations that would prompt the algorithm to reject a certain query.\nIntuitively, it appears that any problem can be made fair this way by simply refusing to predict on certain majority groups thereby artificially boosting the fairness of the algorithm. The correct fairness solution should aim to improve performance on the minority group instead.\nOur standard fairness metric does not require a notion of majority vs. minority. At any given decision point, our algorithm (IPP) bounds the worst-case fairness violations experienced by any subpopulation. Furthermore, the reject-option is not a silver bullet; as it comes with a trade-off with coverage (1-rejection rate). Therefore the more rejections translate more work for the human judge. It is, therefore, in the best interest of deployer of the upstream algorithm (FairPATE or DP-FERMI, or any other fairness-aware) algorithm to minimize fairness violations at training-time; because an unfair algorithm will increase the work that human judges would need to do\u2014 eliminating the need for the fair algorithm in the first place.\nW4\nUnfair Comparisons\nFair-PATE and Loewy et. al's algorithm do not work under the same restrictions of differential privacy. Specifically, Fair-PATE uses public unlabelled data and Loewy et. al doesn't and we know that (even a small amount of) public data can severely help in improving accuracy of DP models (perhaps fairness too). Hence, it appears to me that Loewy et. al. uses a significantly stricter notion of privacy and achieves nearly comparable result and in some cases even better (for tabular datasets, there are no comparison on vision datasets with Loewy et. al.).\nWe wish to clarify and disentangle two issues that the reviewer points out: a) access to public data, b) access to additional data. All of our baselines experiments are conducted using the exact data splits of Lowy et al. 2023. This means that we do not use any additional training data compared to our baselines. Instead, we use the training data available to us and partition it into public and private. Consistent with the original PATE framework, we do not use the labels of the public dataset; instead we train teacher models on the private partition and label the public points under privacy and fairness constraints. In Figure 1.a, a DP-SGD method like Lowy et al. 2023 replaces the shaded Private Model. Therefore, we respectfully disagree with the reviewer's assessment that the comparisons are not fair data-wise. As a matter of fact, in our experiments, DP-Fermi receives additional information compared to FairPATE (namely, the labels of the public data partition)."}, {"Heading": "Response To Questions", "Subheading": "Official CommentbyAuthors18 Nov 2023, 06:02Everyone", "Content": "Comment:\nMotivation.\u00a0I understand that there are several works showing that privacy incurs a sacrifice in fairness. However, one possible approach to this problem is simply improving the accuracy and perhaps the resultant accuracy also leads to high fairness. In fact Berrada et. al. (2023) makes this argument with some experiments in their paper. I did not see an argument in this paper why this approach is not expected to solve all problems. Why should there be a trade-off between privacy and fairness ? Is it known in the literature and are there theoretical studies arguing about the necessity of a trade-off ?\nBerrada et al. 2023 only consider unfairness with respect to loss disparity. This particular fairness metric is inherently a measure of the generalization of the model. That is, if a model is well generalized, then it is also well-generalized for different subpopulations of the data; therefore, it is only natural that it would exhibit lower loss disparity. As a result Berrada et al. 2023 do not consider (or rather, need to consider) an unfairness mitigation step. Our results (and that of Lowy et al. 2023 and others) show that for other group fairness metrics, this is not the case. That is, fairness and accuracy do not align so well all the time.\nIn terms of theoretical results Cummings et al. 2019 provide an the impossibility result for DP learning and exact fairness; while also demonstrating a positive result for the case of approximate fairness (our settings). More recently, Mangold et al. 2023 have showed that \"the fairness (and accuracy) costs induced by privacy in differentially private classification vanishes at a $O(\\sqrt{p}/n)$ rate, where n is the number of training records, and p the number of parameters.\" This is achieved through a proof of pointwise Lipschitz smoothness of group fairness metrics with respect to the model where the pointwise Lipschitz constant explicitly depends on the confidence margin of the model, and may be different for each sensitive group. Therefore, in the presence of minority groups in the data, we can expect a non-zero gap between private and non-private models.\nMissing baselines.\u00a0Why is there no comparison with the algorithm of Zhang et. al. (2021), Kulynuych et. al. (2022) and Berrada et. al. (2023) ? They are mentioned in one or two sentences at the very end of the paper but without any comprehensive empirical comparison.\nKulynych et. al 2021 does not account for the privacy leakage of its importance sampling step therefore a fair comparison with methods such as ours or Lowy et al. 2023 is not possible\nBerrada et al. 2023 only considers loss parity, and has no unfairness mitigation for two of our gorup fairness metrics (see our previous answer for more details)\nZhang et al. 2021 implement early stopping to save on privacy budget; however, they do not measure the privacy leakage of their early stopping criterion therefore the privacy loss reported is underestimated\nWhat is FairDP-SGD ? It doesn't seem to have been defined anywhere ? Is it an existing work ? Where is FAIR-PATE's result on CheXpert ?\nThis is an error on our part. The figure in the appendix was mislabelled. We apologize for the confusion. The figure's label has been corrected in the updated manuscript.\nWe once again thank the reviewer for their detailed feedback and kindly ask them to consider raising their score if we have addressed their concerns successfully. We are happy to answer further questions."}, {"Heading": "Official Comment by Reviewer p5Rv", "Subheading": "Official CommentbyReviewer p5Rv22 Nov 2023, 07:03Everyone", "Content": "Comment:\nI thank the authors for their response. I have read through the entire comment in response to my questions. I will try to keep my replies brief and provide suggestions on what I think can improve the manuscript in subsequent drafts.\n[W1]  Currently the paper presents a new algorithm for improving fairness-accuracy pareto frontier but the main weakness  here is that in a significant number of situations this improvement does not happen but rather a degradation happens. I would recommend that the authors rethink about what the main contribution is, perhaps identify more clearly in the main text where they expect an improvement and where they don't and then show that through experiments. Doing this discussion through rebuttal is not the best as this a major point of this paper.\nMoreover, i had noticed the plots in Appendix H, but the plots were not very informative regarding how important the improvement is due to the missing comparisons. Again I appreciate that the authors are running the experiments but this is an important contribution of the paper and hence the experiments for this should not really be done during the rebuttal and it should be possible to present the \"improvements of the algorithm on seven distinct datasets\" in the main paper. Perhaps, the paper needs a restructuring if all the exerimental improvements obtained by a novel algorithm cannot be presented in the main pages of the paper."}, {"Heading": "Response to Weakness 2", "Subheading": "Official CommentbyReviewer p5Rv22 Nov 2023, 07:06Everyone", "Content": "Comment:\nMy point in weakness 2 is that the result shows that the composed mechanism is at least $(\\epsilon,\\delta)$ DP.\nHowever the argument in the text says that the \"will necessarily degrade the guarantee\".\nThe first sentence is an upper bound on the privacy guarantee of the mechanism whereas the second sentence is a sentence akin to a lower bound."}, {"Heading": "Official Comment by Reviewer p5Rv", "Subheading": "Official CommentbyReviewer p5Rv22 Nov 2023, 07:11Everyone", "Content": "Comment:\nI am a bit confused with what the authors mean by \"Please note that IPP is an inference-time algorithm. It is incorrect to say that the\n\"algorithm predicts correctly that the fairness will be violated because there is no prediction involved in this process\"\nand\n\"Since IPP is releasing decisions, it can keep an exact record of the decisions released (this is what \n counter keeps in Algorithm 1). Therefore, the rates calculated based on this record is exact and so are the detected violations that would prompt the algorithm to reject a certain query.\"\nIf the model is creating an output, which is then judged by IPP whether to release or not - isn't this an example of prediction ? Or pehaps I am misunderstanding something"}, {"Heading": "Official Comment by Reviewer p5Rv", "Subheading": "Official CommentbyReviewer p5Rv22 Nov 2023, 07:13Everyone", "Content": "Comment:\nRegarding W4, can the authors clarify whether the number of\nprivate\ndata (excluding public unlabelled data) points seen by FairPATE and Loewy et. al. are exactly same ?"}, {"Heading": "Regarading existing literature", "Subheading": "Official CommentbyReviewer p5Rv22 Nov 2023, 07:24Everyone", "Content": "Comment:\nKulynych et. al. - Do the authors mean that Algorithm in Kulynych et. al. (which does DP importance sampling) does not respect the $(\\epsilon,\\delta)$-DP ?\nBerrada et. al. - Perhaps the authors can then compare their method with the algorithm in berrada et. al. and show that Berrada et. al.'s method indeed suffers in DP and EO. This would be a strong and important point to make showing the necessity of this algorithm.\nI am a bit confused what the authors mean about Cummings et. al's result. If their paper shows a negative result about pure DP but positive result about Approximate DP, then this is arguing against the existence of unfairness problem in approx DP ?"}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:46Everyone", "Content": "Comment:\nWe thank the reviewer for engaging with us. We appreciate the time and effort you have dedicated to reviewing our work.\n[W1] Currently the paper presents a new algorithm for improving fairness-accuracy pareto frontier but the main weakness here is that in a significant number of situations this improvement does not happen but rather a degradation happens. I would recommend that the authors rethink about what the main contribution is, perhaps identify more clearly in the main text where they expect an improvement and where they don't and then show that through experiments. Doing this discussion through rebuttal is not the best as this a major point of this paper.\nMoreover, i had noticed the plots in Appendix H, but the plots were not very informative regarding how important the improvement is due to the missing comparisons. Again I appreciate that the authors are running the experiments but this is an important contribution of the paper and hence the experiments for this should not really be done during the rebuttal and it should be possible to present the \"improvements of the algorithm on seven distinct datasets\" in the main paper. Perhaps, the paper needs a restructuring if all the exerimental improvements obtained by a novel algorithm cannot be presented in the main pages of the paper.\nWe thank the reviewer for their feedback. We made a general statement regarding our contributions (in the Author General Response). Please also consider our edits and response to\nreviewer Xkhf\nnewest comments.\nMy point in weakness 2 is that the result shows that the composed mechanism is at least\u00a0 $(\\epsilon, \\delta)$ -DP. However the argument in the text says that the \"will necessarily degrade the guarantee\". The first sentence is an upper bound on the privacy guarantee of the mechanism whereas the second sentence is a sentence akin to a lower bound.\nWe have edited the sentence in question for clarity to:\n(C2) Fairness pre-processing increases the cost of private training. In Theorem 1, we show that pre-processing the training data to equalize subpopulation rates will degrade the privacy guarantee of the composed mechanism (i.e., preprocessor followed by private learning) compared to that of the private learning mechanism alone\nI am a bit confused with what the authors mean by \"Please note that IPP is an inference-time algorithm. It is incorrect to say that the\n\"algorithm predicts correctly that the fairness will be violated because there is no prediction involved in this process\"\nand\n\"Since IPP is releasing decisions, it can keep an exact record of the decisions released (this is what counter keeps in Algorithm 1). Therefore, the rates calculated based on this record is exact and so are the detected violations that would prompt the algorithm to reject a certain query.\"\nIf the model is creating an output, which is then judged by IPP whether to release or not - isn't this an example of prediction ? Or pehaps I am misunderstanding something\nWhen IPP is employed (and it can be employed for any model, not just FairPATE) then, a binary classification task $X \\mapsto {0, 1}$ is turned into a selective classification $X \\mapsto {0, 1, \\bot}$ where $\\bot$ means that no prediction is released (a query is not answered). Therefore, the answer to the question is no. If the query is rejected, it is as if no prediction has taken place. For instance, this query will not be considered when measuring accuracy\u2014but it will be considered when measuring coverage.\nRegarding W4, can the authors clarify whether the number of\nprivate\ndata (excluding public unlabelled data) points seen by FairPATE and Loewy et. al. are exactly same ?\nWe believe we have answered this question in the follow-up in the general comment. The answer is no because evaluating any PATE model involves throwing out labels for a subset of the training set that is going to function as the unlabeled set. Note two things: first,\nthis is not in the favor of the PATE mechanism (because we are losing label information)\n, two, this is not a choice given the semi-supervised nature of PATE.\nFor sake of completeness, please consider the alternative scenario where we have ensured the premise of the question (that is we have exactly equal number of private samples for both DP-SGD and PATE), then this means that we should find another dataset to use as unlabeled dataset in PATE. Under this evaluation scheme, the PATE model receives strictly more data samples.\nWe find this evaluation scheme unfair to supervised models such as DP-SGD."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:50Everyone", "Content": "Comment:\nKulynych et. al. - Do the authors mean that Algorithm in Kulynych et. al. (which does DP importance sampling) does not respect the\u00a0 $(\\epsilon, \\delta)$ -DP ?\nNo, their mechanism remains $(\\epsilon, \\delta)$ -DP private (per Lemma B.7). We also wish to correct a prior assertion that they do not account for the cost of their importance sampling. They do. We reached out to one of the authors and quote them here verbatim:\nIn the sense of \u201cprivacy cost\u201d as in your Theorem 1, in DP-IS-SGD you are indeed also paying additional cost for importance sampling. In DP-IS-SGD, we set the mini-batch sampling probability for privacy accounting to $p^* \\propto \\frac{1}{n_G}$ , which grows as $n_G$ gets smaller, where $n_G$ is the size of the smallest group. (The privacy guarantee deteriorates when the mini-batch sampling probability increases given all other parameters are fixed.) As a result, you would likely need to add more noise in DP-IS-SGD for the same $(\\epsilon, \\delta)$ . DP-IS-SGD can be effective when groups are not too small or when the effects of reducing disparate impact through importance sampling outweigh the additional privacy costs.\nBerrada et. al. - Perhaps the authors can then compare their method with the algorithm in Berrada et. al. and show that Berrada et. al.'s method indeed suffers in DP and EO. This would be a strong and important point to make showing the necessity of this algorithm.\nWe note that Berrada does not introduce a novel method. Their main contention is that DP-SGD models trained to good generalization levels  (e.g. trained with large enough batch sizes, etc.)  do not exhibit large loss disparity.  We previously argued that this group fairness notion (Loss Parity) is essentially a generalization notion which is why Berrada et al's claims are reasonable. Unfortunately, they have not released any code or data, and we cannot be expected to replicate their industrial-scale experiments due to computation and resource constraints.\nHaving said that, we have run more experiments with the error parity notion using our test bench and indeed we observe that compared to other group fairness notions (demographic parity and equality of odds), error parity results are the least affected by the introduction of differential privacy training.\nWe report the new results in Section E.3 of the revised manuscript.\nI am a bit confused what the authors mean about Cummings et. al's result. If their paper shows a negative result about pure DP but positive result about Approximate DP, then this is arguing against the existence of unfairness problem in approx DP ?\nNo. What Cummings defines as approximate fairness is non-zero fairness violations ( $\\gamma > 0$ ). In other words, Cummings' is a theoretical result that does not differentiate between different positive $\\gamma$ values, as long as they are non-zero. In Theorem 2, they show that the bound on the number of samples needed to learn a hypothesis  (in a PAC sense) in a differentially private way, has an inverse relationship with $\\gamma$ (they use the notation $\\alpha$ instead); meaning that the smaller the $\\gamma$ the more samples it would be needed to satisfy this existence condition. In the context of your question, the negative result is more meaningful which proves eliminating the unfairness problem altogether is impossible.\nThank you"}, {"Heading": "Official Comment by Reviewer p5Rv", "Subheading": "Official CommentbyReviewer p5Rv23 Nov 2023, 06:58Everyone", "Content": "Comment:\nI thank the authors for engaging.\nRegarding W1 and W2. I maintain my concern about W1 that this is not something that can solved with minor edits and clarification but needs restructuring. The comments here in the rebuttal is a good place to start in my opinion.\nRegarding W2, if the authors agree with the difference between what the result conveys and what a lower bound would convey, perhaps the author can include a lower bound result in the work to make their original point, which is I think important to the work that pre-processing indeed necessarily costs privacy.\nRegarding IPP, I do not agree with the characterisation. This is prediction with abstention or selective classification, as the authors describe. And it is not clear to me from the text, how IPP guarantees that it will not simply relegate examples that are 1) easy to classify 2) and classification will lead to unfairness to the human classifier even though there are other examples in the test set which are more nuanced and should have been relegated to the human classifier. An easy-to-see instance is that first the model can do usual classification without abstention, then check its fairness, then figure out which examples it can refuse to classify in order to stay within this budget (note that there can be many such sets; the algorithm can choose a set randomly), and then abstain from predicting for these.\nFinally, regarding W4, I see the stress the authors are putting on the absence of labels and using it to say that PATE sees stricly less information than Loewy et. al. I also reject this characterisation by arguing that the PATE algorithm seems (unlabelled) data points whose privacy it does not need to preserve whereas Loewy et. al. needs to preserve privacy of every data point it sees."}, {"Heading": "Official Comment by Reviewer p5Rv", "Subheading": "Official CommentbyReviewer p5Rv23 Nov 2023, 07:02Everyone", "Content": "Comment:\nThank you for the correction. I would recommend adding an \"addendum\" to the previous comment so that people reading this conversation do not get the wrong information.\nI would also recommend adding your own comparisons and analysis in the paper about Kulynych et. al. as opposed to quoting the authors, in particular empirical comparisons.\nThank you for adding E.3, I beleive this should go in the paper and you can compare their \"extended DP-SGD\" with your algorithm on all the vision datasets you use for all three metrics.\nFinally, Regarding Cummings et. al. result, I agree their negative result is more important which is why i was surprised to see your mention their positive result in the previous message. However, their negative result only deals with pure DP and not with approximate DP and I would recommend searching for a negative result from literature that also shows a trade-off with privacy and fairness."}]}]}, "w73feIekdO": {"paper_info": {"Supplementary Material": "zip", "Primary Area": "applications to robotics, autonomy, planning", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Coreset, Motion vectors, Segments, Robotics, Structure from motion, non-convex optimization", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "In this work, we suggest computer vision methods, specifically for video tracking and map creation from video.\nTo this end, we utilize motion vectors and clusters, which are computed very efficiently in standard video encoders, usually via dedicated hardware.\nWe suggest a provably good tracking algorithm for clustering these vectors, by considering them as segments.\nFor this, we utilize a definition of a \\emph{coreset} which is essentially a weighted set of points that approximates the fitting loss for every model, up to a multiplicative factor of $1\\pm\\varepsilon$.\nOur method supports $M$-estimators that are robust to outliers, convex shapes, lines, and hyper-planes.\nWe demonstrate the empirical contribution of our clustering method for video tracking and map creation from video, by running it on micro-computers (Le-Potato and Raspberry Pi) on synthetic and real-world videos with real-time running time.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9453", "PDF Url": "https://openreview.net/pdf?id=w73feIekdO"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9453 by Area Chair Jo5a", "Subheading": "Meta ReviewbyArea Chair Jo5a07 Dec 2023, 09:49 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThe paper introduces a novel clustering approach grounded in the concept of coresets. The idea of directly using motion vectors generated by video codecs as inputs for computer vision tasks is intriguing. The paper demonstrates the effectiveness of this approach in achieving real-time tracking and 3D map creation from videos. The contributions of the paper include a pioneering clustering algorithm designed for motion vectors, a coresets-based strategy that effectively reduces the computational complexity of the clustering algorithm, and the successful implementation of the clustering algorithm on low-end boards, facilitating real-time performance.\nThe paper's primary contributions encompass extending generalization coreset concepts from points to segments and the development of a computationally efficient tracking algorithm. The notion of utilizing vector motions directly generated by video codecs as inputs for computer vision tasks is intriguing. The suggested vector clustering approach appears theoretically robust, offering real-time solutions to a couple of computer vision tasks. The paper also brings to the attention of the computer-vision community an important class of \"probably approximately correct\" algorithms.\nAll reviews express concerns regarding the insufficient and unconvincing experiments presented in the paper. Firstly, the paper exclusively showcases results in only two scenarios, with the demonstration often lacking clarity, comprehensive reporting, and in-depth analysis. Secondly, a notable gap exists in the absence of comparisons with other methods. Classic tracking methods frequently employ clustering and robust statistical approaches, and it is crucial to benchmark the proposed method against these alternatives to showcase its advancements in the state of the art. The paper's presentation requires enhancement, as highlighted by several reviewers' suggestions. Notably, there is a need for an introduction and a discussion of related work, which would aid readers unfamiliar with the field in comprehending the paper's contributions. The proofs presented in Section 2 are deemed obscure, posing difficulty for readers lacking relevant background knowledge to grasp. Additionally, there are suggestions for improving the presentation of algorithms. There was no rebuttal posted by the authors before the deadline for discussion. As a result, all issues raised remain unresolved.\nJustification For Why Not Higher Score:\nThere are serious concerns regarding the experiments and presentation of the paper. There was no rebuttal posted by the authors before the deadline for discussion. As a result, all issues raised remain unresolved.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "Official Review of Submission9453 by Reviewer MMZa", "Subheading": "Official ReviewbyReviewer MMZa09 Nov 2023, 15:31 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper focuses on a tracking algorithm that takes as input motion vectors obtained from standard video encoders.  The main contribution is the leveraging of the notion of coresets applied to segments, obtaining representative clusters and tracking. The bulk of the paper is on the extension of coresets for point sets to segments.  The tracking algorithm result is illustrated in two examples - big buck bunny video example, and 3D map estimation from drone video.\nSoundness:\n2 fair\nPresentation:\n1 poor\nContribution:\n2 fair\nStrengths:\nThe main claim of the paper is in the generalization coreset ideas for points to segments and in the derivation of a tracking algorithm that is computationally efficient.  Certain claims are made about generalization of previous theoretical work (that I am not fully familiar with and cannot comment).\nWeaknesses:\nWhile I understand the rationale and setup of the problem for translating motion vector inputs as coresets and tracking,  the  results on the two datasets are not convincing.  While the paper talks a lot about how this approach is substantially better in comparison to neural-net based methods, it fails to refer to any of the classic methods in tracking where clustering, robust statistical methods are used.  The paper does refer to a review paper and states that there are over 1000 articles on the subject.  However, if the central aim of the paper is to demonstrate the advancement in tracking algorithms the paper should demonstrate the effectiveness of the algorithm designed by comparing it with at least one alternative (e.g. mean-shift based tracking ,  Comaniciu et al (CVPR 2000)).  I note that the mean-shift based tracker performed in real-time in low computational power settings for given candidate regions in a video over two decades ago.\nQuestions:\nI have several questions that will help me identify what the central contributions are and on how the proposed method outperforms over other methods in the state of the art.\nIs your contribution mainly the extension of coreset idea to segments?   There has been work on coresets for sets of lines (e.g. Coreset for Line-Sets Clustering, Lotan et al, 2022).  Please elaborate on how your method is different.\nHave you compared your tracker with other methods in opencv and if so, what was the outcome? You refer to OpenCV in your paper and it is not clear from the paper how it was used in your experiments.\nCan you elaborate on the tradeoff between computational complexity of your technique and (epsilon, delta) choices during coreset construction?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9453 by Reviewer wXa9", "Subheading": "Official ReviewbyReviewer wXa901 Nov 2023, 04:02 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper introduces innovative computer vision techniques that integrate classical machine learning strategies to enhance efficiency and robustness. The authors showcase the practical impact of their clustering method in video tracking and map creation from video, successfully executing it in real-time on micro-computers. The contributions of the paper encompass a novel clustering algorithm for motion vectors, a coreset-based approach that reduces the computational complexity of the clustering algorithm, and the implementation of the clustering algorithm on low-end boards, enabling real-time performance.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n3 good\nStrengths:\nS1. The paper is well-written and most of the content is quite easy to follow. \nS2. The main contribution of this work is significantly interesting by incorporating traditional machine learning techniques in the age of deep learning. \nS3. The proposed vector clustering is theoretically sound, I tried my best to examine most of them and did not find obvious errors. \nS4. Overall, I have significant concerns regarding the experimental section of the paper. Firstly, the proposed method is only validated in three application scenarios, and the experimental results are not extensively reported or analyzed, neither in the main text nor in the supplementary material.\nWeaknesses:\nW1. The proofs in Section 2 are rather obscure and difficult for readers without relevant background knowledge to comprehend. Additionally, many crucial steps are relegated to the supplementary material, greatly impacting the readability of this paper.\nW2. This paper lacks an introduction and discussion of related works, making it challenging for readers unfamiliar with the field to fully understand the contributions of this article.\nQuestions:\nPlease check the weaknesses listed above.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9453 by Reviewer qadD", "Subheading": "Official ReviewbyReviewer qadD31 Oct 2023, 18:59 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper introduces a fully polynomial randomized approximation scheme for the clustering of motion vectors, which is then applied to the motion vectors produced by standard video encoders to the problem of visual tracking. The approximation scheme is an adaptation of the results of Feldman and Schulman (2012), which is concerned with robust clustering of points in arbitrary metric spaces, to\nsegments\nas defined by the paper. Specifically, points on segments are sampled at uniform intervals, under a condition on the number of points $\\epsilon'$ such that the approximation in Feldman and Schulman (2012) is preserved.\nSoundness:\n1 poor\nPresentation:\n1 poor\nContribution:\n2 fair\nStrengths:\nOriginality\nThe idea of directly using vector motions produced by video codecs as inputs to computer-vision tasks is interesting, as it is the broader approach of designing FPRAS for computer vision problems.\nQuality\nThe paper brings a broad review of the literature and is self-contained, including detailed proofs of its several lemmas and theorems.\nClarity\nEvery term is defined, and the illustration in Figure 2 helps the reader to understand the geometric meaning of the cost function defined in Equation 1.\nSignificance\nThe paper brings to the attention of the computer-vision community an important class of \"probably approximately correct\" algorithms, as in the title of Valiant's book.\nWeaknesses:\nGeneral\nThe paper is not well organized. The first two sections of the Introduction, titled \"Video Tracking\" and \"Motion Vectors,\" do not describe the problem addressed by the paper. The subsection \"Our Approach\" does not describe the approach at all but introduces and illustrates the definition of a cost function which is discussed only much later in the paper. The subsection \"Coresets\" brings a definition of coreset, followed by an unusually long quote from the paper by Denisov et al. (2023). That section makes a reference to a \"segment clustering problem stated in Section 2.1\" that has a small coreset, but I was not able to parse the remaining of that paragraph. The references to Jubran et al. (2021) and Rosman et al. (2014) seem unnecessary, as they refer to exceptions (or so I understood) to the stated goal of having coresets which are weighted subsets of the inputs.\nAlgorithm 1 should be replaced for the simple formula that computes $\\epsilon'$. This value is then used to produce samples at uniform intervals on the motion vectors. It is not clear how the claimed novelty of Algorithm 1 generalizes, as stated, previous work by Rosman et al. (2014), which is concerned with fitting segments to points, rather than sampling from segments.\nThe structure of Algorithm 2 is not at all illustrated by Figure 3, as attempted. A key component of that algorithm (Feldman and Shulman CORESET algorithm (2012)) was replaced in Figure 3 with a different method (Bachem et al. (2018)) for \"easier implementation.\"\nEvaluation\nExperimental evaluation is insufficient. There is scant comparison, and no quantitative evaluation other than an unusual computation of frames-per-second (FPS). It is not valid to subtract all computing times but clustering from the pipeline, divide the number of frames by whatever remains and claim that as an FPS.\nThe role of Artuhr and Vassilvitskii (2007) in the empirical evaluations is unclear since the output of Algorithm 2 should be a clustering of the segments. One the other hand, there is no mention of Algorithm 2 in that section, only of Algorithm 1.\nThe steps of the video tracking method are unclear. The is no explanation for what \"Add for each motion vector its degree to (0, 10 and (1, 0)\". The is no discussion of how one moves on beyond $k = 2$.\nThe jump from clustering of motion vectors to map creation leaves a gaping hole in the paper. The empirical evaluation of 3D map creation follows similar steps, which are repeated almost verbatim and should be omitted.\nThere are citations that are unusual to the computer-vision community: the OpenCV library, the Python 3 reference Manual, an Ubuntu Linux guide, the Rasberry Pi user guide, Vigdear manual, CutstomTkInter, and others.\nThe Conclusion section of the paper cannot be moved to an appendix.\nQuestions:\nIt is curious that the number of samples on a segment does not depend on the length of the segment, according to Algorithm 1. Is there any intuition for why?\nStill in Algorithm 1, it is correctly stated that $r$ is defined in Definition 2.3; however, given the comment on the second paragraph following the description of the algorithm (\"Note that $r$ in Algorithms 1...\"), it should be provided as an input, since the function $D$ to which $r$ corresponds is not.\nI assume the word \"tracing,\" which appears twice in Section 3, is at typo, and \"tracking\" was meant instead?\nWas Algorithm 2 used at all? What is the purpose of using Arthur and Vassilvitskii (2007) if Algorithm 2 already produces a clustering? How is it possible for Arthur and Vassilvitskii (2007) algorithm to have been implemented in Bradski (2000)?\nHow is a motion-vector clustering algorithm applied to map creation? Why computations on Raspberry-Pi and utilization of gyroscope contribute to \"fair comparison\"?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n1: strong reject\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9453 by Reviewer aQsA", "Subheading": "Official ReviewbyReviewer aQsA30 Oct 2023, 05:39 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper proposes a clustering approach based on the idea of coresets. It is demonstrated in the paper that the proposed formulation helps perform tracking and 3D map creation from videos in real time. Few experimental results are shown to demonstrate the claims made in the paper.\nSoundness:\n1 poor\nPresentation:\n1 poor\nContribution:\n2 fair\nStrengths:\nReal-time solution to a couple of popular computer vision problems.\nWeaknesses:\nNot a well-written paper. So many typos and grammatical mistakes.\nThe paper widely discusses the existing literature in theory and emphasizes less of the actual contributions of the paper other than making few methods real-time.\nThe results are poorly demonstrated. I am unable to conclude how good of a map is obtained using the proposed method.\nAlso confusion about video tracing and tracking \u2014see Sec. 3.\nRefer Questions section for more comments.\nQuestions:\nAbstract:\nTo this end, we utilize motion vectors and clusters. What clusters authors are referring to. I believe it should be clustering algorithms/methods.\nwith real-time running time -> that gives real-time performance.\nIntroduction\nA meta-survey on such approaches Zou et al. (2019) states that in recent years -> kindly use \\citep{} to put parentheses for citation or rewrite this line.\n\u201cfool\" -> use `` and \u2019\u2019 for the apt quotes.\nFigure 1 -> the blue motion vector is hardly visible. Furthermore, kindly use a different color for the blue motion vector as it correlates with the flower in the background.\nFigure 2 -> figures are placed side to side, whereas captions suggest top and bottom. Kindly correct.\nThere are many grammatical mistakes in the paper. Kindly improve the writing of the paper.\nGeneral Comment:\nWith all due respect, tracking and map creation is not computer vision. These are a couple of  problems studied in computer vision. Kindly modify your paper title.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}]}, "kmn0BhQk7p": {"paper_info": {"Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Privacy, Large Language Models", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "TL;DR": "We present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from texts given at inference.", "Abstract": "Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models\u2019 inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals\u2019 privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to 85% top-1 and 95% top-3 accuracy at a fraction of the cost (100x) and time (240x) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for stronger and wider privacy protection.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "Supplementary Material": "zip", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Primary Area": "societal considerations including fairness, safety, privacy", "Submission Number": "9451", "PDF Url": "https://openreview.net/pdf?id=kmn0BhQk7p"}, "review_info": []}, "p4B7rl1UFA": {"paper_info": {"Keywords": "Memory-efficient, Optimization, Large language models", "TL;DR": "A memory-efficient optimization method.", "Abstract": "Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update normalization to stabilize convergence. Our experiments with instruction-tuning and further pre-training demonstrate that AdaLomo achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models.", "Primary Area": "optimization", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9450", "PDF Url": "https://openreview.net/pdf?id=p4B7rl1UFA"}, "review_info": [{"Heading": "Official Review of Submission9450 by Reviewer vpaW", "Subheading": "Official ReviewbyReviewer vpaW02 Nov 2023, 04:53 (modified: 10 Nov 2023, 13:38)EveryoneRevisions", "Content": "Summary:\nThis paper proposes AdaLomo, a low-memory optimization method for large language models that provides an adaptive learning rate for each parameter while maintaining memory efficiency. The key ideas are using non-negative matrix factorization to estimate the second-order moment for the adaptive learning rate, and employing grouped update normalization to stabilize training. Experiments on instruction tuning and further pretraining of LLaMA models show AdaLomo achieves comparable results to AdamW while significantly reducing memory requirements.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nTackles the important challenge of reducing memory footprint for large LM training. The motivation is well articulated.\nEmpirically analyzes differences between SGD, Adam and LOMO highlighting the role of second-order moments. Provides insight.\nAdaLomo integrates sensible ideas - NMF for second-order moment estimation and grouped update normalization - to offer adaptive learning rates with low memory.\nExperiments cover instruction tuning and further pretraining with solid results on par with AdamW and LoRA. Reduced memory requirements demonstrated.\nWeaknesses:\nThe main ideas borrowed from prior work like Adafactor and grouped normalization limit novelty. Contributions appear incremental.\nThe two core components of AdaLomo are the use of non-negative matrix factorization (NMF) for estimating the second-order moment and the grouped update normalization. However, the paper does not contain ablation studies to directly demonstrate the benefits of each component.\nConvergence plots during pretraining could be insightful to compare optimization behavior.\nLack of comparisons to related memory efficient methods like SM3, ZeRO, and 8-bit optimizer.\nQuestions:\nHave you experimented with other ways to estimate second-order moments besides NMF? How did they compare in terms of memory and performance?\nHow sensitive is AdaLomo to the hyperparameters? Any guidelines for settings based on model architecture or task?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9450 by Reviewer 6pvp", "Subheading": "Official ReviewbyReviewer 6pvp01 Nov 2023, 11:40 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposes the AdaLOMO optimizer for training large language models. Specifically, the work 1) takes Adam and removes the second moment buffer, 2) factors the second momentum buffer of each parameter group into a Rank 1 matrix (outer-product of two vectors) and re-scales the second moment update to have a similar magnitude to the inverse RMS of the previous iterations parameters in the case where the updates grow very large. Numerical experiments are provided with LLaMA instruction tuning, and further LLaMA pretraining.\nSoundness:\n1 poor\nPresentation:\n1 poor\nContribution:\n1 poor\nStrengths:\nSignificance\nThis paper studies an important problem, namely how to improve the memory efficiency of optimizing large language models.\nWeaknesses:\nOriginality\nThe proposed method is EXACTLY Algorithm 5 in Shazeer et al., 2018. The only difference is that the gradient of a layer is deallocated once it is used to compute the gradient of the next layer so as to save memory.\nAdafactor does already propose using Adam with a factored second moment. Similarly, they remove the first moment buffer. The RMS normalization is also taken directly from Adafactor, where it is motivated as a strategy for preventing very large updates when using slow decay of the second order buffer in the absence of a first order buffer.\nQuality\nNumerous mathematical issues in terms of clarity + a few minor mathematical errors which are probably typos.\nRMS on page 5 is incorrect, terms inside summation should be squared. However, $u$ is a matrix, so unclear what this computation involves.\nIn equation 9 it should be $v_{t,i} = r_{t,i} c_{t, i}$\nIn equation 10, unclear what $g_{t,i} / v_{t,i}$ involves, since $g_{t,i}$ and $v_{t,i}$ are matrices of size $m \\times n$\nUsed $\\theta_{t,i}$ to refer to a \u201cparameter\u201d but this is described as matrix of size $m \\times n$, should it is not just a single parameter.\nQuestions:\nWhat are the novel contributions of AdaLOMO compared to Adafactor?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n1: strong reject\nConfidence:\n5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9450 by Reviewer t9K6", "Subheading": "Official ReviewbyReviewer t9K627 Oct 2023, 10:49 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper presents the low-memory optimizer with adaptive learning rate (AdaLomo) for training large language models. The optimizer combines LOMO with the first/second-order moment estimations used in adaptive methods such as Adam. Computing the second-order moment estimation uses Non-negative matrix factorization. This paper shows numerically that AdaLomo is comparable with AdamW and reduces memory requirements.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe strength of this paper is to show numerically that the proposed method, AdaLomo, is comparable with AdamW and reduces memory requirements.\nWeaknesses:\nThis paper numerically studies the performance of AdaLomo that can be obtained by combining the existing methods such as LOMO and Adam. It seems that AdaLomo has the best of both LOMO and Adam. This paper lacks theoretical explanations why AdaLomo performs better than the existing optimizers, such as LoRA, AdamW, and LOMO.\nQuestions:\nIn general, I am not very familiar with this topic. Although this paper studies mainly practical numerical comparisons, I think that it needs theoretical explanations and evidences to emphasize the usefulness of AdaLomo. In particular,\nThere have been convergence analyses of Adam. Is it guaranteed theoretically that AdaLomo converges to a (local) minimizer and does not fall into the worst local minimizer (since Figure 1(b) shows that Adam decreases the value of the loss function more than SGD and SGD with momentum)?\nMoreover, I have the following concern.\nThe optimizers use that a batch size $b$ is 128. I do not know why the setting is appropriate. In intuition, the optimizers with a smaller batch size than 128 decrease the value of the loss function. Please compare the performance of using $b=128$ with the one of using smaller/larger batch sizes than $b = 128$. I would also like to check the performances of the optimizers using a cosine annealing step-size and a step-decay step-size. Could you compare the optimizers using constant step-sizes (Table 3) with the ones using cosine annealing step-sizes/step-decay step-sizes?\nTypos and minor comments:\nPage 1, Line -9: identifie $\\to$ identify\nPage 4, Line -14: What is \"Adafactor\"?\nAlgorithm 1, Step 9: $s_{t,i}$ is not defined?\nPage 5, footnote 1: $\\sum_{i=1}^{i=n} \\to \\sum_{i=1}^n$\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9450 by Reviewer VMbV", "Subheading": "Official ReviewbyReviewer VMbV23 Oct 2023, 09:15 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis manuscript aims to provide a low-memory optimizer for LLMs.\nSoundness:\n1 poor\nPresentation:\n2 fair\nContribution:\n1 poor\nStrengths:\nThis manuscript studies a crucial problem in the field.\nThis manuscript considers fine-tuning LLaMA with various model sizes\nWeaknesses:\nThe novelty of the AdaLomo is limited. It is very close to the design of Adafactor, while no performance comparison can be found in Table 2, Figure 2, and Figure 3.\nThe convergence of AdaLomo has no theoretical guarantee.\nSome statements are weak, as no corresponding supports can be found. For example, in the sentence ``Through our ablation study on Adam, we found that its second-order moment estimation has a significantly greater impact on its convergence than the first-order moment estimation'', the ablation study refers to which table/figure?\nSome design choices have no justification. For example, why it is a good idea to consider $u_{t, i} / \\max(1, RMS(u_{t, i})) \\times \\max(\\epsilon, RMS(\\theta_{t-1, i}))$ (though it is borrowed from AdaFactor)? Compared to other gradient clipping ideas, and gradient normalization techniques, why current design choice is a better idea? Some theoretical justifications or empirical evidence should be provided.\nLoRA is a parameter-efficient fine-tuning method, while AdamW and AdaLoMO are optimizers. It looks unfair to directly compare LoRA with AdamW and AdaLoMO as they are orthogonal.\nSome other baseline optimizers need to be considered. e.g., Lion [1] and Adan [2]; or even some other memory-efficient techniques should be evaluated with any other adaptive optimizers.\nReference\n[1] Symbolic Discovery of Optimization Algorithms\n[2] Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models\nQuestions:\nWhat is the exact definition of ``SGD with variance''?\nWhat is the choice of optimizer used for LoRA?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}]}, "cQgjz0mf0r": {"paper_info": {"Primary Area": "visualization or interpretation of learned representations", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Double Descent, Partition Density, Linear Regions, Local Complexity", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "TL;DR": "We present a novel method to measure the local complexity of Deep Networks, and show that it exhibits a double descent phenomenon", "Abstract": "The study of Deep Network (DN) training dynamics has largely focused on the dynamics of the loss function, evaluated on or around train and test set samples. In fact, many DN phenomenon were first introduced in literature with respect to the loss or accuracy dynamics during training, e.g., double descent, grokking. No other statistics about the DN has been found to be as informative as the loss function. In this study, we provide a novel statistic that measures the underlying DN\u2019s local complexity, exhibiting two key benefits: (i) it does not require any labels, and (ii) it is informative about the training loss and accuracy dynamics. Our proposed statistic is based on the concentration of partition regions around samples \u2013which encompasses the local expressivity or complexity of a DN\u2013 and can be applied on arbitrary architectures, e.g. CNNs, VGGs and Resnets. We show that our statistic exhibits a double descent phenomenon during training, with the partition density first decreasing around training samples, then increasing (ascent), followed by an other descent during which neurons migrate towards the decision boundaries. We see this phenomenon happening for a number of different experimental setups, e.g., training with label noise, delayed generalization, i.e., grokking. Our observations provide a novel lens to study DN training dynamics from a spline theory perspective.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9446", "PDF Url": "https://openreview.net/pdf?id=cQgjz0mf0r"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9446 by Area Chair RGQm", "Subheading": "Meta ReviewbyArea Chair RGQm14 Dec 2023, 02:13 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThis paper proposes a new complexity measure of deep (ReLU) network that can be measured during the training. Through numerical experiments, they examined the proposed complexity measure to see how it is correlated to important phenomena (grokking, double-descent, memorization) throughout the training.\nUnfortunately, this paper contains several issues.\n(1) First, the paper's contribution significantly overlaps with existing works, especially with SplineCam. In that sense, the theoretical contributions are not sufficiently novel. On the other hand, the numerical experiments are not strong and solid to locate this paper as an empirical verification of existing work.\n(2) Writing can be much improved. The methodology is not well explained, and the experimental results are not exposed in well organized way. It is difficult to see what is the main contribution of this paper and what is the new insight obtained by the analyzes.\n(3) Some notations are not defined. For example, the quantity $r$ is not defined although it is a critical quantity in the experiments.\nFor these reasons, I cannot recommend acceptance for this paper.\nJustification For Why Not Higher Score:\nAs I mentioned in metareview, this paper has less novelty and its writing should be much improved. It cannot be accepted unfortunately.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "Official Review of Submission9446 by Reviewer 4pKG", "Subheading": "Official ReviewbyReviewer 4pKG03 Nov 2023, 14:20 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe authors propose a method to measure the complexity of deep neural network classifiers. In particular, the measure approximates around any given input the number of convex regions that the ReLU activation induces in the input space. Empirical results demonstrate the behavior of the proposed measure in several settings of deep classifiers.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe proposed measure seems to be useful for understanding the local complexity of the classifier while being simple and general enough to be used under different settings.\nMany empirical results about the behavior of the measure in different settings.\nWeaknesses:\nI think that the writing of the paper can be improved, as in many parts it was quite hard to understand the actual information. Also, in some figures, some extra information is necessary, for example, in Fig. 6 what is the difference in each panel?\nEven if the proposed measure seems to capture the local complexity of the classifier, apart from some connections to related works, I think that a thorough analysis is missing.\nQuestions:\nI think that the proposed measure is an interesting and simple approach to approximate the number of convex regions (in some sense the complexity) of the classifier around the training data. However, I do not understand how it should be used and how it helps to analyze the actual behavior of the deep classifier.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Author Response Part 1", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:30 (modified: 23 Nov 2023, 06:35)EveryoneRevisions", "Content": "Comment:\nWe thank the reviewer for their positive comments, especially on the simplicity of our approximation method and our empirical experiments. Below we respond to individual points raised by the reviewer.\nI think that the writing of the paper can be improved, as in many parts it was quite hard to understand the actual information.\nWe apologize for the lack of clarity especially Sec. 3.1 and Sec. 4.1 which are integral to understanding how the proposed method works.\nTo summarize, in this paper we provide a method to approximate the local complexity (LC) of DNNs and show that for a wide range of training settings, the local complexity exhibits an epoch-wise double descent phenomenon. To compute local complexity around any given point $x$ in the input space, we perform the following steps.\nFirst we start by sampling $P$ orthonormal vectors $B=${$ v_p : p=1...P$}.\nWe use $B$ to denote vertices of a local neighborhood $V$ around any data point $x$ s.t. $V=${$x \\pm rv_p : p = 1\u2026P$} where $r$ is a radius parameter. $x$ is therefore the centroid of $V$ consisting of 2P vertices each $r$ distance away from $x$. We define the convex hull $conv(V)$ as the local neighborhood of $x$ in the input space. The dimensionality and volume of the neighborhood can be controlled via $P$ and $r$. Each neighborhood can be considered a non-axis aligned unit $\\ell_1$-norm ball scaled by radius $r$.\nGiven an input space neighborhood $conv(V)$, we embed $V$ to the input space of layers $\\ell$ of the network which precede a non-linearity. We denote the embedded vertices in the input space of layer $\\ell$ as $V^\\ell$.\nFor a given embedded neighborhood $conv(V^\\ell)$, we check if there is a change in the pre-activation sign of any given neuron between the vertices. If there is, the neuron (hyperplane) intersects $conv(V^\\ell)$. We repeat this for all neurons in all layers and count the total number of intersections as the local complexity.\nSome extra information is necessary, for example, in Fig. 6 what is the difference in each panel\nWe apologize for the lack of clarity in Fig 6. From left to right each panel presents the local complexity for train, test and random points. For training LC, the LC during the ascent phase changes from low to high to low again while the number of parameters of the network is increase. We do not see such a phenomenon for LC around the test or random points.\nEven if the proposed measure seems to capture the local complexity of the classifier, apart from some connections to related works, I think that a thorough analysis is missing.\nIn appendix, we have added empirical results analyzing how the parameters of neighborhood $conv(V)$, namely radius $r$ and dimensionality $P$ affects LC computation. In summary, we provide the following observations:\nAs we increase P, after a certain point, there is marginal increase in LC with increasing P for the same input space point $x$. In Fig.16 left we show that for a 784 dimensional input space and a randomly initialized MLP with width 100 and depth 18, $P=42$ has marginal difference in LC compared to $P=382$.\nAs we increase radius $r$, the volume of the neighborhood $conv(V)$ increases. While higher neighborhood volume allows computing LC at coarser scales (Fig.16 right), it can also introduce distortion in the embedded neighborhood reducing approximation quality. In this regard, we compute the eccentricity and diameter of the graph formed via $V^\\ell$ which is the layer $\\ell-1$ embeddings of vertices $V$.  We use $1000$ training points from CIFAR10 to define the neighborhoods for a trained CNN. We see that for larger $r$ both of the deformation metrics exponentially increase, whereas for $r \\leq 0.014$ the deformation is lower and more stable. This shows that for lower $r$ our LC approximation for deeper CNN networks is stable since the neighborhood does not get deformed significantly."}, {"Heading": "Author Response Part 2", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:52Everyone", "Content": "Comment:\nI do not understand how it should be used and how it helps to analyze the actual behavior of the deep classifier\nWhile previous work has clearly motivated the need for methods to compute the local complexity of DNNs, exact computation of complexity measures, e.g., the number of linear regions, has combinatorial complexity and is intractable for large deep networks with arbitrary dimensional input spaces. The following are ways in which our results help understand neural networks:\nOur method can be used to perform fast approximation of the DNN local complexity and can be scaled up to Imagenet experiments as well. In appendix we have presented the LC computed during Resnet18 training on Imagenet upto a top 1 accuracy of 69%. We can also use LC to compare between trained models, compare between sub-groups within the training/test set to quantify bias.\nUsing our method we have presented a novel observation that based on the phase of training, the local complexity around training points might be increasing or decreasing with training epochs. To the best of our knowledge this is the first observation of this kind. This raises new questions regarding our current theoretical understanding of DNN training dynamics, opening new avenues for exploration.\nOur results indicate that grokking could be a result of 'partition migration' - a phenomenon that we have presented where during the ending stages of training, layerwise neurons (hyperplanes) start orienting themselves with each other and grouping together.\nIn supplementary materials, we present an animation showing the partition migration phenomenon during training, as was shown in Fig.1. Note that while the visualizations are produced via earlier work on partition visualization, our proposed method allows monitoring the partitions at scale during training, which can be used to probe the input space to find where to look. Partition migration can be used to explain the neural collapse phenomenon in DNN training. Because partitions as well as neurons (black lines in Fig.1) start grouping together, loosing linear independence and causing neural collapse."}]}, {"Heading": "Official Review of Submission9446 by Reviewer n8fX", "Subheading": "Official ReviewbyReviewer n8fX01 Nov 2023, 23:41 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis work builds upon the Max Max/Affine Spline line of works from Balastriero/Baranuik. It shows that the partitions derived from those ideas can be used to calculate summaries that exhibit double descent phenomena during training dynamics. Experimental evaluations are presented on CNNs on MNIST, CIFAR type datasets. Grokking and batch normalization effects are also discussed.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThe overall rationale of the work is clear. Deriving a better characterization of the training dynamics beyond inspection of the loss is generally valuable. The paper shows that repurposing the affine spline formulation does show a double descent behavior.\nThe experiments are satisfactory:  a number of different architectures and datasets are used to demonstrate that the local partition complexity summary is useful.\nOn the practical side, some of the heuristics proposed here make splinecam calculations faster and GPU friendly.\nWeaknesses:\nWhile I appreciate the main double descent observation and some of the experimental findings supporting it, I find the extent of daylight between this work and some of the published results a weakness. The paper does acknowledge clearly that the development will rely on the affine spline. This is not a problem, but except the double descent observation, much of the development described here veers too close to the splinecam paper at CVPR 2023. It is not obvious whether to view this paper as an empirical evaluation of the splinecam work in the context of double descent. Even several figures from that work appear in this submission directly.\nIf the technical adjustments and modifications to the splinecam work are significant, it would be desirable to identify this more explicitly in the text to help the readers assess the distinction clearly.\nIn the absence of new theoretical findings, the paper would also be better served by a deeper analysis of some of the settings mentioned in passing in the Introduction (e.g., self-supervised). What does the proposed characterization of training dynamics reveal in that setting?\nQuestions:\nCan this work be viewed as applying splinecam with some moderate adjustments, and deriving a summary to check double descent behavior?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9446 by Reviewer prSj", "Subheading": "Official ReviewbyReviewer prSj29 Oct 2023, 15:19 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe authors propose an approximation for computing continuous piecewise affine operators in deep (ReLU, I believe) networks and then study how the (approximate) partitions evolve during model overtraining.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nOverall, I think this paper is well written.\nWeaknesses:\nOverall, I think that this paper has a number of significant shortcomings. To summarize, I think\nthe motivation is weak\nthe authors dodge the hard central research question of how to make the continuous piecewise affine (CPA) operator perspective useful at scale\nthe method is not well explained and is based on an approximation that, to the best of my ability to discern, goes untested\nthe experiments are too simple (e.g, 1k samples from MNIST) or odd (e.g., training MNIST for 100K training steps, which is massive overkill, or changing how the MNIST targets are encoded to induce grokking)\nthe experimental results are messy and at times unclear, with dubious connection to double descent\nIn detail:\nMotivation\nNo other statistics about the DN has been found to be as informative as the loss func-\ntion\nI think this claim is untrue. My understanding is that there is work in the deep kernel literature in both lazy learning and feature learning regimes, e.g., [1, 2] and mechanistic interpretability literature, e.g., [3, 4] about studying DN learning dynamics using more granular statistics than the loss function.\nIf the authors do wish to advocate for a CPA perspective, which is valid, they should make a case of (1) what properties we want deep network learning statistics to reveal, (2) why other existing approaches are better and (3) why CPA is the right way to go. But I don't see this, and thus I am left wondering \"Sure, we\ncan\ntake a CPA approach, but what are we looking for that we don't already have and that CPA can provide?\"\n[1] Bordelon, Canatar & Pehlevan ICML 2022. Spectrum Dependent Learning Curves in Kernel Regression and Wide Neural Networks.\n[2] Bordelon & Pehlevan NeurIPS 2022. Self-Consistent Dynamical Field Theory of Kernel Evolution in Wide Neural Networks.\n[3] Nanda, et al. ICLR 2023. Progress measures for grokking via mechanistic interpretability.\n[4] Liu, Kitouni, Nolte, Michaud, Tegmark, Williams. NeurIPS 2022. Towards Understanding Grokking: An Effective Theory of Representation Learning\nIn this study, we provide a novel statistic that measures the underlying DN\u2019s\nlocal complexity, exhibiting two key benefits: [...] (ii) it is informative about the training loss and accuracy dynamics.\nPrevious work gave loss error bounds as a function of the number of non-overlapping regions in continuous piecewise affine operators in DN [1] and studied their performance empirically, so this manuscript\u2019s second main claim (that changing local complexity results in changing loss and changing accuracy) seems unsurprising.\nAt a minimum, I would recommend the authors cite [1]. I also happened to attend a talk by the first author of [1] and Professor Shai Ben-David in the audience said that the authors of [1] were ignoring significant amounts of prior work from the 80s-2000s; while I myself don\u2019t know the correct citations, I suspect that the authors of this manuscript might also want to do an older literature search. (Note: I am unaffiliated with [1] and with Professor Ben-David.)\nI should also note that this manuscript\u2019s Fig 2 almost exactly matches [1]\u2019s Fig 1b.\n[1] Ji, Pascanu, Hjelm, Lakshminarayanan, Vedaldi CoLLAs 2022. Test Sample Accuracy Scales with Training Sample Density in Neural Networks.\nIn fact, our development will heavily rely on the affine spline formulation of DNs\nBalestriero & Baraniuk (2018). Such formulation is exact for a wide class of DNs which naturally\narise [...]  nonlinearities such as max-pooling, (leaky-)ReLU.\nI might be mistaken, but does the CPA perspective apply to more modern nonlinearities? Balestriero and Baraniuk 2018 studied ReLU, Leaky ReLu and absolute value, but many modern models use newer nonlinearities like GeLU and SwiGLU, used in ViT and LLMs e.g. Llama 2. I guess my question is: to what extent is CPA applicable only to certain classes of nonlinearities?\nMethod\nMoving to deep layers involve a recursive subdivision Balestriero et al. (2019) that goes beyond the scope of our study.\nMy understanding is the key reason why the CPA perspective has struggled is because of the combinatorial complexity. Waving this aside amounts to waving aside the key research challenge with CPAs.\nEquation 3\nWhat is the notation $\\partial \\Omega$? Is equation 3 a definition?\nThe key insight we will leverage however is that the number of sign changes in the layer\u2019s pre-activations is a proxy for counting the number of regions in the partition. To see that, notice how input space samples who share the exact same pre-activation sign effectively lie within\nthe same region \u03c9 \u2208 \u03a9 and thus the DN will be the same affine mapping (since none of the activation functions vary). Therefore for a single layer, the local complexity for a sample in the input space can be approximated by the number of hyperplanes that intersect a given neighborhood V which is itself measure by the number of sign changes in the pre-activations that occur within that neighborhood.\nI think this is a key point that should be explained in more detail, with examples and/or figures and/or equations.\nAlso, doubling back to my question, I\u2019m unsure of whether the insight still holds with depth. One would imagine that multiple regions at layer $i$ might be mapped to the same region at layer $i+1$ or $i+2$.\nNote that for deeper layers, our complexity measure is actually computing the number of hyperplane intersections with the convex hull of the embedded vertices instead of the convex hull of the vertices in the input space.\nThis comment is somewhat concerning. This comment suggests that the quantity one desires (the convex hull of the vertices in input space) has subtly shifted to a different quantity (the convex hull of the embedded vertices) and how these two are related (if at all) is not analyzed, discussed or explored mathematically or empirically.\nWe control the size of the local neighborhood by controlling the diagonal lengths via the r parameter mentioned in section 3.2.\nI cannot identify an $r$ parameter in Section 3.2, and this $r$ parameter seems to be important for subsequent experiments.\nExperiments\nBefore delving into the experiment, the proposed approximation (Section 3.1 & 3.2) is not yet validated. One would first want to know: how well does this method approximate the full (and extremely expensive) partition computation? How many orthogonal vectors are necessary to get a good approximation, and how does this depend on the data, architecture, and other implementation choices? What effect does the parameter $r$ have? As best as I can tell, there is no answer to these questions.\nThe closest answer I can find is Section 3.3, which compares this method against SplineCAM in 1 architecture (depth 3 width 200 MLP) on 500 MNIST samples. This feels inadequate to me for two reasons:\nI want a comparison to an exact baseline. But whether SplineCAM is exact is unclear. The authors call SplineCAM both an approximation (\u201cwith the local partition density approximated via SplineCAM\u201d) and exact (\u201cSplineCAM is an exact computation method\u201d). SplineCAM also appears to be limited (\u201cSplineCAM can only be used to compute the local complexity for 2D neighborhoods\u201d), which an exact method should not be.\nEven if SplineCAM is exact, I see no characterization of how the number of orthogonal dimensions, or $r$, or architecture, or dataset plays a role.\nConsequently, I am left with no understanding of how good this approximation is or under what conditions the approximation is good. This makes me skeptical of the subsequent experimental results.\nWe perform experiments on MNIST with fully connected DNs and on CIFAR10 with CNN and Resnet architectures\nDespite this claim, it appears to me that _most) of the experiments focus on a subset of 1k data from MNIST (Figures 1, 3, 6, 7, 8, 9, 10). The CIFAR experiments, while fewer and less prominent, are performed on 2.5k data. In 2023, such small networks trained on such simple data feels too inadequate. Moreover, the training diets seem bizarre. Most of the models appear to display high plateau accuracy within ~100 optimization optimization steps, but training continues till 100000 training steps without motivation or explanation. Models are known to exhibit strange behaviors with such long training diets (e.g., neural collapse) so I am skeptical that we should be studying networks in such late atypical stages. I personally have never seen anyone train MNIST to 100k gradient steps unless they're looking for atypical model behavior.\nFor the MNIST and CIFAR10 experiments with fully connected and convolutional networks, we use p = 25 as the dimensionality of the hypercubes that define the neighborhood.\nI don\u2019t know why p=25 was chosen, and it would be good to see some sensitivity analysis to how much $p$ affects the results.\nAs depicted in the top-right figure, the amount of region near training samples follow a double descent curve ultimately falling until training is stopped\nThere are two oddities with the Figure 1 result.\nDouble descent is almost always observed as a function of parameters to data, not optimization steps.\nThe\ntrain\nloss peaks, even though double descent is characterized by the\ntest\nloss peaking.\nNow, perhaps the authors do not mean \u201cdouble descent\u201d in the way that is commonly used. If so, I would urge them to avoid such terminology. But if not, this needs clarification.\nFor all the depths, the accuracy on both the train and test sets peak during the first descent phase.\nGiven that train and test accuracy plateau seemingly within ~10 gradient steps, I am confused why the models are trained to 100,000 gradient steps. Moreover, most fluctuations and interesting patterns of LC occur beyond ~100 gradient steps (typically past the point that accuracy has saturated), leading me to be skeptical why we are looking at such extremely late training dynamics.\nFigure 3:\nWhy is the training LNL so spikey but the test LNL so smooth?\nFigure 4: Effect of Batch Normalization on Double Descent\nI see no divergence of the test loss near an interpolation threshold. What does this have to do with double descent? And again, double descent is a phenomenon typically described with data to parameters, not gradient steps.\nMinor:\n\u201c As such, as pose the following question,\u201d\nAdd a subject to this sentence e.g.,  \u201cAs such, we pose the following question,\u201d\nwhich naturally arise form the\n\u201cform\u201d should be \u201cfrom\u201d\nQuestions:\nMost of my equations are integrated into Weaknesses. To extract a few:\nIs Equation 3 a definition? What is the $\\partial \\Omega$ notation?\nWhat is the parameter $r$?\nIs SplineCAM an exact method? If so, why does it only work in 2D?\nBroadly, what do you mean by double descent?\nFlag For Ethics Review:\nNo ethics review needed.\nDetails Of Ethics Concerns:\nN/A\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9446 by Reviewer dy7R", "Subheading": "Official ReviewbyReviewer dy7R28 Oct 2023, 21:50 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposes a new measure of the local complexity of functions that deep networks (abbrv. DN) learn throughout training.  The authors develop this measure as a follow up to work establishing a large class of DNs as a composition of max-affine spline operators, which distinguishes their model of DNs from those that uses information theoretic tools.  They use their measure of complexity to examine different phenomena exhibited by DNs during training (grokking, double descent, memorization), and explain them in light of their local complexity formalism.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nThis paper is well written, and builds solidly on prior work by\nBalestriero et al.\n.\nThe motivation of the theory is well developed in section 3.1\nThe experiments are well motivated also, and address some of the most relevant phenomena observed in DN training dynamics.\nMany other papers that attempt to explore local representations of deep network points usually rely on a trust region about a point, and make assumptions about the geometry of a latent representation space.  It's interesting to me that the authors instead use a convex hull to construct the neighbourhood, though they might want to spend a bit more time motivating this choice (if it's the best choice given their\nearlier work\n., another citation would not go amiss here).\nWeaknesses:\nThe biggest weakness in the paper seems to be that a critical part of it is missing.  The experimental setup in section 4.1 states\nWe control the size of the local neighborhood by controlling the diagonal lengths via the $r$ parameter mentioned in section 3.2\nBut section 3.2 makes no mention of a parameter $r$, and it is difficult to make a full evaluation of the experiments in section 4 absent a definition of $r$.\nThere is a line of prior work that seems closely related to the authors' own proposed work, which is the idea of polytope lenses by\nBlack et al.\n.  Briefly, this work attempts to look at polysemy of neurons by using ideas from casting neural networks as compositions of splines, though they are chiefly interested in interpretation rather than complexity.  I think this paper would be made stronger by contrasting the ideas in Black et al to this work's own, by showing the benefit of viewing DNs as splines has multiple uses.\nThe lack of a need for labels makes this measure ripe for helping to help explain the effects of self-supervised learning.  I'm a bit surprised that the authors don't include experiments for why (e.g) BYOL can work by measuring the local complexity of teacher & student networks as they train, and iteratively replace each other.\nThis is a minor point, but it would be helpful for the authors to spend a few words about why the cost of computing the $P$ orthonormal basis vectors for each $x$ either isn't prohibitive, or why it might be in certain cases.\nRe-reading section 3.1, I'm surprised to see no characterization of the properties of $[\\mathbf{v}_1, \\dots, \\mathbf{v}_P]$ other than they form an orthonormal set, and that $x$ is within $conv(\\mathbf{V})$.  This latter point is implied but not stated.  Maybe 3.1 could do with some more care in specifying the relation between $\\mathbf{x}$ and $\\mathbf{V}$?\nOther minor points:\nitalicized sentence at the end of the second paragraph of the introduction has some typos.  But it's compelling!\nIf this sampling procedure is for each sample input, it\u2019s going to become really expensive computationally.  How do they compute a succession of P orthonormal vectors, is this by Gram-Schmidt (or similar procedure?). Is there any way to amortize this complexity?  I think it would help make this measure more practically helpful\nQuestions:\nHow stable is local complexity about a point $\\mathbf{x}$ as a consequence of the choice of $\\mathbf{V}$?  And choice of $\\mathbf{P}\"?  These seem like important questions to answer.\nThe authors intentionally leave out extension of this to deeper networks, as a clear continuation of this line of work.  But I think the conclusion would be strengthened by spending some more of the word budget to help lead the reader into how the authors plan to do so.\nDo the authors have any conjecture about how the partition migrating dynamics relate to the phenomenon of neural collapse?\nTo conclude, I think this is a good paper that needs a bit more work and polishing to become a really good paper.  If the authors will consider the points I've raised here (and in the weaknesses section), I'm willing to raise my score.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Author Response Part 1", "Subheading": "Official CommentbyAuthors23 Nov 2023, 05:50 (modified: 23 Nov 2023, 05:51)EveryoneRevisions", "Content": "Comment:\nWe thank the reviewer for their positive comments, especially their appreciation of the motivation behind our work.\nThe biggest weakness in the paper seems to be that a critical part of it is missing. The experimental setup in section 4.1 states\nWe thank the reviewer for raising this very important point. We have re-written section 4.1 and 3.1 adding the definition of the radius $r$ and its relationship with $conv(V)$. In the appendix, we have also added experiments showing the effect of varying $r$ and number of orthonormal vectors $P$.\nTo summarize, we start by sampling $P$ orthonormal vectors $B=${$ v_p : p=1...P$} and use them to denote vertices of a local neighborhood $V$ around any data point $x$ s.t. $V=${$x \\pm rv_p : p = 1\u2026P$} where $r$ is the distance from $x$ at which we choose vertices defining the neighborhood. Therefore, $x$ is the centroid of $V$ consisting of 2P vertices. We define the convex hull $conv(V)$ as the local neighborhood of $x$ in the input space. The dimensionality and volume of the neighborhood can be controlled via $P$ and $r$. Each neighborhood in the input space around any data point $x$ is a cross-polytope or the dual of a hypercube with dimensionality P and diagonal length $2r$. Each neighborhood can also be considered a unit $\\ell_1$-norm ball scaled by radius $r$.\nThere is a line of prior work that seems closely related to the authors' own proposed work, which is the idea of polytope lenses by Black et al.\nWe thank the reviewer for bringing the paper to our attention. Indeed the analysis by Black et al. is very relevant to the results of our paper. While they are focusing on interpretability, the authors provide experiments showing that the density of polytope boundaries are related to semantic boundaries. In our paper, while we compute local complexity in terms of layerwise hyperplane intersections for a given neighborhood, the number of polytope edges formed via an arrangement of $N$ different $(d-1)$-hyperplanes is of the order $N^d$. Therefore, our proposed local complexity would be highly correlated with polytope boundary density. We will add a short discussion contrasting our method with Black et al. in our conclusion and connect it with potential future directions on interpretability.\nI'm a bit surprised that the authors don't include experiments for why (e.g) BYOL can work by ...\nWe thank the reviewer for raising this excellent point. Indeed, evaluating BYOL especially the alignment between the teacher and student network, can be assessed in a sample-wise manner using our local complexity metric. In this paper however, we have mostly focused on the double descent phenomenon of local complexity that we observe in a wide variety of settings, while providing a method to do fast approximation of local complexity. The double descent phenomenon is important since it shows that based on the phase of training, neural networks tend to increase or decrease local complexity around training points, which opens up avenues for new theoretical studies on neural network training and generalization. We leave the SSL experiments for future work and will add a note on that in the conclusion.\n...about why the cost of computing the orthonormal basis vectors for each either isn't prohibitive...\nIf this sampling procedure is for each sample input, it\u2019s going to become really expensive computationally.\nIn our experiments, we have seen no significant difference between sampling the orthonormal basis vectors for every data point $x$, vs using the same orthonormal frame being used for all the data points. Using the Pytorch implementation of sampling P orthonormal vectors, finding P=25 vectors takes approximately 9.49 ms, which needs to be done only once at the beginning of our experiments. We have added these details in the paper.\nRe-reading section 3.1, I'm surprised to see no characterization of the properties of\nWe apologize for the lack of clarity, we are rewriting Sec 3.1 and clarifying the notations as discussed above."}, {"Heading": "Author Response Part 2", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:00Everyone", "Content": "Comment:\nHow stable is local complexity about a point $x$ as a consequence of the choice of $V$?\nIn appendix, we have added experiments exploring how the parameters of neighborhood $conv(V)$, namely radius $r$ and dimensionality $P$ affects LC computation. In summary, we provide the following observations:\nAs we increase P, after a certain point, there is marginal increase in LC with increasing P for the same input space point $x$. In Fig.16 left we show that for a 784 dimensional input space and a randomly initialized MLP with width 100 and depth 18, $P=42$ has marginal difference in LC compared to $P=382$.\nAs we increase radius $r$, the volume of the neighborhood $conv(V)$ increases. While higher neighborhood volume allows computing LC at coarser scales (Fig.16 right), it can also introduce distortion in the embedded neighborhood reducing approximation quality. In this regard, we compute the eccentricity and diameter of the graph formed via $V^\\ell$ which is the layer $\\ell-1$ embeddings of vertices $V$.  We use $1000$ training points from CIFAR10 to define the neighborhoods for a trained CNN. We see that for larger $r$ both of the deformation metrics exponentially increase, whereas for $r \\leq 0.014$ the deformation is lower and more stable. This shows that for lower $r$ our LC approximation for deeper CNN networks is stable since the neighborhood does not get deformed significantly.\nextension of this to deeper networks\nIn appendix, we have added an experiment computing the LC while training a Resnet18 on Imagenet upto 69% top-1 accuracy. Computing LC for 1000 imagenet samples require ~28s on a Quadro RTX 8000 GPU. We use a hull dimensionality of $P=25$ and radius $r=0.001$. We have also previously presented, local complexity dynamics for Resnet18 trained on CIFAR10, and shown that without Batch Normalization, it exhibits similar dynamics as we have seen in CNNs and MLPs.\nDo the authors have any conjecture about how the partition migrating dynamics relate to the phenomenon of neural collapse?\nWe thank the reviewer for this excellent point. In Fig.1, we can see that as training progresses, the neurons (denoted by black lines) start orienting themselves with each other and grouping together. We call this phenomenon partition migration. This can be further seen in the animation we have provided as supplementary material, which presents an exact visualization of the dynamics presented in Fig.1 as training progresses. The grouping of neurons is equivalent to the neurons losing linear independence, which is directly related to neural collapse. There are two additional observations in our result that to the best of our knowledge have not been explored previously in Neural Collapse literature. The first, neurons from different layers of the network may \u2018collapse\u2019 together, meaning that neural collapse is not a layerwise phenomenon. Secondly, as can be seen in the supplementary animation, the grouped neurons can jump between different orientations while staying grouped together, indicating that a network may keep changing its features locally while in a collapsed form. We will add further discussions on this point taking into account space limitations."}]}]}, "i8PjQT3Uig": {"paper_info": {"Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "model-based rl, online learning, incremental learning, catastrophic forgetting", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "Acquiring an accurate world model $\\textit{online}$ for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even with very high dimensional nonlinear features. We validate the representation power of our encoding and verify that it allows efficient online learning under data covariate shift. We also show, in the Dyna MBRL setting, that our world models learned online using a $\\textit{single pass}$ of trajectory data either surpass or match the performance of deep world models trained with replay and other continual learning methods.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Primary Area": "reinforcement learning", "Submission Number": "9441", "PDF Url": "https://openreview.net/pdf?id=i8PjQT3Uig"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:53 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nAccept (poster)"}, {"Heading": "Meta Review of Submission9441 by Area Chair GF5s", "Subheading": "Meta ReviewbyArea Chair GF5s12 Dec 2023, 00:34 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThis paper develops a model-based reinforcement learning (MBRL) method. It seeks to develop a method for learning world model with efficient incremental updates. Specifically, the world model is a linear regression supported by nonlinear random features (based on locality sensitive encoding that is sparse in nature). All the reviewers agree that the paper addresses an important problem in MBRL, and that the work makes an interesting contribution. The reviewers also agree that the concerns raised during review have been sufficiently addressed. The authors are encouraged to take all the feedback into account when revising their paper for final publication.\nJustification For Why Not Higher Score:\nThere are a few weak side of the paper such as not comparing to the state-of-the-art baselines. Although it may not be the major issue for the paper, it still makes it less strong for a higher tier such as spotlight or oral.\nJustification For Why Not Lower Score:\nThis paper makes meaningful contribution to an important problem in MBRL problem. It is worth publication in the venue."}, {"Heading": "Official Review of Submission9441 by Reviewer 2Y3F", "Subheading": "Official ReviewbyReviewer 2Y3F31 Oct 2023, 17:42 (modified: 20 Nov 2023, 20:44)EveryoneRevisions", "Content": "Summary:\nThe paper introduces a model-based reinforcement learning approach that utilizes a sparse representation-to-representation model. The use of sparse representation aims to address the challenge of catastrophic forgetting in a reinforcement learning (RL) setting, where data generation constantly shifts. The architecture employed for model-based reinforcement learning is Dyna. The proposed method involves learning nonlinear sparse features and building a model based on this sparse representation. To enhance computational efficiency, a method for updating model weights using sparse representation is presented. Empirical experiments are included to demonstrate the effectiveness of this approach.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThe paper targets an important topic of learning a model in RL\nI do not see many related works of building sparse representation-based models.\nWeaknesses:\nI will list below main weaknesses for improvements, centred around the main contribution of the paper.\nIs there any reason for why the particular sparse representation learning method is chosen? Furthermore, in the experiments part, FTA should also be compared as a baseline. It is unclear why you compare it in a supervised learning setting but omit in a RL setting. The performance on a SL setting does not invlidate/validate another. As an empirical paper, I think a rigorous comparison is necessary.\nCould you clarify do you update both your model and representation every environment time step?\nThere is a critical weakness in the paper: the paper claims to develop a sparse representation-based approach for model learning, but it is not justified the reported benefits come from the use of the sparse representation for policy learning or for model learning. Note that the former has been extensive studied. in general, a full replay method should be the best in mitigating catastrophic forgetting, but the empirical results reported that the proposed algorithm can sometimes even outperform full replay. That raises a natural question that the benefit mainly comes from the policy learning part by using sparse representation, rather than the proposed model learning part.\nother issues.\nAlg 1. it is better to be specific, use the title Dyna architecture, rather than MBRL, as there are numerous MBRL algorithms and not everyone is as Alg 1 described.\nAlg 2, line 4 & 5: shouldn\u2019t it be outer product? Please specify the dimension of the matrices capital Phi and letter phi. This is nontrivial as it affects the understanding of the algorithm.\nThe term \u201cworld model\u201d might intrigue the readers to see much more challenging tasks than the paper presented, this can be seen by other papers using such terms. It is better to rephrase it to be more precise.\nQuestions:\nsee above.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer 2Y3F (1/2)", "Subheading": "Official CommentbyAuthors15 Nov 2023, 09:19 (modified: 15 Nov 2023, 09:23)EveryoneRevisions", "Content": "Comment:\nThank you very much for recognizing that the problem we attempt to tackle (learning models online) is important in RL, as well as your constructive feedback and suggestions. We have updated the paper with text changes highlighted in yellow, and elaborate on each of the questions below. Note that the points below do not match those in the original questions due to reorganization, but they cover all questions.\n1. Why is the particular sparse representation chosen?\nWe adopt linear models to have guaranteed online learning performance without forgetting. However, linear models cannot fit complex functions. Adding a nonlinear random feature turns linear models into universal approximators [1], but this usually requires a very wide layer to have good capacity. We make our high-dimensional representation sparse to achieve efficient incremental update. In addition, we also compare our sparse representation with other encoding methods and we show favorable results (Table 1). The idea of our sparse update is inspired by the model mixer in [2].\nWe have updated the paper on page 6 with a \u201cconcluding remark\u201d to make this point clearer.\n2. Why is FTA not included in the experiments?\nFuzzy Tiling Activation (FTA) [3] proposes an effective way to reduce catastrophic forgetting of neural networks in online continual learning problems, by learning sparse activations inside the NNs. In Section 4.2, we only borrow their nonstationary online learning setting (the Piecewise Random Walk) and show our method is more capable than NNs of learning temporally correlated data. However, we did not include experiments to compare FTA and our method in either SL or RL settings, for the reasons below.\nIn SL, from Figure 3.c of [3], we can see FTA slightly suffers from nonstationarity if the data is highly correlated. The reason could be that FTA only mitigates the catastrophic forgetting issue in NNs, but it does not fully solve the problem. Therefore, we explore a more assured pathway in this work, by which a Follow-the-Leader solution can be obtained online efficiently, aiming to\neliminate\ncatastrophic forgetting. As Figure 3 in our paper suggests, Losse-FTL can attain very low error for all levels of correlation, showing its non-forgetting property.\nIn RL, we have attempted to integrate the official FTA codes into NN-based world models, but we found it hard to make it work well (perhaps due to hyper-parameter tuning, such as the tiling dimension $k$ and the sparsity factor $\\eta$, or where the FTA layer should be applied). Nevertheless, we compared our method with other representative continual learning techniques, including Synaptic Intelligence and Coreset, to show its effectiveness.\n3. Do you update both your model and representation every environment time step?\nOur high-dimensional sparse representation is not learnable with a prefixed random projection matrix. Thus we only update the model (the linear weights) but not the representation.\nFor all RL experiments, we keep the update interval as $25$ time steps for discrete control tasks and $250$ for continuous control tasks to accelerate the experiments. However, we note that our updates are much more efficient compared to NN-based ones with replay (referring to Figure 5) because the latter needs to train until convergence over accumulated experiences."}, {"Heading": "Response to Reviewer 2Y3F (2/2)", "Subheading": "Official CommentbyAuthors15 Nov 2023, 09:20 (modified: 15 Nov 2023, 09:24)EveryoneRevisions", "Content": "Comment:\n4. Does the benefit come from policy learning or model learning?\nThanks for this insightful question. We agree that sparse representation helps continual policy learning [3,4,5], as it could potentially reduce the catastrophic interference in NNs. However, in this work, we utilize the high-dimensional sparse representation\nonly during model learning\nfor Losse-FTL, while keeping policy/value learning the same for apple-to-apple comparison. Therefore, we believe the benefit indeed comes from model learning; better model learning ensures a better quality of synthetic transitions to be used for policy/value learning.\n5. Why does the proposed algorithm sometimes even outperform the Full-replay baseline?\nWe agree that theoretically, Full-replay should be as good as ours in eliminating forgetting. We hypothesize that the reason for our method outperforming Full-replay in some tasks is due to the criterion we adopt for \u201cconvergence\u201d \u2013 at which point we stop the NN training, while ours is solved incrementally in closed form. Apart from the convergence criterion, there is more brittleness in NN training in this case. For example, not resetting the weights may make NNs overfit early data; but when to reset is hard to know because there are no explicit task boundaries; even if we know when, resetting and training from scratch is even more time-consuming and impractical for lifelong agents.\nWe have updated the paper on page 8 about \u201cFull-replay\u201d to clarify the experimental settings.\n6. Other issues.\nThanks for pointing out the issue of Algorithm 1 and the need for better notations. We have updated them directly in the text. To be more specific on Dyna architecture, we remove the original Algorithm 1 and use a diagram of Dyna in Figure 2 to give a more intuitive illustration in Section 2.2.\nIt also took us some time to consider the term \u201cworld model\u201d in this context. We agree that it can refer to tasks that are more challenging, as envisioned in [6]. We hope to retain it for several reasons. (1) Simply using \u201cmodel\u201d is common in RL contexts, but we feel it might be confusing since \u201cmodel\u201d is frequently referred to with different contexts, e.g. linear model. In the title, especially, without some hints about RL, people may not recognize \u201cmodel\u201d refers to the world model. (2) Other terms like \u201cdynamics model\u201d only describe the dynamics part but not the reward part. We also thought of the \u201cenvironment model\u201d, however, it is not very commonly seen in literature. In the paper, we also tried to put the term \u201cworld model\u201d in the context of MBRL, to avoid misunderstanding. Please let us know if there are more suitable terms.\nThank you again for your detailed feedback and suggestions, and we hope our responses and new experiments were able to address any remaining concerns. Please do let us know if you have any further questions as well as what would be expected for score improvement.\nReferences\n[1] Huang, G. B., Chen, L., & Siew, C. K. (2006). Universal approximation using incremental constructive feedforward networks with random hidden nodes. IEEE Trans. Neural Networks.\n[2] Knoll, B., & de Freitas, N. (2012). A machine learning perspective on predictive coding with PAQ8. In 2012 Data Compression Conference.\n[3] Pan, Y., Banman, K., & White, M. (2021). Fuzzy tiling activations: A simple approach to learning sparse representations online. In ICLR.\n[4] Liu, V., Kumaraswamy, R., Le, L., & White, M. (2019). The utility of sparse representations for control in reinforcement learning. In AAAI.\n[5] Lan, Q., & Mahmood, A. R. (2023). Elephant Neural Networks: Born to Be a Continual Learner. In ICML Workshop on High-dimensional Learning Dynamics.\n[6] LeCun, Y. (2022). A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62."}, {"Heading": "Looking forward to further feedback", "Subheading": "Official CommentbyAuthors20 Nov 2023, 08:18 (modified: 15 Mar 2024, 00:26)EveryoneRevisions", "Content": "Comment:\nDear Reviewer 2Y3F,\nThank you again for your valuable comments and suggestions, which are very helpful to us. We have posted responses to the proposed concerns.\nWe understand that this is quite a busy period, so we sincerely appreciate it if you could take some time to return further feedback on whether our responses resolve your concerns. If there are any other comments, we will try our best to address them.\nBest,\nThe Authors"}, {"Heading": "Official Comment by Reviewer 2Y3F", "Subheading": "Official CommentbyReviewer 2Y3F20 Nov 2023, 20:45 (modified: 15 Mar 2024, 00:26)EveryoneRevisions", "Content": "Comment:\nThank you for your response. I updated my rating."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors22 Nov 2023, 23:22 (modified: 15 Mar 2024, 00:26)EveryoneRevisions", "Content": "Comment:\nThank you very much for the score improvement and your constructive feedback. We will further polish the paper in the final revision. Thank you!"}]}, {"Heading": "Official Review of Submission9441 by Reviewer LJy1", "Subheading": "Official ReviewbyReviewer LJy130 Oct 2023, 01:04 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper presents a method for the online learning of a world model for model-based reinforcement learning (MBRL). To obtain efficient updates to the world model, the world model is expressed as a linear combination of a set of spare features. This efficiency allows online learning at a constant computational cost.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nThe work is well motivated in the introduction and a sufficient and clear background is provided for non-expert readers in the preliminaries section. The algorithms, definitions, etc. are mathematically rigorously presented.\nA comprehensive set of experiments has been conducted demonstrating the efficacy of Losse-FTL\nWeaknesses:\nSignificant discussion around catastrophic forgetting was mentioned in the introduction but little discussion is presented in the main text and left in the appendix.\n\u201cExample 3.1\u201d could be a regular paragraph. Formatting this as an Example does not improve readability and is, in fact, the only Example in the entire paper.\nQuestions:\nIn figure (1): d(s_(t+1), f(st, at)) and \\delta were not defined in the caption or anywhere obvious in the main text.\nIn eq (3), does || . ||^2_F denote the Frobenius matrix norm? It is only stated so after eq (5). It helps to have the notation introduced earlier here. Especially since \u201cF\u201d is the dimension of the feature space\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n8: accept, good paper\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer LJy1", "Subheading": "Official CommentbyAuthors15 Nov 2023, 09:24Everyone", "Content": "Comment:\nThank you very much for highlighting our comprehensive experimental results, as well as your valuable feedback on the paper presentation. We have updated the paper with text changes highlighted in yellow, and elaborate on each of the questions below.\n1. Significant discussion on catastrophic forgetting in the introduction but little discussion in the main text and left in the appendix.\nThanks for this suggestion to improve our paper presentation. We have updated the paper such that catastrophic forgetting is emphasized more in the main text, especially at the end of Section 3 on page 6. Due to the space constraints, however, we still use Appendix A for more detailed discussions.\n2. Other issues regarding the notations.\nWe have updated accordingly in the paper.\nThank you again for your feedback to help us refine the paper quality, and we hope our responses and updates in the text were able to address any remaining concerns."}]}, {"Heading": "Official Review of Submission9441 by Reviewer CrA3", "Subheading": "Official ReviewbyReviewer CrA324 Oct 2023, 09:52 (modified: 18 Nov 2023, 10:13)EveryoneRevisions", "Content": "Summary:\nThis paper presents a world model for model-based reinforcement learning (RL) which can be learned online and does not\nrequire full retraining on all previous data.\nThe authors highlight that training world models is subject to issues arising in continual learning.\nThat is, each sequential data set can be interpreted as coming from a new task and thus the world model needs to be retrained\nafter each agent-environment interaction.\nThis is because the data collected via agent-environment interaction is non-stationary.\nThey propose a world model based on a linear regression model which uses high-dimensional nonlinear features.\nImportantly, the linear model can be updated given new data whilst retaining good predictive performance on old data.\nThey compare their feature encoding method to other feature encoding techniques in an image denoising task on MNIST.\nThey then evaluate their method's ability to handle training data covariate shift in an artificial online learning experiment.\nFinally, they evaluate their method's ability to combat non-stationary data in model-based RL.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n3 good\nStrengths:\nThis paper addresses an important problem in model-based RL, which is likely a problem that must be solved for developing lifelong agents.\nThe method for updating the linear model online is simple but appears to be effective.\nThis is also the first time I have seen a non-trainable encoder used for world models.\nIt is very common to see world models with NN encoders and NN transition models operating in the encoder's latent space.\nTypically the dynamic model operates on a latent state which is lower dimensional than the high-dimensional observations.\nPerhaps I am not aware of the relevant literature, but this seems like an interesting and original idea.\nWeaknesses:\nThis paper has two main weaknesses.\nFirstly, there is no comparison to state-of-the-art MBRL strategies that use world models, e.g. Dreamer/TD-MPC.\nAs such, there is no experiment highlighting the main issue that the paper is trying to address:\nthat NN-based world models suffer from catastrophic interference due to non-stationary data.\nSecond, all of the RL experiments are in simple RL environments.\nFrom the current results, it is impossible to know how practically useful this world model is.\nThere is no discussion about its limitations nor is there a comparison to other model-based RL algorithms that use world models.\nSure the proposed method works on some simple RL environments but can it scale to difficult environments like humanoid and can it handle image-based observations?\nIt is OK if the method cannot do this but it should be addressed in the text.\nMoreover, it should be made clear what benefit it does have over other world model methods (like Dreamer).\nFor example, I'd like to see a state-of-the-art world model method (like Dreamer) performing poorly/failing because it cannot handle non-stationary data.\nI also have questions regarding the training of the NNs in the experiments.\nDid the full-replay experiment involve resetting the neural network's weights? If so, what initialization was used?\nWhen was the NN training stopped? Was the data split into train/validation sets and used to stop training when the\nvalidation loss stopped improving?\nThe paper needs more details to explain exactly how this was implemented.\nIn my experience, these steps are important to ensure the NNs don't overfit on early data sets.\nI am also unsure why the full-replay strategy (which is model-based), does not appear to have better sample\nefficiency than the model-free experiment. Am I missing something here?\nPerhaps this is an interesting point for discussion. Do the high-dimensional features sacrifice sample efficiency\nin favour of formulating a linear model which can handle the non-stationary data?\nI'm not sure if this is correct.\nMy main point here is that the paper has not answered all of my questions about the method.\nThe experiments tell the first part of a nice story.\nTable 1 compares to other encoding methods and Fig. 3 clearly shows how the method handles covariate shift better than NNs.\nFig. 4 also acts as a nice ablation for comparing the method to other CL strategies within the same set-up.\nHowever, the experiments section lacks a comparison to other MBRL strategies which use world models.\nIn particular, there is no experiment highlighting the main issue the paper is trying to address.\nThat is, there is no MBRL experiment failing due to the non-stationarity of the training data.\nMinor comments and corrections:\nThe abstract is very long. I would recommend shortening it.\nSections shouldn't lead straight into subsections (Section 4/4.1, 5/5.1, B/B.1, C/C.1). There should be text explaining what the reader can expect to read in the section.\nIn the first paragraph of Section 2.2, the reward function is defined as $R(\\mathbf{s}, \\mathbf{a}, \\mathbf{s}')$ but then in the optimal policy equation you use $R(\\mathbf{s}, \\mathbf{a})$.\nFourth line of Section 2.2 the initial state distribution is $\\rho$ but earlier it is $\\rho_0$.\nThird paragraph of Section 2.2 - \"We firstly formulate\" should be \"We first formulate\".\nSection 2.1 - \"When the input is a convex set $\\mathcal{S}$, the prediction a vector $\\mathbf{w}_{t} \\in \\mathcal{S}$\". This sentence doesn't read properly.\n$\\rho$ is used to denote the initial state distribution and to denote the dimension of the grids in Section 3.2.\nWhat is the value of $\\delta$ in Fig. 1?\nWhat are $\\pi_0$, $\\pi_t$ and $\\pi_{t'}$?\nThe first sentence of the abstract says model-based RL has better sample efficiency. Better than what? It's model-free counterparts?\nIt is unusual to end the paper with a section titled \"Summary\". I recommend changing this to \"Conclusion\".\nQuestions:\nWhat are the limitations of the proposed method? Can it handle image observations? Can it scale to difficult environments like Humanoid?\nWhy haven't you compared to any other world model algorithms? E.g. Dreamer, TD-MPC.\nHow was the NN full-replay experiment implemented? Did the full-replay experiment involve resetting the neural network's weights? If so, what initialization was used? When was the NN training stopped? Was the data split into train/validation sets and used to stop training when the validation loss stopped improving?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer CrA3 (1/2)", "Subheading": "Official CommentbyAuthors15 Nov 2023, 09:28 (modified: 15 Nov 2023, 09:30)EveryoneRevisions", "Content": "Comment:\nThank you very much for appreciating the originality of our paper, as well as your detailed feedback and suggestions. We have updated the paper with text changes highlighted in yellow, and elaborate on each of the questions below.\n1. There is no experiment highlighting the main issue that the paper is trying to address: NN-based world models suffer from catastrophic interference due to non-stationary data.\nExperiments in Section 5 are designed to compare our method with different variants of NN-based world models while keeping the value/policy learning the same. Except for the \u201cModel-free\u201d setting, all other baselines train NN-based world models. From Figure 4, we can observe that NN-based world models trained online (referred to as \u201cOnline\u201d in the legend) show inferior performance. This is an indication that they suffer from catastrophic interference due to non-stationary data. As a result, the online learned NN world model could only generate synthetic transitions of poor quality, deteriorating the agent performance. Applying continual learning techniques (such as \u201cSI\u201d and \u201cCoreset\u201d) alleviates the forgetting issue to some extent. And learning world models with our method outperforms other baselines for most environments.\n2. Why aren't SoTA MBRL algorithms (e.g., Dreamer/TD-MPC) included for comparison?\nFirst, we hope that we can get aligned to the point that NN-based MBRL suffers from catastrophic forgetting, including the SoTA algorithms. The arguments are following,\nCatastrophic forgetting is a universal phenomenon for neural networks.\nExisting end-to-end NN-based MBRL methods, such as Dreamer, TD-MPC, and MuZero, etc. rely heavily on techniques to make data more stationary, for example, maintaining a large replay buffer, updating the target network once in a while, or simultaneously running multiple environments. When these components are removed, they would fail to work.\nHowever, when they fail we don't know how much is due to the forgetting in policy/value/model because all components are entangled and trained end-to-end in these methods.\nIf we're aligned on the point that NN-based MBRL suffers from catastrophic forgetting due to non-stationary data, then what we're studying is that we focus on making the world model fully online, which relies on\nnone\nof the above techniques. With this scope, we only compare our world models and NN-based world models using the Dyna architecture, because it decouples policy/value learning from model learning, making an apple-to-apple comparison on model learning possible.\n3. Can your method work for more difficult tasks like Humanoid or handle image-based observations?\nCurrently, we have not validated it for more challenging tasks like you mentioned. In Humanoid, the dimension of the state is very high, posing a curse of dimensionality to our method. For image-based observations, finding a compact representation with a non-trainable encoder is non-trivial, and our attempts so far do not yield positive results. We have added a Limitation section on page 9 to address them in the paper."}, {"Heading": "Response to Reviewer CrA3 (2/2)", "Subheading": "Official CommentbyAuthors15 Nov 2023, 09:30Everyone", "Content": "Comment:\n4. NN training details for the Full-replay experiment.\nThe Full-replay experiments did not involve resetting the NN\u2019s weights. We have tried resetting the weights but empirically found this required much more compute budget since it trained from scratch every time, which is impractical for a lifelong RL agent. Besides, it raises another design choice about when to reset, which may be not trivial since there are no explicit task boundaries.\nInstead, we \u201cfinetune\u201d a single model continuously, with the train/holdout split (with a ratio of 8:2) to determine when to stop the training and avoid overfitting. We agree that training without resetting weights is likely to overfit early data sets, but empirically this seems okay. We closely follow the codes of MBPO (\nhttps://github.com/jannerm/mbpo\n), which also store all previous experiences and do periodic training without weights resetting, for our implementation. We have updated the paper and provided more details about NN training in Section 5.1 on page 8.\n5. Why isn\u2019t the Full-replay MBRL better than the model-free baseline?\nThe expectation that Full-replay MBRL should outperform the Model-free baseline is based on an assumption: an accurate world model is learned such that it can synthesize transitions on which planning updates accelerate value/policy learning. However, achieving so is non-trivial. As you have mentioned, successful training requires stopping training properly. Though we have employed a train/holdout split to determine when to stop, our parameters may not be optimally set. Apart from this, there is more brittleness in NN training in this case. For example, as you mentioned, not resetting the weights may make NNs overfit early data; but when to reset is hard to know because there are no explicit task boundaries; even if we know when, resetting and training from scratch is even more time-consuming and impractical for lifelong agents. In our experiments, we choose not to reset the weights for efficiency consideration, and this might be a source of training difficulty that leads to inferior Full-replay results, especially for Acrobot environment.\n6. Other minor corrections.\nThank you very much for the detailed feedback! We have updated the paper to address all the mentioned issues.\nThank you again for providing insightful comments which helped us to improve our paper, and we hope our responses and updates in the text were able to address any remaining concerns. Please do let us know if you have any further questions as well as what would be expected for score improvement."}, {"Heading": "Official Comment by Reviewer CrA3", "Subheading": "Official CommentbyReviewer CrA318 Nov 2023, 10:13Everyone", "Content": "Comment:\nThank you for the detailed comments and for updating your manuscript. I will increase my score to a 6.\nIn my opinion, this paper doesn't isolate the issue of nonstationary data for world model training. I appreciate that model-based RL algorithms have a lot of moving parts, which can make it hard to isolate specific problems. I do think that training until the validation loss stops decreasing will 100% lead to the NN overfitting early in training. As such, the limited performance of Full-replay is likely due to primacy bias and not the nonstationary data. A simple solution is to add one extra baseline. A modification of the Full replay experiment where the NN is reset at each episode. Whilst I know this is not practical for a lifelong agent, it should be feasible for the simple environments reported in the paper. This would remove issues with primacy bias and allow the baseline to act as a sort of upper bound on model-based performance.\nWhen talking about world models, readers will immediately think of methods like Dreamer. My questions about Dreamer/TD-MPC arose because you refer to your method as a world model. I'd suggest the authors move away from the term world model and instead stick to Dyna. Also, consider adding this part of your response to the manuscript:\n\"Existing end-to-end NN-based MBRL methods, such as Dreamer, TD-MPC, and MuZero, etc. rely heavily on techniques to make data more stationary, for example, maintaining a large replay buffer, updating the target network once in a while, or simultaneously running multiple environments. When these components are removed, they would fail to work.\nHowever, when they fail we don't know how much is due to the forgetting in policy/value/model because all components are entangled and trained end-to-end in these methods.\"\nGiven that you have now clearly stated your method's limitations, with regard to image-observations etc, I am OK without a direct comparison to Dreamer/TD-MPC.  I agree that comparing to dyna style methods makes a comparison of model learning easier."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors19 Nov 2023, 02:02Everyone", "Content": "Comment:\nThank you very much for the score improvement and for providing valuable feedback.\nWe have updated the manuscript to include the quoted discussion about other NN-based MBRL methods (in Section 2.2), as well as putting the term \u201cworld model\u201d in the context of Dyna wherever needed. We will add another baseline for Full-replay with weights reset to demonstrate the upper bound performance for NN models."}]}]}, "e47RxA52sT": {"paper_info": {"Keywords": "Recommender system, Selection bias, Doubly robust", "Abstract": "Recommender system aims to recommend items or information that may be of interest to users based on their behaviors and preferences. However, there may be sampling selection bias in the process of data collection, i.e., the collected data is not a representative of the target population. Many debiasing methods are developed based on pseudo-labelings. Nevertheless, the effectiveness of these methods relies heavily on accurate pseudo-labelings (i.e., the imputed labels), which is difficult to satisfy in practice. In this paper, in contrast to the existing doubly robust estimators that take strictly accurate pseudo-labelings as an unbiasedness condition, we theoretically propose several novel doubly robust estimators that are unbiased when either (a) the pseudo-labelings deviate from the true labels with an arbitrary user-specific inductive bias, item-specific inductive bias, or a combination of both, or (b) the learned propensities are accurate. We further propose a principled propensity reconstruction learning approach that adaptively updates the constraint weights using an attention mechanism and effectively controls the variance. To summarize, the proposed methods greatly relax the unbiasedness condition of the widely-adopted doubly robust estimators, which empirically result in much lower bias. Extensive experiments show that our approach outperforms the state-of-the-art on one semi-synthetic dataset and three real-world datasets.", "Supplementary Material": "zip", "Primary Area": "general machine learning (i.e., none of the above)", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9439", "PDF Url": "https://openreview.net/pdf?id=e47RxA52sT"}, "review_info": [{"Heading": "Official Review of Submission9439 by Reviewer d1UZ", "Subheading": "Official ReviewbyReviewer d1UZ04 Nov 2023, 20:55 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposes User-DR, Iten-DR, and User-Item-DR for unbiased recommendation. The proposed methods have both strong theoretical guarantees and improvement in comparison with the baselines.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nStrong theoretical guarantee on the proposed method\nThe experimental studies are solid. I would like to point out that a ~10% improvement in NDCG@5 is very significant for unbiased recommendation. Note that the machine learning model is not changed in this paper. To be more accurate, only the debiased method is different from the baselines if I understand correctly!\nWeaknesses:\nSome baselines are lacking introduction. For example, the best baseline, ESCM-DR, is not introduced in detail.\nThe running time (or time complexity) is lacking in analysis in the paper. I think this paper can be improved if ESCM-DR is slower than the proposed algorithms.\nThe evaluation matrics, AUC, NDCG, and F1, are missing clear definitions (can be in the appendix).\nQuestions:\nCan the authors provide some detailed explanations for ECSM-DR and Multi-DR?\nAny record of the running time (or time complexity) can be provided?\nCan the authors provide a rigid definition for F1 measure?\nCan the authors also provide the experimental results similar to Table 3 for NDCG and F1 @ other values in the appendix? At least for Coar and music datasets, Top-1 should be also an important measure.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n8: accept, good paper\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9439 by Reviewer uhbY", "Subheading": "Official ReviewbyReviewer uhbY31 Oct 2023, 23:59 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper presents a study on the challenges and potential solutions associated with debiasing the recommender system models due to the sampling selection bias in the process of data collection. The authors propose several novel doubly robust estimators that are unbiased.\nThese estimators are unbiased for arbitrary user-specific, item-specific inductive bias, and even both. Authors also theoretically prove these estimators\u2019 unbiasedness. Besides, they propose a propensity reconstruction learning approach that adaptively updates the constraint weights to meet the constraints of the proposed UIDR estimator.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\n(1) The authors introduce a series of innovative double robustness (DR) estimators through a rigorous theoretical framework. These estimators maintain their unbiased nature even when pseudo labelings diverge from the true labels, accommodating arbitrary and unknown biases specific to users, items, or a combination thereof. This represents a significant stride in addressing user-specific and item-specific inductive biases, showcasing the adaptability and robustness of the proposed methods.\n(2) In a further extension of their work, the authors present a principled propensity reconstruction learning strategy, which adeptly utilizes an attention mechanism to adaptively update the constraint weights. This approach not only enhances the adaptability of the model but also ensures that the variance of the DR estimators remains within controllable and manageable bounds. This aspect of the work underscores the authors\u2019 commitment to developing robust and reliable estimators, contributing to the stability and efficacy of the proposed methods.\n(3) The paper\u2019s empirical validation is robust, encompassing semi-synthetic experiments that attest to the effectiveness of the proposed methods in scenarios involving arbitrary user-specific and item-specific inductive biases. This is a notable achievement, as previous methodologies have fallen short in providing unbiased estimates of the ideal loss under these conditions. Additionally, the authors extend their validation to real-world contexts, conducting comprehensive experiments across three real-world datasets. These experiments serve to highlight the tangible advantages and superior performance of the proposed methods, solidifying the paper\u2019s contributions to the field.\nWeaknesses:\n(1) While the theoretical foundation appears robust, there is a potential concern regarding the complexity and practicality of implementing such estimators in real-world scenarios when we have large U and large I. The paper could benefit from a more detailed discussion on the potential challenges and limitations associated with these novel DR estimators, providing a more balanced and critical perspective.\n(2) The introduction of a principled propensity reconstruction learning approach, utilizing an attention mechanism to adaptively update constraint weights, is indeed a novel contribution. However, the claim that the variance of the DR estimators is highly controllable and manageable warrants a more rigorous scrutiny. Can we see any tradeoff between the bias and variance, since sometimes we want to minimize the MSE in the ML community when you mentioned controllable and manageable.\nQuestions:\nIn section 2, it seems that\nr\nu\n,\ni\nin [0,1] rather in R, which is the rating/\nAfter corollary 1, \u201cthe biases of the proposed DR estimators are still strictly smaller than the previous DR\u201d , Is this toy example realistic? Could you please show a real example here?\nFor the optimization problem, do we have a constraint that\np\n~\n< 1?\nWhat is A? What is b? What is\ns\nu\nand\nt\nI\n? How do we learn it? Why\ns\nu\nhas been applied tanh but\nt\ni\nis not been applied?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9439 by Reviewer aJVp", "Subheading": "Official ReviewbyReviewer aJVp30 Oct 2023, 04:16 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper presents a new doubly robust estimator for the missing-not-at-random bias.\nThe authors propose constraints on propensity scores to handle pseudo-labels deviating from the true value with the user-specific bias.\nThey provide theoretical analysis and practical experiments, showing how their approach outperforms benchmark methods.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nthe solution (a constrained propensity model) is simple yet effective.\nthe authors provide theoretical analyses on the unbiasedness and the variance of the proposed estimator.\nWeaknesses:\nthe target problem is too specific and minor. The authors noted that UIDR can effectively alleviate the \"inaccurate pseudo-labeling problem\" in the previous DR estimators. However, they only treat the situation where the pseudo-labelings deviate from the true labels with arbitrary user-specific inductive bias. This assumption looks quite unrealistic as the user-specific inductive bias is assumed to be equivalent for every item.\nthe proposed procedure is not well-motivated. The authors put constraints on the propensity model, not the imputation model, in order to tackle the inaccurate imputation model. If the imputation model is inaccurate the straightforward remedy would be either adjusting the imputation model itself or designing a new loss function robust to the inaccurate imputed errors. If we adjust the propensity model for the imputation model, the accuracy of the propensity model can be harmed.\nQuestions:\nplease refer to weaknesses.\nex) If we adjust the propensity model for the imputation model, how does the accuracy of the propensity model become?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9439 by Reviewer BXqD", "Subheading": "Official ReviewbyReviewer BXqD24 Oct 2023, 09:04 (modified: 12 Nov 2023, 04:08)EveryoneRevisions", "Content": "Summary:\nThe paper introduces a novel doubly robust estimator to address observational bias in collaborative filtering.\nIt focuses on the effectiveness of the method, particularly in handling inaccurate pseudo-labels.\nThe authors propose debiasing techniques with constraints on propensity scores, aiming to ensure unbiasedness when the pseudo labels deviate from the true labels with an arbitrary inductive bias.\nThe paper combines theoretical analysis and experimental evaluations to demonstrate the method's effectiveness in comparison to benchmark approaches.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nComprehensive Background: The authors provide a well-explained introduction and extensive coverage of prior related work, enhancing the paper's accessibility to readers by providing context and a clear understanding of the research landscape.\nNovel and Effective Method: The proposed method effectively addresses the issue of pseudo-labels deviating from true values with specific inductive bias.\nTheoretical and Empirical Validation: The paper combines theoretical analysis with empirical results, covering both semi-synthetic and real-world datasets, demonstrating the method's technical soundness and its superior performance compared to existing approaches.\nWeaknesses:\nLimited Motivation: The paper may lack novelty in the central idea. Is there any empirical evidence that existing pseudo-labeling suffers from user/item specific inductive bias? If users have an inductive bias in the training set, the user should have the exact inductive bias in the test set.\nStrong Assumptions: The assumptions regarding inductive biases for users and items might not be realistic. The paper assumes constant inductive biases for users across all items, which may not hold true in real-life scenarios. Users typically exhibit varying preferences for different items.\nUnclear Propensity Score Calculation: The paper lacks a clear description of how propensity scores were calculated. There is a need for a more detailed explanation.\nQuestions:\nPersonally, I cannot understand the intuitive concept of the experiment with the synthetic dataset. Can you explain the intuition?\nIs there any empirical or theoretical evidence for the motivating inductive bias problem?\nFlag For Ethics Review:\nNo ethics review needed.\nDetails Of Ethics Concerns:\nNA\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}]}, "JWrl5pJCnl": {"paper_info": {"Supplementary Material": "zip", "Primary Area": "applications to robotics, autonomy, planning", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "large language models, robotic manipulation, code generation", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "Foundation models have significantly advanced in various applications, including text-to-image generation, open-vocabulary segmentation, and natural language processing. This paper presents Instruct2Act, a framework that leverages Large Language Models (LLMs) to convert multi-modal instructions to sequential actions for robotic manipulation tasks. Specifically, Instruct2Act uses LLMs to generate Python programs that form a comprehensive perception, planning, and action loop for robotic tasks. It uses pre-defined APIs to access multiple foundation models, with the Segment Anything Model (SAM) identifying potential objects and CLIP semantically classifying them. This approach combines the strengths of foundation models and robotic actions to transform complex high-level instructions into precise policy codes. Our approach is adaptable and versatile, capable of handling various instruction modalities and input types, and meeting specific task requirements. We validated the practicality and efficiency of our approach on robotic tasks including different tabletop and 6 Degree of Freedom(DoF) manipulation scenarios in both simulation and real-world environments. Furthermore, our zero-shot method surpasses many state-of-the-art learning-based policies in several tasks. The code for our proposed approach is available athttps://anonymous.4open.science/r/Instruct2Act, providing a solid benchmark for high-level robotic instruction tasks with diverse modality inputs.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9438", "PDF Url": "https://openreview.net/pdf?id=JWrl5pJCnl"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9438 by Area Chair cC5g", "Subheading": "Meta ReviewbyArea Chair cC5g10 Dec 2023, 09:16 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nSynopsis: This paper presents a system to leverage LLMs and visual foundation models to perform tabletop manipulation tasks. The task prompt is used to generate python code using the LLM, which then interactively queries visual foundation models to perform the task.\nStrengths:\nLots of empirical evaluations showing the performance on a variety of tasks\nThe use of visual foundation models eliminates the need for engineered solutions\nWeaknesses:\nThe paper does not convincingly articulate what is technically novel w.r.t. previous work, such as code-as-policies. The stated innovations seem minor, and the empirical results do not convince the reader that the stated innovations provide a significant boost.\nWhile the results include a few real world tasks, this is significantly fewer than the state of the art, again, including code-as-policies.\nThe choice of using an oracular detector is questionable.\nJustification For Why Not Higher Score:\nHaving read the paper and the reviews in reasonable depth, I am not convinced by the stated innovations over the state of the art. The empirical evaluations (e.g., comparisons to CaP, use of oracular detectors, etc.) raise more questions, as in the author response stage. While the authors tried to perform a few additional experiments to allay these concerns, I do not think they were convincing.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "Official Review of Submission9438 by Reviewer F1ER", "Subheading": "Official ReviewbyReviewer F1ER01 Nov 2023, 07:02 (modified: 23 Nov 2023, 03:50)EveryoneRevisions", "Content": "Summary:\nThe paper looks at the problem of generating control programs for robotic control with the help of foundation models. Part of the functionality that can be leveraged in the program are based on visual (SAM) and visual-language (CLIP) foundation models. The programs themselves are obtained by prompting an LLM. The prompt is equipped with external libraries, function descriptions and example programs. The method is tested on benchmarks with text only instructions as well as text-visual instructions and performs competitively. Extensive ablation studies show the importance of individual elements of the prompt and how the choice of foundation models impacts performance. The model shows OOD capabilities, that cannot be achieved by prior methods.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nExtensive ablation studies that showcase specific abilities of the model (types of OOD generalization), investigate the role of the foundation models and study the importance of the components of the prompt.\nRemoving the dependence of engineered perceptual models from the CaP method by using foundation models.\nSlight improvements over CaP via the different prompting method.\nThe methods and results are presented in an understandable manner.\nWeaknesses:\nStandard Errors: The VIMABench experiments were run over three random seeds for each meta-task but results are reported without any standard errors? In case of the textual prompts benchmarks its also not clear to me why no standard errors on results are reported?\nChoice of baselines: For the VIMABench I find the choice of baselines not insightful. On the one hand the baselines make use of a large set of training trajectories, but on the other hand these methods train policies that choose low-level actions. The presented method chooses action primitives such as \"PickAndPlace\" but does not need much training data (apart from the examples). I think adapting CaP to the VIMA benchmark would be a more insightful baseline.\nI think it makes sense in the comparison with CaP to compare using an oracle object detector, but one of the main novelties of the work is replacing the perceptual modules of CaP with foundation models. Therefore it would be interesting to see how this affects performance, i.e. just run the method on the CaP benchmark without the oracle.\nToo much detail in related work: I think the related work section is too long and just a list of papers rather than helping to place the work in the research area. I would shorten it and move interesting results from the Appendix to the main paper.\nQuestions:\nParagraph 4.4: What does RR stand for?\nAre the example programs in the prompt fixed for every task or do they depend on the task ?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer F1ER", "Subheading": "Official CommentbyAuthors22 Nov 2023, 13:46 (modified: 22 Nov 2023, 14:00)EveryoneRevisions", "Content": "Comment:\nQ1: Standard Errors: The VIMABench experiments were run over three random seeds for each meta-task but results are reported without any standard errors? In case of the textual prompts benchmarks its also not clear to me why no standard errors on results are reported?\nA1\n: We are grateful for the reviewer's insightful suggestion. We reported our method's performance with a success rate following the original VIMA paper, where the standard errors are missing as shown in Table 10 in [1]. To make a direct comparison, we also eliminated that item in our paper. One possible reason for this choice is that the SR is calculated with more than 100 tasks for each meta-task, where the average has already been taken. We are glad to provide the standard errors for these three random seeds.\nvisual_manipulation\nRotate\nscene_understanding\npick_in_order_then_restore\nrearrange\nrearrange_then_restore\nL1\n0.0039\n~0\n0.0103\n0.0308\n0.0699\n0.0155\nL2\n0.0067\n0.0194\n0.0124\n0.0476\n0.0492\n0.0168\nL3\n0.0068\n0.031\n0.0144\n0.0039\n0.0413\n0.033573127\nQ2: Choice of baselines: For the VIMABench I find the choice of baselines not insightful.\nA2\n: To make a fair comparison with other methods on the VIMABench, we mainly follow the VIMA'a paper and use the experiment result from their table.\nQ3:  I think adapting CaP to the VIMA benchmark would be a more insightful baseline.\nA3\n:  We thank the reviewer for this insightful suggestion. However, one problem with such adaption is that the CaP needs the ground-truth information of the target objects while VIMABench did not directly offer such information. To serve as a walkaround, we extract the GT masks from the simulator and use the center of the mask as the object position. We evaluate the CaP method on the single-step task visual_manipulation,  and multi-step task rearrange_then_restore, and the task requires both memory and saptional reasoning.\nvisual_manipulation\nrearrange_then_restore\nmanip_old_neighbours\nOurs\n91.3\n72\n64\nCaP-Oracle\n100\n70\n40\nAs shown in Table, the CaP -Oracle could achieve 100% SR for the simple single-step task. However, it struggles when longer steps, e.g. longer code generation are required. When spatial reasoning, e.g, with directional reasoning and memory mechanism are required, its performance degrades a lot.\nQ4: Too much detail in the related work.\nA4\n: We thank the reviewer very much for this suggestion. And we will reformulate the paper in the revised version.\nQ5: What does the RR stand for?\nA5\n: We thank the reviewer for pointing out this unclearness. The RR stands for Rotate and Restore task, as described in the caption of Table 4.\nQ6:Therefore it would be interesting to see how this affects performance, i.e. just run the method on the CaP benchmark without the oracle.\nA6:\nWe thank the reviewer a lot for this insightful suggestion. We run our method on the CaP benchmark, and the results are shown below. Due to the simple environment setting, our model achieves consistent performance in the task pick_and_place and directional_corner. However, in the tasks where there exist multiple target objects, our method suffers a performance degradation. It is mainly due to the fact that the CLIP calculates the classification scores with the similarity matrix, when there is more than one target object, the classification is affected. At the same time, the task instruction generally did not contain the number information of target objects, thus the generated code sometimes only considers one object.\nBesides, we noticed a huge performance gap in the task put_in_line, which is defined as an unseen task in the CaP benchmarks. In our previous implementation, we did not include any information for this task. However, when we adapt some prompts on the line description as done in CaP, we witnessed a boost from 55 to 85.\nModel\npick and place\nput in corner\ndirectional corner\nstack blocks\nput in line\nCaP\n88\n92\n72\n82\n90\nOurs\n100\n85\n90\n85\n45\nOurs-Oracle\n100\n95\n90\n90\n55\nQ7: Are the example programs in the prompt fixed for every task or do they depend on the task ?\nA7:\nYes. They are all the same for all tasks in the same benchmark. There are quite minor modifications for the cross-benchmarks setting, e.g. adding a depth parameter for RLBench.\n[1] Jiang, Yunfan, et al. \"Vima: General robot manipulation with multimodal prompts.\" arXiv (2022)."}, {"Heading": "Official Comment by Reviewer F1ER", "Subheading": "Official CommentbyReviewer F1ER23 Nov 2023, 03:49 (modified: 23 Nov 2023, 03:49)EveryoneRevisions", "Content": "Comment:\nThank your for the detailed response and running the additional experiments.\nA1. Thank you, it seems that standard errors are not reported since they are too small.\nA3. I think this results strengthens the evidence that the approach improves over CaP.\nA6. I think this additional experiment adds value to the paper by showing that performance can largely be maintained even without the Oracle.\nAll of my main concerns have been addressed. Accordingly I will adjust my score."}]}, {"Heading": "Official Review of Submission9438 by Reviewer KV7t", "Subheading": "Official ReviewbyReviewer KV7t01 Nov 2023, 02:12 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe proposed methodology introduces a way to utilize existing off-the-shelf vision and language foundation models for solving robotic tasks. The framework can be conditioned on language and or visual input, and produces Python programs for completing the given task. Experiments in both simulation and the real world demonstrate that the proposed system can outperform baselines.\nSoundness:\n3 good\nPresentation:\n4 excellent\nContribution:\n2 fair\nStrengths:\nWell-written. The paper is very well-written and easy to follow. The figures very much aid in understanding the paper.\nAblations. Ablations are conducted to better understand various design choices of the proposed methodology.\nWeaknesses:\nUnfair comparison to baselines.\nCaP\nThe proposed work's claimed distinction with CaP is unclear. The authors write \"Unlike existing methods such as CaP (Liang et al., 2022), which directly generates policy codes, our approach generates decision-making actions that can help reduce the error rate when performing complex tasks.\" But, the proposed methodology also uses LLMs to generate Python programs, so the distinction is unclear. Furthermore, about CaP, the authors write: \"However, its capabilities are limited to what the perception APIs can provide, and it struggles with interpreting longer and more complex commands due to the high precision requirements of the code.\" This seems to be a limitation of the proposed methodology as well, which also relies on a finite number of available APIs. Hence, again, the distinction between the proposed methodology and CaP is unclear.\nWhy is the oracle version of the proposed method used when comparing to CaP, instead of the non-oracle method? This seems to be an unfair comparison.\nEven then, the performance gap seems trivial. Is there any evidence to suggest that the gap in performance is non-trivial?\nPerAct\nThe authors write: \"For simple 3D tasks, such as stacking objects, we introduced an additional parameter to indicate the current occupancy. For more complex tasks, such as opening a drawer, we added some heuristic movements to ease the execution.\" PerAct was able to operate without such simplifying assumptions, which makes this comparison unfair.\nEven then, the performance gap is very small. How do we know this is a non-trivial gap in performance? Is there a trend of tasks / cases where the proposed methodology succeeds and PerAct fails? What is the intuition behind the proposed methodology outperforming PerAct, which was trained directly for the task at hand?\nDecision Transformer\nHow was comparison to DT done, as DT doesn't take in language instructions?\nWhat is the intuition behind the page gap in performance between the proposed methodology and DT?\nSimplistic Tasks.\nThe tasks considered for evaluation seem limited in complexity, e.g. are simply pick-and-place like tasks. These are tasks which can be very easily solved. More complex tasks have not been explored.\nFurthermore, as the \"Generated Policy Code\" in Figure 3 depicts, there is an API for pick-and-place, which the system simply calls upon. This simplifies the already simple problem quite a bit, by abstracting away the more difficult low-level control.\nFinally, a lot of processing / engineering is done to get the system to work (e.g. applying a gray threshold filter followed by a morphological closing operation, a morphological opening operation, Non-Maximum Suppression, etc.). If all of this engineering was required to get a simple task like pick-and-place to work (which is further simplified with the use of a pick-and-place API), then I find it difficult to see how the proposed methodology could be extended to more complex manipulation tasks, thereby limiting its utility.\nQuestions:\n\"Although the language models may occasionally generate incomplete or incorrect code, such as missing brackets, punctuation, or mismatched cases, the Python Interpreter can detect these errors and prompt us to generate new code.\"\nWhat is done in these situations? Is the LLM simply prompted a second time, with the hope that the output will not contain incomplete or incorrect code?\nHow much prompt engineering went into this when evaluation? Were the prompts the same across tasks, and across methods (ie when comparing to the baselines)?\nHow computationally expensive / slow is the framework? Can the tasks be solved in real-time?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer KV7t [1/2]", "Subheading": "Official CommentbyAuthors22 Nov 2023, 14:14Everyone", "Content": "Comment:\nQ1: The authors write \"Unlike existing methods such as CaP ...\nA1\n: We thank the reviewer for the valuable comment. This comment originates from the observation in Table 6 in the CaP paper. \nThe success rate (SR) for task 6 \u201cPick up the block to the <direction> of the <receptacle-bowl> and place it on the <corner>\u201d is only 72 compared with 100 SR for the naive pick place task. We derive deep into the failure case and find two main failure causes:\nCode generation error. The generated codes from CaP could contain some basic errors. One observed failure case is misused (, like \u201cdiffs = (points[(:, 1)] - point[1]) \u201d, which causes SyntaxError.\nLLM Hallucinations. Although the CaP has implicitly taught the LLM about the geometric relationship through the prompt in-context examples, the LLM sometimes still gets confused about geometric reasoning.\nTo enhance the original CaP [1], we utilized the Instruct2Act methods and added functions as class members in the original CaP classes, and provided function APIs information in the prompts as done in Instruct2Act. We then run 20 trials for task 6 and obtained 18/20, e.g. 90 SR, with 25% improvements.\nQ2: the authors write: \"However, its capabilities are limited to what the perception APIs can provide, and it struggles with interpreting longer and more complex commands due to the high precision requirements of the code.\" This seems to be a limitation of the proposed methodology as well, which also relies on a finite number of available APIs. Hence, again, the distinction between the proposed methodology and CaP is unclear.\nA2\n: We thank the reviewer for this valuable comment. The representation of \"its capabilities are limited to what the perception APIs can provide\" comes from the fact that the current version of CaP mainly derives object existence information directly from the simulator's system information with some pre-defined template. In contrast, our method employs visual foundational models to handle complex visual information more flexibly.\nQ3: Why is the oracle version of the proposed method used when comparing to CaP, instead of the non-oracle method? This seems to be an unfair comparison.\nA3\n: We respectfully disagree with this comment. As stated before, the open-sourced CaP implementation is the version used in the simulation and it assumes that all\nobject-related information could be accessed by the object's name and id information\n. To make a fair comparison, we also implemented an Instruct2Act-Oracle version, where we assume the object detection module returns the ground truth information.\nQ4: Even then, the performance gap seems trivial. Is there any evidence to suggest that the gap in performance is non-trivial?\nA4\n: Our strengths are from two folders:\nMuch less token consumption.  As shown in Table 1, our method consumes almost only 40% of tokens when both are evaluated on the CaP benchmarks.\nFewer LLM Hallucinations with API style. It is evident in the Task Directional Corner, as shown in Table 16, where our method achieves 90 SR while CaP achieves 72 SR.\n**Q5: \"PerAct was able to operate without such simplifying assumptions, which makes this comparison unfair.\" **\nA5\n: We thank the reviewer a lot for pointing this out. Firstly, PerAct needs to be trained before testing. They already gain depth reasoning ability during the training. Then, we use a single camera for visual input, while PerAct needs two cameras to induce depth information. Furthermore, the proposed assumption is only a naive walkaround, which could be replaced by the naive projection to 3D space (as suggested by Reviewer V9hM) or directly estimated by other visual models which are invoked also in the API style.\nQ6:  Is there a trend of tasks / cases where the proposed methodology succeeds and PerAct fails? What is the intuition behind the proposed methodology outperforming PerAct, which was trained directly for the task at hand?\nA6\n: We thank the reviewer for this valuable comment. As shown in Table 15 in our appendix, our method succeeds more with the tasks requiring multiple target objects interaction and multi-step execution, such as push_buttons, we achieved 70 vs PerAct 48. One possible reason is the PerAct is trained with different tasks, some require single-step action, while some require multi-step multi-object interaction, and the learned policy struggles to generalize overall. In contrast, our method abstracts different types of tasks with API-style representation, making it more stable and easy to execute. Moreover, we used the source codes and provided weights in the PerAct webpage, and found that the PerAct is unable to handle the OOD instructions mentioned in the Table 6."}, {"Heading": "Response to Reviewer KV7t [2/2]", "Subheading": "Official CommentbyAuthors22 Nov 2023, 14:18Everyone", "Content": "Comment:\nQ7: How was the comparison to DT done, as DT doesn't take in language instructions?\nA7\n: We follow the evaluation pipeline stated in VIMA[1], where DT's initial reward prompt with the VIMA's multimodal task prompt embeddings.\n[1] Jiang, Yunfan, et al. \"Vima: General robot manipulation with multimodal prompts.\" arXiv (2022).\nQ8: Simplistic tasks with a lot of processing work.\nA8\n: This simple action pattern is mainly constrained by the VIMABench simulator, where most tasks are intrinsically the pick-and-place action sequences. However, we also evaluated our method in RLBench, where more complex tasks like opening the drawer and closing jar are used. And we respectfully disagree with the reviewer on the processing/engineering part. Firstly, most of the processing modules mentioned in this paper are very commonly used in CV or the robotic community. Secondly,  we kept the same processing pipeline for all three evaluation benchmarks and found with this simple pipeline, we could achieve promising and robust performance.\nQ9: \"Although the language models may occasionally generate incomplete or incorrect code, such as missing brackets, punctuation, or mismatched cases, the Python Interpreter can detect these errors and prompt us to generate new code.\"What is done in these situations?\nA9\n: We thank the reviewer a lot for pointing out this unclearness. In our implementation, we append the last-round generated code lines $C_{t-1}$ with the errors $E_{t-1}$ from the Python Interpreter to construct history information $H$. This $H$ with the task instruction, original in-context examples will be sent to LLM to prompt the LLM to generate the improved code lines with the knowledge of historical errors.\nQ10: Were the prompts the same across tasks, and across methods (ie when comparing to the baselines)?\nA10\n:  Yes, we use the same prompts when evaluated across tasks in the same benchmark.\nQ11: How computationally expensive/slow is the framework?\nA11\n: When tested on a single NVIDIA RTX 3090Ti, FastSAM takes about 50ms, CLIP operation takes about 40ms, and other general operations take 10ms in total. However, we have to admit that the connection to OpenAI could not be that stable sometimes."}]}, {"Heading": "Official Review of Submission9438 by Reviewer UX8k", "Subheading": "Official ReviewbyReviewer UX8k30 Oct 2023, 10:21 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper presents Instruct2Act, a framework that leverages Large Language Models (LLMs) to convert multi-modal instructions to sequential actions for robotic manipulation tasks. It uses pre-defined APIs to access multiple foundation models. Its approach is validated in both simulation and real-world experiments.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nInstuct2Act represents a whole pipeline from utilizing multimodal instructions to actions. This is meaningful in embodied ai domain.\nIt can handle a diverse range of task types.\nThis paper includes suckers and gripper in simulations, and conducts real-world experiments.\nWeaknesses:\nThe embodied ai plus LLM develops too fast. It seems that this paper a little bit lacks of novelty. Instruct2Act seems like the combination of VIMA and CaP.\nQuestions:\nI think the author should add GPT-4 api and Codellama in ablation?\nWhat\u2019s the main improvement of Instruct2act? Could the authors list some difference between Instruct2act and VIMA, except for using foundation model to detect the objects?\nFor the franka manipulation, as the paper mentioned: 'Our method is presently limited by the basic action primitives, such as Pick and Place.' So, the franka-based tasks also uses the action primitives?\nFlag For Ethics Review:\nNo ethics review needed.\nDetails Of Ethics Concerns:\nNo ethic concerns.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer UX8k", "Subheading": "Official CommentbyAuthors22 Nov 2023, 14:20Everyone", "Content": "Comment:\nQ1: I think the author should add GPT-4 api and Codellama in ablation.\nA1\n: We greatly thank the reviewer for this valuable suggestion. We add the comparison with GPT-4 API and Codellama as below. Since CodeLLama has a more limited token length, we removed some unrelated function definitions and in-context examples. Same as Table 5, we chose Visual Manipulation (VM) and reported the success rate with 40 runs. Besides, we inspect the error sources due to LLM's incorrect code generation with the case number as described in Appendix A.1. Note that all these LLMs are prompted again without historical information when Python interpreter raises error information. Besides, we test the robustness of these LLMs with the OOD instructions as described in Table, such as appending  \u201dCancel the task, stop!\u201d at the end of the original instruction.\nLLM\nSR\nSyntax error\nAttributes hallucination\nSR OOD-Instruction\nLLama-Adapter\n85.5\n3\n2\n20\nGPT-3.5-Default\n92.5\n1\n1\n90\nCodeLLama-13B\n95\n0\n1\n45\nGPT-4\n100\n0\n0\n100\nAs shown in the table above, with a stronger LLM, the LLM code generation quality is getting better. As a result, the final performance is getting stronger. And one thing to note is that, since our task could be represented with executable policy python code generation, which directly fails in the strength of CodeLLama, CodeLLama-13B even achieves better performance than GPT-3.5.\nHowever, when tested with OOD instructions, only GPT-3.5 and GPT-4.0 demonstrate robustness.\nQ2:  What\u2019s the main improvement of Instruct2act? Could the authors list some difference between Instruct2act and VIMA, except for using foundation model to detect the objects?\nA2\n: We extend our appreciation to the Reviewer for their astute observation. In response, we would like to formulate the differences except for the visual foundation model usage as follows:\nTraining Independence\n: Notably, our Instruct2Act operates devoid of any training phase. This stands in stark contrast to VIMA, which necessitates an extensive 650K training trajectories and a robust cluster node infrastructure for its training process.\nFlexibility in Task Expansion\n: In scenarios demanding solutions for novel tasks, Instruct2Act offers inherent scalability. The model can be effortlessly augmented by integrating the requisite function definitions in the python file and supplementing API directives in the prompt. Conversely, VIMA mandates either finetuning or a comprehensive re-training.\nOut-of-Distribution (OOD) Generalization\n: Capitalizing on the capabilities of the LLM, Instruct2Act exhibits commendable adeptness in managing OOD instructional inputs. For instance, when the directive \u201c90 degrees\u201d is substituted with \u201c0.5 radians\u201d, Instruct2Act demonstrates impeccable reasoning prowess to deduce accurate angular values for rotation-oriented tasks, a feat where VIMA-200M conspicuously falls short. Additionally, when the policy is evaluated against alternative benchmarks, such as RLBench, Instruct2Act accomplishes tasks with only minimal alterations. In similar conditions, VIMA-200M, fails to produce meaningful outputs.\nQ3:  For the franka manipulation, as the paper mentioned: 'Our method is presently limited by the basic action primitives, such as Pick and Place.' So, the franka-based tasks also uses the action primitives?\nA3\n: We thank the reviewer for pointing out this unclearness. The limitation statement of the action primitive is mainly raised by the VIMA simulator. In the evaluation tasks in RLBench, such as opening the drawer, action composing locating, adjusting orientation, and closing the gripper is constructed to grasp the drawer's handler is constructed."}]}, {"Heading": "Official Review of Submission9438 by Reviewer V9hM", "Subheading": "Official ReviewbyReviewer V9hM30 Oct 2023, 02:45 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis study targets the challenge of robotic manipulation tasks utilizing a Large Language Model (LLM) guided by multi-modal instructions. The efficacy of this approach has been tested with various instructions across different benchmarks including VIMABench and the CaP benchmark.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThis system conducts experiments on diverse benchmarks such as VIMABench and the CaP benchmark.\nDetailed ablation studies are also provided, highlighting the efficacy of 'Library API Full Examples' and different segmentation methodologies.\nWeaknesses:\nImportant related work RT-2 [1] is missing, likely because it is too recent to be included and discussed.\nThe improvement in performance is not robust. Figure 5 showcases results from evaluations on VIMABench. However, the baseline VIMA-200 performs slightly better than the suggested approach across all tasks. Although the authors argue that VIMA requires large-scale pre-training, their proposed method also depends on large-scale pre-trained foundation models, such as SAM. It would be beneficial for the authors to demonstrate performance gains on tasks where VIMA has not been trained. Furthermore, the performance gain of 3% over PerAct and 1% over CaP in Table 1, is also not significant.\nAlthough employing cursor clicks to create point prompts and guide SAM\u2019s segmentation is clever, applying this approach for the Pick and Place task seems unnecessary. If you have the camera extrinsics, you can easily obtain the 3D location of the click by projecting from the image space to the 3D space, given the depth or known height. In this case, there's no need for SAM. In fact, other example tasks should be selected to better highlight the advantages of the proposed method.\n[1] Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., ... & Zitkovich, B. (2023). Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818.\nQuestions:\nThis work mentions the utilization of two types of LLMs: text-davinci-003 and LLaMA-Adapter. Could you provide an explanation on the performance differences between these two?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer V9hM", "Subheading": "Official CommentbyAuthors22 Nov 2023, 14:24Everyone", "Content": "Comment:\nQ1: Important related work RT-2 [1] is missing, likely because it is too recent to be included and discussed.\nA1\n: We thank the reviewer for this valuable suggestion. RT-2[1] proposes a vision-language-action model that learns from both web and robotics data. In contrast, our method uses LLM to generate logic policy codes, and execute the robotic tasks in a training-free.\nQ2: However, the baseline VIMA-200 performs slightly better than the suggested approach across all tasks. Although the authors argue that VIMA requires large-scale pre-training, their proposed method also depends on large-scale pre-trained foundation models, such as SAM. It would be beneficial for the authors to demonstrate performance gains on tasks where VIMA has not been trained.\nA2\n:  Thank you for your insightful feedback.\nFirstly, regarding the performance comparison with VIMA-200, we agree that the overall performance difference might not be substantial in the tasks evaluated. However, the primary advantage of our method lies in its adaptability, versatility, and training-free. As shown in Table 6 of the main paper, VIMA fails to handle the OOD instructions often, even with 0% SR for Human Intervention. Besides, we have evaluated our method with the (almost) same prompts across different benchmarks, e.g. RLBench, VIMABench and CaP. However, VIMA failed to output reasonable actions when trained in VIMABench while tested in RLBench.\nSecondly, we agree that our approach, like VIMA, depends on pre-trained foundation models. However, VIMA requires large-scale pre-training for robotic-specific tasks. Our approach leverages general pre-trained vision foundation models that are not task-specific. This makes our method more scalable and adaptable.\nQ3: Furthermore, the performance gain of 3% over PerAct and 1% over CaP in Table 1, is also not significant.\nA3\n: Regarding the performance gains over PerAct and CaP, we would like to point out that CaP consumes almost 2.5 times tokens to execute tasks. And they suffer more from the LLM's hallucinations when tasks require strong spatial reasoning. As for PerAct, it needs training on the RLBench, and is unable to handle OOD instructions and to transfer to new tasks, even new benchmarks.\nQ4: Although employing cursor clicks to create point prompts and guide SAM\u2019s segmentation is clever, applying this approach for the Pick and Place task seems unnecessary. If you have the camera extrinsics, you can easily obtain the 3D location of the click by projecting from the image space to the 3D space, given the depth or known height. In this case, there's no need for SAM. In fact, other example tasks should be selected to better highlight the advantages of the proposed method.\nA4\n: As for the use of cursor clicks in the Pick and Place task, we agree that if camera extrinsics and depth information are known, the 3D location can be computed directly.  However, the strength of our approach is its ability to operate effectively even when such information (the click or extrinsic) is not available or reliable, which is often the case in real-world scenarios.\nAdditionally, we recognize that objects in the real world are not always uniformly distributed or symmetrical, and the clicked location might be on the edge of the object, which is not suitable for robot grasping. In these cases, if we first use the click to mark the target object, then use the SAM model for semantic segmentation to capture the complete object information, we can determine a more suitable grasping position through simple post-processing.\nLastly, we appreciate your suggestion to select different example tasks to better illustrate the advantages of our method. We plan to conduct additional experiments on tasks where the benefits of our approach are more evident."}]}]}, "uvYgx8raPO": {"paper_info": {"Primary Area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Multi-objective optimization, Trust region method, probabilistic models, global convergence.", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "Multi-objective expensive optimization problems appear in many real-world applications. These problems involve multiple computationally expensive objectives, and their derivative information is usually unavailable or hard to compute. Most existing methods focus on constructing high-quality surrogate models for individual objective functions or aggressive subproblems, and these often come with a prohibitive cost. This paper extends the trust-region method based on probabilistic models to solve multi-objective optimization problems. Specifically, we adopt the decomposition mechanism from MOEA/D to decompose the multi-objective optimization problem into multiple Tchebycheff subproblems. Each subproblem is then approximated using high-quality probabilistic models within a trust region framework. Under certain mild assumptions and the properties of Martingales, we can prove that the proposed method can converge to the Pareto critical point with probability one. Experimental results further illustrate that the proposed algorithm outperforms other representative algorithms on a low budget.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9437", "PDF Url": "https://openreview.net/pdf?id=uvYgx8raPO"}, "review_info": []}, "jUNSBetmAo": {"paper_info": {"Primary Area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Disentanglement, Orthogonality, Unsupervised Learning, Representation Learning, DCI, DCI-ES", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "TL;DR": "Proposing Importance-Weighted Orthogonality (IWO) as an alternative to traditional disentanglement metrics, we evaluate orthogonality in learned representations, demonstrating enhanced correlation with downstream task performance across applications.", "Abstract": "Evaluating learned representations independently of designated downstream tasks is pivotal for crafting robust and adaptable algorithms across a diverse array of applications. Among such evaluations, the assessment of disentanglement in a learned representation has emerged as a significant technique. In a disentangled representation, independent data generating factors are encoded in mutually orthogonal subspaces, a characteristic enhancing numerous downstream applications, potentially bolstering interpretability, fairness, and robustness. However, a representation is often deemed well-disentangled if these orthogonal subspaces are one-dimensional and align with the canonical basis of the latent space \u2013 a powerful yet frequently challenging or unattainable condition in real-world scenarios \u2013 thus narrowing the applicability of disentanglement. Addressing this, we propose a novel evaluation scheme, Importance-Weighted Orthogonality (IWO), to gauge the mutual orthogonality between subspaces encoding the data generating factors, irrespective of their dimensionality or alignment with the canonical basis. For that matter, we introduce a new method, Latent Orthogonal Analysis (LOA), which identifies the subspace encoding each data generating factor and establishes an importance-ranked basis spanning it, thereby forming the foundational bedrock for IWO. Through extensive comparisons of learned representations from synthetic and real-world datasets, we demonstrate that, relative to existing disentanglement metrics, IWO offers a superior assessment of orthogonality and exhibits stronger correlation with downstream task performance across a spectrum of applications.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9436", "PDF Url": "https://openreview.net/pdf?id=jUNSBetmAo"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9436 by Area Chair gUP7", "Subheading": "Meta ReviewbyArea Chair gUP713 Dec 2023, 14:23 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThis paper proposes an alternative metric to evaluate disentanglement and demonstrate enhanced correlation with downstream task performance.\nThis paper was quite borderline. One reviewer was positive, while the other three recommended rejection. The more negative reviewers appreciated the changes made by the authors to the paper, but during the discussion, they thought that the paper was still not ready for publication at ICLR. In particular, they mentioned that while Section 3's clarity had improved, it was still \"a hard read for a relatively straightforward algorithm\". Second, while the authors added another dataset to their evaluation, there are still several important missing benchmarks, and the reviewer wanted to properly review the additional experiments that the authors promised before potentially accepting the paper. As the ideas were deemed on the incremental side, a more solid experimental section was required.\nWithout stronger support for this paper, I decided to reject it. I encourage the authors to take the detailed feedback from the reviewers to improve their manuscript and re-submit at the next machine learning conference.\nJustification For Why Not Higher Score:\nThere was not sufficient support for this paper among the reviewers. See the meta-review for concerns to address in the revision.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors16 Nov 2023, 18:26Everyone", "Content": "Comment:\nWe are grateful for the insightful and constructive feedback provided by the reviewers. We are pleased to read that our research question is\nstrong\n[VoxK], our technical contribution and math derivation are\nsolid\nand\nsound\n[J4X7, ccpN] and that the paper is\nwell organized\n,\nclearly written\nand of\ngood presentation quality\n[J4X7, ccpN].\nFurthermore, the constructive suggestions have been instrumental in refining our manuscript and amplifying its contributions to the field. We have carefully considered and addressed each point raised, ensuring that our research not only remains robust but also becomes more accessible.\nExperiments\n:\nWe have enriched our experimental setup with the SmallNorb dataset and additional models as suggested, enhancing the comprehensiveness and robustness of our findings. Our revised Table 2, now with improved readability through colour-coding, better demonstrates our methodology\u2019s effectiveness.\nFigure 1 Redesign\n:\nWe have modified Figure 1 for greater clarity, aligning with the reviewers recommendations. The new 2D representation succinctly conveys the intended concept, making it more digestible for the readers.\nExpanded Related Work\n:\nOur expanded section on weak disentanglement and symmetry-based learning provides a richer context for our work, drawing clearer distinctions and similarities with existing literature.\nMethodological Refinements\n:\nResponding to reviewer [ccpN], we\u2019ve refined the importance weighing in IWO and IWR, we have also adapted IWR's calculation, enhancing the metrics\u2019 resilience and alignment with established standards.\nEnhanced Readability\n:\nThe method section has been revised for improved clarity and readability, ensuring that our approach is easily comprehensible and accessible."}, {"Heading": "Official Review of Submission9436 by Reviewer Voxk", "Subheading": "Official ReviewbyReviewer Voxk01 Nov 2023, 02:14 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper considers the question of evaluating the quality of disentangled representations with respect to the orthogonality of factors. The authors propose latent orthogonal analysis used to devise a new metric called importance weighted orthogonality. The method is evaluated on several datasets and shows promising results.\nSoundness:\n3 good\nPresentation:\n1 poor\nContribution:\n3 good\nStrengths:\nThe research question is strong and to the best of my knowledge this problem is still open at large, and so any advancement on this front is highly important. Another strength is the relative simplicity of the approach, involving basic neural networks and standard linear algebra operations. The results are also compelling, although somewhat basic, in my opinion.\nWeaknesses:\nThe main weakness of this submission is the clarity of exposition. In particular, Sections 2 and 3 could be improved significantly. For instance, the illustration in Fig. 1 is unclear. I believe the authors could do better by considering a 2D case instead of 3D, minimizing the use of colors and angles in the figure. Further, several crucial algorithmic components are described in a minimal fashion with supporting equations,illustrations, or pseudo-code. For example, the text above Eq. (2) and the text above Eq. (4). Given that the proposed method does not seem to be overly complex, I find it disappointing that its description is somewhat vague.\nAnother weakness is the evaluation section. Evaluating disentangled factors is a long-standing problem in representation learning. In particular, there are established benchmarks and papers focused on this particular problem. While I am not an expert on this issue specifically, I would assume that suggesting a new metric that is arguably better than others should be motivated better and empirically justified with more than two real-world datasets and a few toy examples.\nQuestions:\nSee above\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors16 Nov 2023, 18:34 (modified: 16 Nov 2023, 19:42)EveryoneRevisions", "Content": "Comment:\nThank you for your insightful review and constructive feedback. We have made several improvements to our manuscript based on your comments.\nFigure 1 Clarity and Simplification\nIn response to your suggestion, we have revised Figure 1 to present a 2D case, which simplifies the concept and minimizes the use of colors and angles. The figure now describes the difference between disentangled and orthogonal representations, in terms of DCI-D and IWO. As for the comparison with the Explicitness, acknowledging the original importance of the figure, as highlighted by Reviewer [1Jck], we have retained it in the appendix with an expanded caption for a more in-depth understanding.\nEnhanced Algorithmic Description\nWe have thoroughly revised Sections 2 and 3 to provide a clearer exposition of our methodology. This includes a more logical structure in presenting our formulas, where we first elaborate on the concept and purpose of e.g. IWO, before delving into its algebraic representation. This approach ensures a more intuitive understanding of our method and its theoretical underpinnings.\nComprehensive Evaluation Section\nTo address your concerns regarding the evaluation of disentangled factors, we have expanded our experimental section to align closely with the benchmark established by Locatello et al., 2019 [1], recognized as a gold standard in disentanglement research. We now include analyses on dSprites, SmallNORB, and Cars3D datasets. In the camera-ready version of the paper, we plan to introduce three further variations of the dSprites dataset. This expansion not only adheres to established benchmarks but also showcases the versatility and robustness of our proposed metric across a broader spectrum of scenarios.\nWe hope these revisions address your concerns comprehensively and enhance the overall quality and clarity of our work.\n[1] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar R\u00e4tsch, Sylvain Gelly, Bernhard Sch\u00f6lkopf, Olivier Bachem. ICML (Best Paper Award), 2019."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 11:18Everyone", "Content": "Comment:\nDear Reviewer,\nAs the discussion period nears its conclusion, we wish to ensure that all your concerns have been adequately addressed. We would greatly appreciate any additional feedback you may have. Our goal is to finalize the rebuttal with the confidence that our manuscript reflects the valuable feedback provided by the review committee.\nThank you for your time and consideration,\nthe Authors"}, {"Heading": "Official Comment by Reviewer Voxk", "Subheading": "Official CommentbyReviewer Voxk22 Nov 2023, 05:57Everyone", "Content": "Comment:\nDear Authors,\nI would like to thank you for your response. I also went over the revised version. I still feel that my main concerns (poor exposition and basic evaluation) are not fully addressed, unfortunately. Nevertheless, I will take into account your rebuttal and revised manuscript during the reviewers' discussion."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:19Everyone", "Content": "Comment:\nDear Reviewer,\nWe appreciate the consideration of our rebuttal and revised manuscript. Taking your feedback into account we would like to further address your concerns.\nRegarding the\nexposition\n, we have edited the methodology section, to further improve the clarity. We have ensured that our method is self-sufficient, with Figures 2 and 3 serving as supplementary aids rather than necessities. In particular, we have:\naddressed any ambiguity that our notation may have caused\nbetter clarified the aim of LOA\nmotivated the design choices of our method\nspecifically added a formula for the retrieval of basis vectors from the linear neural network\nenhanced the motivation behind our importance scores\nRegarding the\nconcerns on the evaluation of our work\n, we wish to point out that, in addition to our comprehensive synthetic experiments, we have evaluated our approach using three benchmark datasets (Dsprites, Cars3D, SmallNorb). These evaluations were conducted with four benchmark models ($\\beta$-VAE, Annealed VAE, $\\beta$-TCVAE, Factor VAE) that are prevalently utilized in disentanglement research. Our methodology was compared against four widely used metrics (DCI-D, DCI-C, MIG, DCI-ES). Moreover, we have expanded our analysis to include scenarios with limited data availability (see Appendix B.4). In response to the insightful feedback from reviewer 1Jck, we are also incorporating the Shapes3D dataset and three variations of the DSprites dataset into our evaluation process for the camera ready version of our paper.\nThank you for your time, we hope these revisions address your concerns and enhance the overall quality and clarity of our work.\nKind regards,\nthe Authors"}]}, {"Heading": "Official Review of Submission9436 by Reviewer J4X7", "Subheading": "Official ReviewbyReviewer J4X731 Oct 2023, 12:24 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nA new assessment scheme is introduced to measure disentanglement in latent space in terms of orthogonality between subspaces. The assessment builds on an decomposition methodology which projects the original latent encodings into incrementally smaller subspaces through linear neural models. The empirical analysis validates the proposed assessment scheme against existing disentanglement metrics on synthetic and benchmark datasets.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nS1) Motivations behind the paper are solid: too strict definitions of disentanglement as projection into single orthogonal dimensions are bound to fail in realistic settings. The idea of broadening the definition to orthogonal subspaces, while not being completely novel, is developed here through an approach which is original.\nS2) The technical contribution seems also solid, modulo some points which are not made entirely clear in the presentation. However, the overall methodology is convincing from the perspective of correctness and adequacy of the technical solutions.\nS3) The paper is well organized and mostly of good presentation quality.\nWeaknesses:\nW1) While presentation quality is generally adequate, the paper misses to convey all the necessary details to facilitate reproduction of the method and of the study. This lack of technical detail in the main body is not compensated by the availability of appendices, supplementary materials or code. One key aspect that is unclear to me is how one is expected to identify the generative factors set $z_1,\\dots, z_K$ and how such $K$ is determined in general. The method involves training a potentially large amount of regressors and little information is provided on how this is done in practice (e.g. how much should the training be pushed in terms of regression error? What are the stopping conditions? How are the linear model initialised?).\nW2) The positioning with respect to the literature is on the weak end. The paper misses to discussion and cite works formalising weaker forms disentanglement [A, B, C]. In particular, it would seem relevant to discuss the relationship between the proposed approach and those building on (and measuring) linear symmetry-based disentanglement [B,D].\nW3) While the experiments are generally well-designed, the evidence they provide does not seem enough to support the major claims of this paper. As long as one departs from the ideal setting, it is difficult to assess the added value of IWO over DCI and MIG. Additional experiments are needed on more challenging datasets, such as ModelNet40 and COIL-100, possibly enlarging the scope of methods to compare with by including those in [B,D]. It would also be of help to qualitatively explore the impact of the proposed methodology, e.g. by exploring the effects of manipulating the representations over the relevant subspaces \u201csuggested\u201d by the metrics.\nW4) The proposed methodology seems very computationally involved. I am using the word \u201cseems\u201d as the paper lacks a comparative assessment of the cost of the method. This should be done while considering more realistically sized problems, involving latent spaces of non-trivial size.\n[A]\nhttps://proceedings.neurips.cc//paper/2020/file/9a02387b02ce7de2dac4b925892f68fb-Paper.pdf\n[B]\nhttps://proceedings.mlr.press/v162/tonnaer22a/tonnaer22a.pdf\n[C]\nhttps://doi.org/10.1109/IJCNN55064.2022.9892093\n[D]\nhttps://arxiv.org/pdf/2011.13306.pdf\nQuestions:\nQ1) Can the Authors please clarify how generative factors  $z_k$ are selected for the purpose of implementing the method in general (see W1)?\nQ2)  Can the Authors please discuss the relationship with linear symmetry-based metrics?\nQ3)  The empirical analysis would be substantially strengthened by adding new experiments on as ModelNet40 and COIL-100, considering also computational costs?\nQ4) I am a bit puzzled by the negative correlation values in the experiments: is this classical linear correlation? Because some methods seem to be highly negatively correlated (which is still somehow a form of correlation).\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors16 Nov 2023, 18:55Everyone", "Content": "Comment:\nThank you for your thorough review and valuable insights. We have carefully considered each of your points and made significant revisions to our paper to address them.\nW1: Enhancing Reproducibility and Methodological Clarity\nAppendix and Implementation Details:\nWe have created a detailed appendix providing comprehensive information about implementation and model choices. This includes specifics on the initialization of linear layers, network training procedures, and hyper-parameter optimization strategies.\nCode Repository\n: To further facilitate reproducibility, we have established an anonymous code repository that contains all necessary code and documentation:\nhttps://anonymous.4open.science/r/iwo-E0C6/README.md\nW2: Expanding the Literature Review\nWeaker forms of Disentanglement\n: We have integrated a thorough discussion on linear symmetry-based disentanglement into our related work section. This includes an analysis of the works you mentioned and others in the field, emphasizing the relationship between our approach and these methods. We have also included a segment on causal representation learning, particularly focusing on how it intersects with and differs from disentanglement methodologies, including ours.\nW3: Strengthening the Experimental Section\nSmallNORB Inclusion\n: We agree with your point on the need for more challenging datasets. While ModelNet40 and COIL-100 are indeed valuable, we have chosen to incorporate the SmallNORB dataset beside DSprites and Cars3D. It offers comparable complexity, with diverse variations in object types, camera angles, and lighting conditions, aligning closely with the complexity of ModelNet40 and the natural imagery of COIL-100, while being part of\ndisentanglement_lib\nhttps://github.com/google-research/disentanglement_lib/tree/master\nby Locatello et al., 2019 [1] which offers unparalleled reproducibility and comparability. Utilizing datasets and models from\ndisentanglement_lib\nalso minimizes biases that could arise from custom implementations or training procedures, thus providing a clear and objective evaluation of our proposed metric's efficacy. In the camera-ready version of the paper, we plan to introduce three further variations of the DSprites dataset provided by disentanglement_lib.\nAdditional Downstream Task Experiment:\nWe have added an experiment involving a downstream task which demonstrates the interplay between subspace structures and IWO, offering a deeper insight into the practical implications of our methodology. See\nAppendix B.3: Modulation Task.\nW4: Addressing Computational Costs\nComparative Assessment\n: We acknowledge the importance of understanding the computational cost of our method. We have added a section that showcases the computational requirements of our approach for 60 runs on the Small Norbs dataset. We have also added the necessary strategies to deal with high dimensional latent spaces, such as larger reductions between consecutive LNN heads and an initial LNN-layer of smaller size than the latent space.  See\nAppendix C: Computational Resources Analysis\n.\nQ1 (Generative Factor Identification)\n: We closely align our approach with the current existing literature, using all generative factors provided by the datasets. In Appendix B.1: Datasets, all generative factors used in our analysis are listed.\nQ2 (Linear Symmetry-Based Metrics Relationship)\n: In our expanded literature review, we discuss the relationship between our approach and linear symmetry-based metrics, delineating both the commonalities and distinctions.\nQ3 (Empirical Analysis Enhancement)\n: We have addressed this in W3, as discussed above. The addition of the SmallNORB dataset, additional models, an analysis on reduced data (\nAppendix B.4: IWO on limited data\n) and the new downstream task experiment significantly bolster our empirical analysis.\nQ4 (Negative Correlation Values)\n: The negative correlation values were also observed by  Locatello et al., 2019 [1] in their benchmark study. Indeed, both downstream task performance and disentanglement metric are taken from their disentanglement_lib repository. As to the correlations, they are calculated by averaging both metric and downstream task performance over all seeds of each hyperparameter (the hyperparameter being the model's regularization strength). The correlation therefore describes how down stream task performance and metrics correlate over the different regularization strengths. We use this numpy implementation for the computation:\nhttps://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html\nWe appreciate the opportunity to improve our paper based on your feedback. We hope these revisions address your concerns comprehensively.\n[1] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar R\u00e4tsch, Sylvain Gelly, Bernhard Sch\u00f6lkopf, Olivier Bachem. ICML (Best Paper Award), 2019."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 11:18Everyone", "Content": "Comment:\nDear Reviewer,\nAs the discussion period nears its conclusion, we wish to ensure that all your concerns have been adequately addressed. We would greatly appreciate any additional feedback you may have. Our goal is to finalize the rebuttal with the confidence that our manuscript reflects the valuable feedback provided by the review committee.\nThank you for your time and consideration,\nthe Authors"}, {"Heading": "Thanks for the rebuttal", "Subheading": "Official CommentbyReviewer J4X722 Nov 2023, 04:10Everyone", "Content": "Comment:\nDear Authors, many thanks for having clarified the experimental settings and requirements of your model. I appreciate the extended literature analysis (though it would have been nice to see also the newly referenced methods compared empirically) and the improved transparency and reproducibility. Addition of SmallNORD is ok, as it is the computational analysis: the approach does not seem as computationally involved as I thought. However, it would have been good to measure the baseline computational cost also of the related models to have a means of comparison. \nThere are two aspects which still seem a limitation of the work: i) one needs to know in advance the generative factors and the method does not allow to discover those; ii) importance scores, for some reasons, should be computed on test data. The latter aspect is a bit puzzling as I was expecting that these can be computed, at least, on validation. Both issues, somewhat limit the generality and usefulness of the approach.\nNevertheless I will take the new information into consideration in the review internal discussion and update my score accordingly."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:35Everyone", "Content": "Comment:\nDear Reviewer,\nThank you for your feedback and for taking the changes to our manuscript into consideration. We would like to address both limitations pointed out:\n(i)\nOur method is closely aligned with the setup considered by most frameworks e.g. DCI [1], MIG [2], ES [3], in that it is a metric based on known generative factors. \nThe limitation\n\u201cone needs to know in advance the generative factors\u201d\nis a characteristic of a disentangled representation learning metric, as any attempt to estimate the generative factors without supervision is ill-posed, as pointed out by Locatello [4]. However, we were able to show that IWO and IWR are retrievable consistently from only small samples of labeled data as shown in\nAppendix B.4: IWO on Limited Data\n.\n(ii)\nPlease note that we can compute IWO and IWR on the training loss, the validation loss or the test loss (this is also done by our referenced implementation). The variation in our experimental outcomes was negligible throughout these sets. However, the ability of calculating IWO and IWR on either one of these sets is, in our opinion, a strength of our method. For reference, consider the case of the DCI-D metric. This metric is commonly calculated using GradientBoosting as for example in disentanglement_lib. On that basis, impurity-based feature importances are calculated. These are not computable on test sets, which is a limitation of this approach. Because in our metric the importance is based on the regression losses, we are not limited to calculating the importance on the training set but can calculate it on any one of the training, validation or test set.\nFinal note:\nWe would like to clarify that the goal of this work is to advance the field of disentangled representation learning by expanding it through the concept of orthogonal representations. For that, precise estimation of the disentanglement score is reliant on known generative factors (full-supervision) to provide clear comparisons between different models. \nHowever, we do see future work extending our approach for weakly supervised assessment, opening doors for its application outside of foundational studies (e.g. finding generative factors through causal discovery [5, 6] or exploiting unsupervised information through the use of some regularization term in the loss of the LOA regressors). \nThis would enable the application of our method as an unsupervised approach, further expanding the general use of our formulation and method.\nThank you for your time and consideration,\nKind regards,\nthe Authors\n[1]\nhttps://openreview.net/pdf?id=By-7dz-AZ\n[2]\nhttps://arxiv.org/pdf/1802.04942.pdf\n[3]\nhttps://arxiv.org/pdf/2210.00364.pdf\n[4]\nhttps://arxiv.org/abs/1811.12359\n[5]\nhttps://proceedings.mlr.press/v177/faria22a.html\n[6]\nhttps://www.jmlr.org/papers/volume24/21-1329/21-1329.pdf"}]}, {"Heading": "Official Review of Submission9436 by Reviewer ccpN", "Subheading": "Official ReviewbyReviewer ccpN30 Oct 2023, 19:50 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nIn this paper, the authors first propose Latent Orthogonal Analysis(LOA), a method that can identify latent subspaces for different factors of variation from data. To estimate the mutual orthogonality between subspaces learned with LOA, they then propose importance-weighted orthogonality (IWO), a metric that can do the measurement on disentanglement by investigating the magnitude of the projections from different subspaces onto each other. This is achieved by multiplying the basis matric of one subspace with a diagonal matrix that defines the importance of each dimension w.r.t. the other subspace.\nThey empirically evaluate IWO on multiple datasets that are commonly used in disentangled representation learning, and they show that their metric that can outperform prior metrics such as MIG or DCI-D.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThis paper is well-structured and clearly written. It is easy to understand what problem they try to tackle in this paper. Even though the metric study on disentangled representation learning is not a completely new field, I believe it is still worth thinking of how we evaluate the orthogonality between different subspaces that encode different factors of variation. \nIn their methodology, the authors provide detailed and sound math derivation on their LOA and IWO approach.\nWeaknesses:\nMy main concern is about the insufficiency of evaluation. Give that $\\beta$-TCVAE was a few years ago and there have been a large number of variants of VAEs that do disentangled representations, I would hope that the authors can implement a few more models for comparison. In addition, there are also very commonly used datasets that were not considered here, e.g. CelebA, Shape3D, Clevr, etc. I would like to see results on these more complex data.\nQuestions:\nI wonder why the $\\Delta$ L can be used to measure the importance. Could you justify it in more detail?\nIs the reason that you choose to only apply linear projection using $W_{1:L}$ is technical difficulty or indeed conceptual purpose?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors16 Nov 2023, 19:36 (modified: 16 Nov 2023, 19:39)EveryoneRevisions", "Content": "Comment:\nThank you for your constructive feedback. We have taken your comments into serious consideration and have made revisions to our manuscript accordingly.\nExpanded Model and Dataset Evaluation:\nAddition of SmallNORB and Further Models: In direct response to the reviews, we have expanded our evaluation to include the SmallNORB dataset. This addition, along with the inclusion of two more models - Factor VAE and $\\beta$-VAE - enriches our analysis and addresses the diversity of scenarios you highlighted. These new elements complement our existing use of Dsprites, Cars3D, Annealed VAE, and $\\beta$-TCVAE. In the camera-ready version of the paper, we plan to introduce three further variations of the DSprites dataset provided by disentanglement_lib.\nRationale for Dataset and Model Selection:\nConsistency with\ndisentanglement_lib\n: Our decision to use the Dsprites, Cars3D, SmallNORB datasets, and the selected models is grounded in their presence within\ndisentanglement_lib\nhttps://github.com/google-research/disentanglement_lib\nLocatello et al., 2019 [1]. This library is a benchmark in disentanglement research and offers unparalleled reproducibility and comparability. Utilizing these datasets and models minimizes biases that could arise from custom implementations or training procedures, thus providing a clear and objective evaluation of our proposed metric's efficacy.\nAdvantages of Established Benchmarks:\nEmploying resources from\ndisentanglement_lib\nensures that our study aligns with prevailing research standards, enhancing the reliability and validity of our findings. This approach also allows our results to be directly comparable with a broad spectrum of existing studies, which is crucial for contextualizing and validating our contributions.\nResponses to Questions:\nQ1 (Justification of $\\Delta \\mathcal{L}$ as an Importance Measure)\n: $\\Delta \\mathcal{L}_l$ measures how much the expected loss of regressing a generative factor increases when regressing from a subspace with dimension $l$ vs. $l-1$. It is therefore directly linked to the dimension which lies in the null-space of the projection from $l$ to $l-1$. As such, we use $\\Delta \\mathcal{L}_l$ to allocate the importance which that particular dimension (which is lost in the projection) plays for the generative factor in question.\n In light of your feedback, we have refined the explanation in our manuscript. Additionally we have adjusted the importance-weighting mechanism in IWO to better reflect the variation in overall model informativeness, following the insights from Eastwood et al., 2018 [1].\nQ2 (Use of Linear Projections)\n: Our choice to use linear projections was driven by both conceptual and technical considerations. Conceptually, linear projections are a natural fit for assessing orthogonality in latent spaces. Technically, their efficient implementation via Linear Neural Networks (LNNs) further motivated our choice.\nWe appreciate the opportunity to improve our paper based on your feedback. We hope these revisions address your concerns comprehensively.\n[1] Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of dis- entangled representations. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenRe- view.net, 2018."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 11:19Everyone", "Content": "Comment:\nDear Reviewer,\nAs the discussion period nears its conclusion, we wish to ensure that all your concerns have been adequately addressed. We would greatly appreciate any additional feedback you may have. Our goal is to finalize the rebuttal with the confidence that our manuscript reflects the valuable feedback provided by the review committee.\nThank you for your time and consideration,\nthe Authors"}]}, {"Heading": "Official Review of Submission9436 by Reviewer 1Jck", "Subheading": "Official ReviewbyReviewer 1Jck19 Oct 2023, 07:47 (modified: 17 Nov 2023, 06:54)EveryoneRevisions", "Content": "Summary:\nThe authors propose a novel metric for evaluating disentanglement of learned representations. The method consists of training a Linear Neural Network, essentially an MLP without nonlinearities with decreasing dimensionalities, for each ground truth factor. The objective function involves training (potentially non-linear) predictor heads on top of each hidden layer. Using QR decomposition on the learned NN weights together with loss estimates from each predictor, the authors estimate basis vectors for each ground-truth factor, together with their importance weightings. By computing such vectors for each g.t. factor they estimate both the subspaces in the learned latent space and compute a measure of orthogonality between subspaces for dfiferent g.t. factors.\nUsefulness of the metric is evaluated on both synthetic and real data.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nIWO can be used in scenarios where a ground truth factor can be aligned with exactly one latent dimension\nthe proposed metric actually correlates with downstream task performance\nWeaknesses:\nonly 2 datasets and models (as compared to e.g., Locatello et al. 2019) are compared in Section 4.3. Please consider using all the 7 datasets from\ndisentanglement_lib,\notherwise the choice seems a bit arbitrary\nFigure 1 is quite difficult to grasp. I understand that the concept is not trivial to present (and the caption is already lengthy), but maybe you could consider extending/rewriting the caption to make it clearer? Perhaps in a step wise manner (multiple figures). I find it crucial for conveying the idea of your paper. If you lack space I believe figure 3 could be compressed/removed instead\nQuestions:\nThe sub-optimal performance of Explicitness for the perfectly disentangled case could stem from overfitting. How do the authors handle this problem with their metric? What were the train/test splits used for the experiments? How sensitive is the metric to smaller sample sizes?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n8: accept, good paper\nConfidence:\n5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors16 Nov 2023, 19:55Everyone", "Content": "Comment:\nThank you for your constructive feedback. We have taken your comments into serious consideration and have made revisions to our manuscript accordingly.\nW1: Expansion of Dataset Usage\nInclusion of SmallNorb Dataset\n: In response to your suggestion, we have incorporated the SmallNorb dataset into our study. This dataset offers a significant variance from Dsprites and Cars3D, providing a more comprehensive evaluation of our metric.\nPlanned Inclusion of DSprites Variants\n: For the camera-ready version of our paper, we plan to introduce three further variations of the DSprites dataset available from disentanglement_lib. This addition will further broaden the scope of our evaluation and align with the diversity presented in Locatello et al. 2019. Note that Shapes3D is unfortunately not readily available from disentanglement_lib.\nW2: Clarity of Figure 1\nRevision of Figure 1\n: Following the feedback from multiple reviewers, we have replaced the original Figure 1 with a more straightforward representation in the main text, moving the original figure to the appendix. This change aims to simplify the initial understanding of our concept.\nEnhanced Caption and Explanation\n: As you suggested, we have elaborated on the figure's caption and provided a more detailed step-wise explanation.\nResponses to Specific Questions:\nQ1 (Handling of Overfitting)\n: To mitigate overfitting, we have employed a validation set, separate from our test set, for early stopping during model training. We have added this and further training details in\nAppendix B: Experimental Details\nQ2 (Train/Test Splits)\n: For our experiments, we reserved 15% of each dataset for testing purposes. We then used the test set for calculating the importances for the IWO metric, ensuring that our evaluations are based on unseen data. Note that the subspace basis are part of the model and learned during training.\nQ3 (Metric Sensitivity to Sample Sizes)\n: Recognizing the importance of this concern, we have added a dedicated experiment in\nAppendix B.4: IWO on Limited Data\n. We investigates the sensitivity of our metric to smaller sample sizes, providing valuable insights into its robustness and reliability in varied data conditions.\nWe appreciate the opportunity to improve our paper based on your feedback. We hope these revisions address your concerns comprehensively."}, {"Heading": "Official Comment by Reviewer 1Jck", "Subheading": "Official CommentbyReviewer 1Jck17 Nov 2023, 06:00Everyone", "Content": "Comment:\nThank you for updating the manuscript and the clarifications.\nI have two more comments:\nW1: Expansion of Dataset Usage:\nShapes3D is available in disentanglement_lib, you just need to download the dataset from\nhttps://github.com/google-deepmind/3d-shapes\n, but the code to use it is available in the library:\nhttps://github.com/google-research/disentanglement_lib/blob/master/disentanglement_lib/data/ground_truth/shapes3d.py\nQ3 (Metric Sensitivity to Sample Sizes)\n: is there a reason why experiments on smaller datasets are missing for the baseline methods? will you include these in the camera-ready version?"}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors17 Nov 2023, 06:37Everyone", "Content": "Comment:\nThank you for your prompt response.\nW1: Expansion of Dataset Usage\n: We apologize for any confusion caused by our previous comment. To clarify, in contrast to the other datasets, for Shapes3D no readily trained models are available for direct download. However, as you have pointed out, it is possible to train the models using the provided code. Because for all the other datasets pre-trained models are readily available for download, we prioritized them over Shapes3D. This enables resource efficient reproducibility of our results. However, recognizing the value of including Shapes3D, we commit to incorporating this dataset in the camera-ready version of our paper.\nQ3 (Metric Sensitivity to Sample Sizes)\n: These values are still missing because they are not readily available for download from disentanglement_lib. We are however actively working on procuring these results and will include them in the camera ready version of the paper."}, {"Heading": "Official Comment by Reviewer 1Jck", "Subheading": "Official CommentbyReviewer 1Jck17 Nov 2023, 06:53Everyone", "Content": "Comment:\nThank you again for the clarification. I will update the score accordingly."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors17 Nov 2023, 10:10Everyone", "Content": "Comment:\nThank you for acknowledging our clarifications and for your willingness to update the score. We greatly appreciate your thorough review and valuable feedback throughout this process."}]}]}, "7QlKLvfVge": {"paper_info": {"Primary Area": "societal considerations including fairness, safety, privacy", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "backdoor defense, backdoor attack, neuron pruning", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "Recent studies have indicated the effectiveness of neuron pruning for backdoor defense. In this work, we explore the limitations of pruning-based defense through theoretical and empirical investigations. We argue that pruning-based defense necessitates the removal of neurons that affect normal performance when the effect of backdoor is entangled across normal neurons. To address this challenge, we propose an extended neuron pruning framework, named \\emph{Directional Rank Reduction (\\method)}. \\method consists of three procedures: orthogonal transformation, pruning, and inverse transformation. Through the transformation of the feature space prior to pruning, \\method is able to focus the trigger effects on a limited number of neurons for more efficient pruning with less damage, outperforming existing pruning-based defense strategies. We implement \\method using Sarle's Bimodality Coefficient (SBC) which is optimized as the criterion for the transformation matrix based on the separability assumption of benign and poisoned features. Extensive experimental results demonstrate the superiority of our method. On average, our approach substantially reduces the ASR by 4.5x and increases the ACC by 1.45% compared with the recently strong baselines.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9434", "PDF Url": "https://openreview.net/pdf?id=7QlKLvfVge"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9434 by Area Chair ux2Z", "Subheading": "Meta ReviewbyArea Chair ux2Z05 Dec 2023, 15:50 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nIn this paper, the authors first explored the limitations of pruning-based defense through theoretical an empirical investigations and then proposed a Directional Rank Reduction method, a so-called extended neuron pruning framework, to address the limitations.\nThe authors have address the comments from the reviewers.\nHowever, after several rounds of discussions with Reviewer x3hw, he/she still is not satisfy with some responses.\nIn particular, the motivation or the explanation of the proposed method is questionable.\nThe authors should present the proposed method clearly without creating any possible misunderstnging!\nBesides, Reviewer niUP raised two concerns that the authors fail to give satisfying answers regarding\nModifying the weight matrix may cause a performance drop in many cases. How can your projection keep the performance?'' and\nThe proof needs to be more rigorous. Why use the consequence of the proof in the middle of the proof?''\nJustification For Why Not Higher Score:\nHowever, after several rounds of discussions with Reviewer x3hw, he/she still is not satisfy with some responses.\nIn particular, the motivation or the explanation of the proposed method is questionable.\nThe authors should present the proposed method clearly without creating any possible misunderstnging!\nBesides, Reviewer niUP raised two concerns that the authors fail to give satisfying answers regarding\nModifying the weight matrix may cause a performance drop in many cases. How can your projection keep the performance?'' and\nThe proof needs to be more rigorous. Why use the consequence of the proof in the middle of the proof?''\nJustification For Why Not Lower Score:\nnone"}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 05:32 (modified: 21 Nov 2023, 06:29)EveryoneRevisions", "Content": "Comment:\nWe thank all reviewers for the insightful feedback of our paper. We are glad that our\nnovelty\nand\nalgorithm\nare widely recognized by all reviewers. Most of the concerns are focused on\nthe presentation\nand\nthe insufficient experiments\nalong with some\nmisunderstandings\nof our method, which we have carefully responded in the following rebuttal. Particularly regarding the experimental section, we have significantly expanded our experimental validation, which is included in the revision of the paper.  We hope our newly added rebuttal material can address your concerns."}, {"Heading": "Official Review of Submission9434 by Reviewer 7hoR", "Subheading": "Official ReviewbyReviewer 7hoR05 Nov 2023, 04:15 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper proposes a rank reduction based defense against backdoor attack. Specifically, it first gives a feature-based objective to show the optimal solution to achieve the best defense effect. He then discussed the previous defense's problem based on the given objective and proposes DRR, the rank reduction based defense where aims to find a vector that would maximize the 3rd central moments of the mixed distribution. The proposed method have been verified in CIFAR10 with several backdoor methods. The result shows the proposed method could achieve a little better performance with the state-of-art defense.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nThe paper is well-written and easy to follow with only several typos.\nThe proposed method has some good theoretical analysis and could be meaningful for the future work.\nWeaknesses:\nSome of theoretical analysis might be not accurate. The utility function is defined using ||R-\\gamma_r (R)|| and also ||R||-||\\gamma_r (R)||. However, these two value is not strict equivalent. It also happens in the definition of E(R).\nIt is unclear why the 3rd center moment would show the best performance to measure the difference. In other words, would 2nd order moment or 1st order work as well? Since 3rd order is the main metric selected, the author should explain the choice in detail.\nThe experiment is pretty insufficient. It only covers one datasets with only one poisoning rate. I suggest the author to give a more comprehensive experiments to show their proposed method's effectiveness. Some standard setting in\nhttps://github.com/SCLBD/backdoorbench\nis recommended.\nMinor typo:\nMissing \\hat{x} in the definition of E(R(l).\nQuestions:\nPlease refer to the weaknesses part. To sum,\nWhy does ||R-\\gamma_r (R)|| =||R||-||\\gamma_r (R)|| along with  E(R)?\nWhy does 3rd central moment is selected?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 05:34 (modified: 21 Nov 2023, 05:36)EveryoneRevisions", "Content": "Comment:\nWe appreciate the reviewer's thorough review and have taken their comments into consideration. Here are our responses to their concerns:\nWeaknesses:\nW1: Some of theoretical analysis might be not accurate. The utility function is defined using $||R-\\gamma (R)||$ and also $||R||-||\\gamma (R)||$. However, these two value is not strict equivalent. It also happens in the definition of $E(R)$.\nAnswer: We thank the reviewer for pointing this out. Indeed, the equation $||R-\\gamma(R)||=||R||-\\gamma(R)||$ doesn't hold in the general case. However, it does hold when we use the proposed $L_{1, 1}$ norm, as defined in Definition 1. It makes sense if we specify the norm before introducing this equation. We acknowledge that the organization of this section needs to be corrected to avoid misleading the reader.\nW2: It is unclear why the 3rd center moment would show the best performance to measure the difference. In other words, would 2nd order moment or 1st order work as well? Since 3rd order is the main metric selected, the author should explain the choice in detail.\nAnswer: We clarified our choice in the revision of the paper. The first central moment (the mean of the data), doesn't make sense in this context. Because the mean only affects the position of the data center, which isn't related to the direction of the mean difference. The second central moment yields similar conclusions when it is constrained with the assumptions made in the paper. However, in practice, we find the third central moment works much better. To some extent, the third central moment not only increases with the mean difference but also with the asymmetry of the two clusters. In backdoor attacks, the amount of benign data is usually much larger than poisoned data, which cannot be captured by the second central moment. That's why the third central moment performs better than the second central moment in our scenarios.\nW3: The experiment is pretty insufficient. It only covers one datasets with only one poisoning rate. I suggest the author to give a more comprehensive experiments to show their proposed method's effectiveness. Some standard setting in\nhttps://github.com/SCLBD/backdoorbench\nis recommended.\nAnswer: Following the reviewer's suggestion, we conducted additional experiments on GTSRB. The results indicate that our method performs well in this context too. It's noteworthy that our attack with CLA on GTSRB didn't succeed, which is also the case in the BackdoorBench.\nBackdoored\nFP\nANP\nEP\nCLP\nDRR\nBackdoored\nFP\nANP\nEP\nCLP\nDRR\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nResNet-18\nBadNets\n95.28\n100.00\n91.74\n0.35\n91.10\n4.64\n94.49\n0.33\n94.64\n0.79\n94.93\n0.68\nBadNets(A2A)\n95.31\n95.94\n88.56\n10.85\n94.13\n1.79\n94.74\n0.06\n94.79\n0.13\n95.11\n0.48\nBlended\n95.87\n99.88\n90.44\n3.00\n91.91\n3.20\n94.92\n0.29\n94.68\n0.97\n95.04\n0.79\nCLA\n96.29\n0.09\n90.82\n0.61\n96.14\n0.80\n95.53\n0.13\n94.32\n0.14\n96.21\n0.10\nAverage\n95.69\n73.98\n90.39\n3.70\n93.32\n2.61\n94.92\n0.20\n94.61\n0.51\n95.32\n0.51\nWideResNet-28-1\nBadNets\n94.37\n99.99\n90.86\n11.94\n80.70\n100.00\n93.90\n0.33\n87.09\n11.11\n93.00\n0.45\nBadNets(A2A)\n92.27\n91.72\n89.37\n32.90\n75.48\n48.18\n90.56\n1.01\n92.13\n0.97\n90.70\n1.71\nBlended\n94.68\n99.75\n90.51\n11.35\n86.06\n99.52\n92.86\n10.26\n93.96\n0.26\n92.82\n3.07\nCLA\n95.31\n0.09\n89.09\n0.71\n94.98\n0.50\n94.57\n0.10\n95.27\n0.20\n95.41\n0.09\nAverage\n94.16\n72.89\n89.96\n14.23\n84.31\n62.05\n92.97\n2.93\n92.11\n3.14\n92.98\n1.33"}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 05:35Everyone", "Content": "Comment:\nWe also provide experimental results for different poisoning ratios:\n1%:\nBackdoored\nFP\nANP\nEP\nCLP\nDRR\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nResNet-18\nBadNets\n94.83\n98.79\n88.44\n95.16\n92.74\n3.02\n93.95\n5.00\n93.04\n1.03\n94.69\n0.97\nBadNets(A2A)\n94.81\n86.52\n89.19\n9.97\n91.35\n5.32\n94.15\n8.90\n93.68\n0.82\n94.52\n0.66\nBlended\n95.03\n99.99\n89.61\n100.00\n93.63\n1.68\n91.18\n99.73\n93.03\n35.66\n94.88\n1.84\nCLA\n94.82\n16.18\n89.94\n9.84\n92.08\n9.26\n94.43\n16.80\n91.14\n1.22\n94.86\n6.34\nAverage\n94.87\n75.37\n89.30\n53.74\n92.45\n4.82\n93.43\n32.61\n92.72\n9.68\n94.74\n2.45\nWideResNet-28-1\nBadNets\n92.46\n99.92\n86.11\n40.66\n73.90\n59.50\n90.93\n68.04\n90.97\n45.56\n90.74\n4.41\nBadNets(A2A)\n92.43\n79.94\n86.25\n2.45\n75.37\n15.96\n86.49\n25.92\n91.68\n1.32\n91.46\n1.47\nBlended\n92.55\n99.80\n84.74\n99.34\n83.70\n16.33\n89.43\n95.73\n88.81\n99.81\n89.89\n0.70\nCLA\n92.39\n3.49\n85.49\n23.51\n90.89\n3.32\n90.93\n4.49\n92.29\n3.52\n92.39\n3.49\nAverage\n92.46\n70.79\n85.65\n41.49\n80.97\n23.78\n89.45\n48.55\n90.94\n37.55\n91.12\n2.52\n5%:\nBackdoored\nFP\nANP\nEP\nCLP\nDRR\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nResNet-18\nBadNets\n94.14\n99.99\n87.31\n100.00\n87.92\n1.76\n93.60\n2.73\n93.05\n1.92\n93.90\n1.58\nBadNets(A2A)\n94.18\n84.71\n88.13\n37.80\n94.18\n84.71\n93.28\n12.14\n92.97\n1.02\n93.76\n0.94\nBlended\n94.81\n100.00\n88.42\n98.11\n93.23\n6.03\n93.50\n64.18\n90.00\n0.18\n94.01\n1.73\nCLA\n94.86\n89.94\n88.66\n28.94\n90.18\n50.23\n92.28\n3.10\n91.62\n1.88\n94.78\n0.94\nAverage\n94.50\n93.66\n88.13\n66.21\n91.38\n35.68\n93.17\n20.54\n91.91\n1.25\n94.11\n1.30\nWideResNet-28-1\nBadNets\n92.22\n99.99\n86.41\n30.49\n87.38\n48.26\n84.29\n10.30\n92.14\n5.92\n90.14\n1.51\nBadNets(A2A)\n92.17\n91.26\n84.06\n74.53\n84.24\n77.23\n91.65\n1.53\n91.87\n1.45\n92.06\n1.70\nBlended\n91.72\n93.76\n84.66\n3.23\n83.93\n2.23\n89.27\n4.39\n89.80\n2.06\n89.89\n0.70\nCLA\n92.87\n36.23\n85.49\n23.51\n86.43\n5.09\n85.15\n7.87\n90.23\n1.90\n91.44\n4.63\nAverage\n92.25\n80.31\n85.16\n32.94\n85.50\n33.20\n87.59\n6.02\n91.01\n2.83\n90.88\n2.14\nand new attacks:\nBackdoored\nFP\nANP\nEP\nCLP\nDRR\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nResNet-18\nAdaptiveBlend\n94.79\n100.00\n89.47\n5.29\n82.18\n0.30\n94.43\n1.74\n93.68\n33.52\n90.25\n3.79\nSIG\n94.01\n98.22\n88.94\n45.70\n89.38\n2.36\n87.36\n30.84\n89.75\n94.28\n87.10\n0.07\nSmotth\n94.59\n100\n87.12\n100\n92.65\n81.23\n94.24\n3.99\n87.24\n89.03\n94.03\n3.58\nWideResNet-28-1\nAdaptiveBlend\n92.37\n100.00\n84.77\n51.88\n82.06\n42.70\n90.45\n5.18\n84.40\n74.57\n91.13\n0.86\nSIG\n84.03\n96.20\n82.65\n5.22\n81.37\n0.00\n83.82\n0.00\n84.04\n0.00\n82.85\n0.00\nSmotth\n92.19\n100\n84.52\n6.32\n89.98\n100\n91.45\n8.78\n91.29\n9.03\n91.88\n2.74\nThe results of our experiments indicate that the performance of our proposed method exhibits robustness across a diverse scenario."}]}, {"Heading": "Official Review of Submission9434 by Reviewer niUP", "Subheading": "Official ReviewbyReviewer niUP31 Oct 2023, 10:42 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper presents a fascinating new method for backdoor defense in neural networks. The key idea of projecting the \"toxic direction\" that maximizes the difference between clean and poisoned features is novel and seems promising.\nThe theoretical analysis provides valuable insights into the limitations of standard neuron pruning approaches. Framing the problem as rank reduction along arbitrary directions rather than fixed neuron directions is a significant conceptual shift.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n3 good\nStrengths:\nThe idea of maximizing the third central moment is enjoyable. This idea yields a novel insight.\nThe connection between neuron pruning and rank reduction is also an exciting topic.\nThe visualization of the separation constant C provides good justification for the theoretical assumptions.\nWeaknesses:\nMore experiments can be conducted (BadNet, Blended, CLA, WaNet, and IAB are insufficient.) The authors can consider attacks like SIG [1] and low frequency (Smooth) [2]. Since your method also took latent separability as an assumption, Adapt-blend and Adapt-patch attacks [3] should also be considered. Evaluating robustness to adaptive attacks that try to evade the defense would be useful to understand limitations.\nThe references and notations should be clarified. For example, what is the reference to Proposition 1?\nAlso, the readability and organization of this paper need to be improved. It is better if an algorithm is provided.\n[1] A new backdoor attack in cnns by training set corruption ICIP 2019\n[2] Rethinking the Backdoor Attacks\u2019 Triggers: A Frequency Perspective ICCV2021\n[3] Revisiting the Assumption of Latent Separability for Backdoor Defenses, ICLR 2023\nQuestions:\nThe memory and computational complexity could be analyzed more thoroughly, especially how the approach scales with larger datasets/models. Are there ways to make the optimization more efficient?\nHow many extension directions v_i have you used?\nModifying the weight matrix may cause a performance drop in many cases. How can your projection keep the performance?\nThe proof needs to be more rigorous. Why use the consequence of the proof in the middle of the proof?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 05:38 (modified: 22 Nov 2023, 04:10)EveryoneRevisions", "Content": "Comment:\nWe would like to thank the reviewer for the detailed review. We will make changes based on the feedback. Please see our responses:\nWeaknesses:\nW1: More experiments can be conducted (BadNet, Blended, CLA, WaNet, and IAB are insufficient.) The authors can consider attacks like SIG [1] and low frequency (Smooth) [2]. Since your method also took latent separability as an assumption, Adapt-blend and Adapt-patch attacks [3] should also be considered. Evaluating the robustness of adaptive attacks that try to evade the defense would be useful for understanding limitations.\nAnswer: We've conducted experiments according to the reviewer's suggestions. The results are shown below:\nBackdoored\nFP\nANP\nEP\nCLP\nDRR\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nResNet-18\nAdaptiveBlend\n94.79\n100.00\n89.47\n5.29\n82.18\n0.30\n94.43\n1.74\n93.68\n33.52\n90.25\n3.79\nSIG\n94.01\n98.22\n88.94\n45.70\n89.38\n2.36\n87.36\n30.84\n89.75\n94.28\n87.10\n0.07\nSmotth\n94.59\n100\n87.12\n100\n92.65\n81.23\n94.24\n3.99\n87.24\n89.03\n94.03\n3.58\nWideResNet-28-1\nAdaptiveBlend\n92.37\n100.00\n84.77\n51.88\n82.06\n42.70\n90.45\n5.18\n84.40\n74.57\n91.13\n0.86\nSIG\n84.03\n96.20\n82.65\n5.22\n81.37\n0.00\n83.82\n0.00\n84.04\n0.00\n82.85\n0.00\nSmotth\n92.19\n100\n84.52\n6.32\n89.98\n100\n91.45\n8.78\n91.29\n9.03\n91.88\n2.74\nNote that the latent separability mentioned in the paper [3] only considers the latent space in the penultimate layer, while our methods utilize the separability within each layer of the model. This makes our method effective even when the penultimate layer feature is inseparable.\nW2: The references and notations should be clarified. For example, what is the reference to Proposition 1?\nAnswer: The reviewer's request for clarification on the references and notations is acknowledged. However, regarding the reference for Proposition 1, it should be noted that to the extent of our understanding, Proposition 1 is introduced for the first time in our manuscript. Consequently, there are no prior publications to cite for this proposition.\nW3: Also, the readability and organization of this paper need to be improved. It is better if an algorithm is provided.\nAnswer: We appreciate the feedback regarding the clarity and structural aspects of our manuscript. Recognizing the value that an algorithmic representation would add, we have taken the suggestion into consideration and will include a detailed algorithm in the revised draft to enhance comprehension of our proposed method."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 05:38Everyone", "Content": "Comment:\nQuestions:\nQ1: The memory and computational complexity could be analyzed more thoroughly, especially how the approach scales with larger datasets/models. Are there ways to make the optimization more efficient?\nAnswer: We appreciate the inquiry regarding the computational efficiency of our approach, particularly in the context of scalability to larger datasets and models. Our method indeed incorporates strategies to enhance optimization efficiency.\nPrimarily, the optimization process can be parallelized across different network layers, offering a substantial decrease in the required time for optimization. Furthermore, in scenarios with high feature dimensions or large dataset sizes, dimensionality reduction through Principal Component Analysis (PCA) can be employed prior to the directional learning phase. This step effectively reduces the computational burden. Subsequently, the learned direction within this reduced space is projected back onto the original space, thereby economizing on memory and computational demands while preserving the integrity of the optimization process.\nQ2: How many extension directions v_i have you used?\nAnswer: Across all experiments conducted, we use only 1 direction.\nQ3: Modifying the weight matrix may cause a performance drop in many cases. How can your projection keep the performance?\nAnswer: The reviewer has raised an important point. As we point out in our paper, rank reduction is an extension of neuron pruning (which is also a modification of the weight matrix). Established neuron pruning techniques, such as CLP and EP, have been demonstrated to maintain model performance effectively post-pruning. Our rank reduction, which 1) remove only one rank (instead of multiple ranks in neuron pruning) and 2) remove more targetedly to the direction that related to the backdoor behavior should intuitively affect less to the performance than pruning methods compared to general pruning methods. However, the exact relationship between the way we modify the weight matrices, and the performance is hard to clearly describe and can only be shown by experiments. Here we only provide an intuitive explanation.\nQ4: The proof needs to be more rigorous. Why use the consequence of the proof in the middle of the proof?\nAnswer: We are grateful to the reviewer for highlighting a critical aspect of our proof's construction. To address this issue, we are committed to revising and strengthening our theorem to ensure its logical soundness and rigor. The revised version of our paper has included these modifications in detail. Please kindly refer to it."}]}, {"Heading": "Official Review of Submission9434 by Reviewer 1xjT", "Subheading": "Official ReviewbyReviewer 1xjT30 Oct 2023, 23:15 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposes a novel backdoor defense method, which utilizes rank reduction to mitigate backdoor in the model. The idea of rank reduction is interesting and brings a new insight into the area.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nThe idea is novel and provides a new insight.\nThis paper is technically sound and easy to follow.\nThe experimental results demonstrate its effectiveness in backdoor defense.\nWeaknesses:\n1.Although this work is interesting, it has a limitation. This paper assumes the defender can get access to the backdoored image. However, this is hard to get in actual situations and thus limits its use greatly. I wonder whether it works without these backdoored data.\n2. The backdoor attacks that this paper test is not enough. I suggest the authors to test the newest input-specific backdoor attacks in 2022. It's important to identify whether this method can achieve SOTA.\nQuestions:\n1.Does it work without the attacker's backdoored data?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 05:40Everyone", "Content": "Comment:\nWe would like to thank the reviewer for their appreciation of our work. Below, we have provided our response to the reviewer's concerns.\nWeakenesses:\nW1: Although this work is interesting, it has a limitation. This paper assumes the defender can get access to the backdoored image. However, this is hard to get in actual situations and thus limits its use greatly. I wonder whether it works without these backdoored data.\nAnswer: First, to answer the reviewer's final question, the rank reduction framework can be adpated to\nany\nscenarios with or\nwithout\nbackdoored data, but the metric we adopt to obtain the direction, i.e., third central moment, requires the access to the backdoored data. If other metrics is later being invented to obtain the direction, then the method could be backdoored data-free.\nSecond, we want to argue that the scenario in which the defender has access to the full dataset is a prevalent assumption within the backdoor attack research community, exemplified by the concept of adopting a third-party dataset as discussed in [1]. Some of methodologies that employ this setting are outlined in: [2, 3, 4, 5].\n[1] Li, Y., Jiang, Y., Li, Z. and Xia, S.T., 2022. Backdoor learning: A survey. IEEE Transactions on Neural Networks and Learning Systems.\n[2] Chen, B., Carvalho, W., Baracaldo, N., Ludwig, H., Edwards, B., Lee, T., Molloy, I. and Srivastava, B., Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering.\n[3] Zheng, R., Tang, R., Li, J. and Liu, L., 2022. Pre-activation Distributions Expose Backdoor Neurons. Advances in Neural Information Processing Systems, 35, pp.18667-18680.\n[4] Tran, B., Li, J. and Madry, A., 2018. Spectral signatures in backdoor attacks. Advances in neural information processing systems, 31.\n[5] Hayase, J., Kong, W., Somani, R. and Oh, S., 2021, July. Spectre: Defending against backdoor attacks using robust statistics. In International Conference on Machine Learning (pp. 4129-4139). PMLR.\nW2: The backdoor attacks that this paper test is not enough. I suggest the authors to test the newest input-specific backdoor attacks in 2022. It's important to identify whether this method can achieve SOTA.\nAnswer: Actually, the experiments with IAB and WaNet, which are both input-specific backdoor attacks, are in the paper. Please refer to Table 2 in our submitted paper to see our results tested on input-aware dynamic attack (IAB) and Warping-based backdoor attack (WaNet).\nMoreover, we conduct more experiments on other types of attacks (AdaptiveBlend, SIG and Smooth), where the results is as shown below:\nBackdoored\nFP\nANP\nEP\nCLP\nDRR\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nResNet-18\nAdaptiveBlend\n94.79\n100.00\n89.47\n5.29\n82.18\n0.30\n94.43\n1.74\n93.68\n33.52\n90.25\n3.79\nSIG\n94.01\n98.22\n88.94\n45.70\n89.38\n2.36\n87.36\n30.84\n89.75\n94.28\n87.10\n0.07\nSmotth\n94.59\n100\n87.12\n100\n92.65\n81.23\n94.24\n3.99\n87.24\n89.03\n94.03\n3.58\nWideResNet-28-1\nAdaptiveBlend\n92.37\n100.00\n84.77\n51.88\n82.06\n42.70\n90.45\n5.18\n84.40\n74.57\n91.13\n0.86\nSIG\n84.03\n96.20\n82.65\n5.22\n81.37\n0.00\n83.82\n0.00\n84.04\n0.00\n82.85\n0.00\nSmotth\n92.19\n100\n84.52\n6.32\n89.98\n100\n91.45\n8.78\n91.29\n9.03\n91.88\n2.74"}]}, {"Heading": "Official Review of Submission9434 by Reviewer X3hw", "Subheading": "Official ReviewbyReviewer X3hw30 Oct 2023, 17:08 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper argues that existing pruning-based defense methods can be ineffective at times and introduces Directional Rank Reduction (DRR) to identify toxic directions. In this study, the method approximates the target direction by maximizing the third central moment, supported by rigorous theoretical justification, and constructs a projection matrix to eliminate the toxic direction. DRR demonstrated outstanding performance in terms of both accuracy (ACC) and adversarial success rate (ASR).\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThis study shows an interesting finding that the backdoor trigger effects are not always aligned with fixed dimensions of the feature space, pruning-based methods are usually ineffective.\nThe proposed DRR method performed well on both ACC and ASR compared to other methods.\nWeaknesses:\nIn the first equation on Page 3, it seems feasible to do the defense by reducing the norm of the residual matrix to align the benign and poisoned features seems feasible. The features from benign examples move towards the backdoored features. Does the movement hurt the model's clean performance?\nThe last equation on Page 4 has a strong assumption that all the clean examples are centered around the mean of them. Namely, the method assumes that the distances from all the clean examples to the example center are the same. The examples marked as yellow in Figure 1 are distributed like a circle. However, the real-world data distribution often deviates from the assumption. The distribution could be elliptical-like. In this case, the obtained v is not optimal anymore.\nIn the third row of Table 2, DRR achieves a better trade-off. Why it demonstrates a higher accuracy (ACC) instead of a lower ASR?\nThis approach requires the optimization of a vector in each layer, which could be expensive.\nminor: All the equations are not numbered!\nQuestions:\n\"How the direction vector v is initialized in the paper, and do different initialization methods lead to varying results?\nIn Figure 2, the value of C for certain layers is not significant. Is it possible to skip some layers when computing v?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 07:16Everyone", "Content": "Comment:\nWeakenesses:\nW1: In the first equation on Page 3, it seems feasible to do the defense by reducing the norm of the residual matrix to align the benign and poisoned features seems feasible. The features from benign examples move towards the backdoored features. Does the movement hurt the model's clean performance?\nAnswer: As we points out in our paper, rank reduction is an extension of neuron pruning (which is also a modification of the weight matrix). Established neuron pruning techniques, such as CLP and EP, have been demonstrated to maintain model performance effectively post-pruning. Our rank reduction, which 1) remove only one rank (instead of multiple ranks in neuron pruning) and 2) remove more targetedly to the direction that related to the backdoor behavior should intuitively affect less to the performance than pruning methods in general. Empirically, we show that our method affects the less on the performance compare to the previous method.\nW2: The last equation on Page 4 has a strong assumption that all the clean examples are centered around the mean of them. Namely, the method assumes that the distances from all the clean examples to the example center are the same. The examples marked as yellow in Figure 1 are distributed like a circle. However, the real-world data distribution often deviates from the assumption. The distribution could be elliptical-like. In this case, the obtained v is not optimal anymore.\nAnswer: We appreciate the opportunity to clarify a potential misunderstanding highlighted by the reviewer regarding the assumptions underlying our method. Contrary to the reviewer's interpretation, our approach does not assume uniform distances of all clean examples from their mean, nor does it presuppose a circular distribution of data points as depicted in Figure 1. The shape of the ellipse in a multivariate Gaussian distribution is determined by its covariance matrix. Only when the covariance matrix is\nisotropic\n, which means the variance along all directions are the same, the data is distributed like a circle as mentioned by the reviewer. However, we\ndo not\nput any constraints on whether the covariance is isotropic or not. The only assumption about the covariance matrix we made is assumption 2, which limits the maximum variance of the data distribution. Hence, an elliptical-like distribution is considered in our method, where the Gaussian distribution has a covariance matrix that is not isotropic.\nThe equation on page 4, which the reviewer refers to, formulates an optimization problem. The objective of this problem is to determine an optimal unit direction vector that maximizes the third central moment when data is projected onto this vector. The stipulation that the vector be of unit length is a constraint applied to the direction vector itself, rather than to the data samples. This is a standard practice in such optimization problems to ensure the direction vector is normalized and hence, the focus is on the direction rather than the magnitude.\nW3: In the third row of Table 2, DRR achieves a better trade-off. Why it demonstrates a higher accuracy (ACC) instead of a lower ASR?\nAnswer: We appreciate the reviewer's insightful observation and recognize the necessity of providing a clearer explanation in our manuscript. The comparatively minimal impact on model performance observed in our study can be attributed to our method's strategy of removing at most one rank per layer. This approach is considerably more conservative than traditional neuron pruning methods, which often entail the removal of multiple ranks. If the number of ranks is tuned to more according to specific scenario, DRR can achieve even lower ASR.\nW4: This approach requires the optimization of a vector in each layer, which could be expensive.\nAnswer: We appreciate the inquiry regarding the computational efficiency of our approach.\nPrimarily, the optimization process can be parallelized across different network layers, offering a substantial decrease in the required time for optimization. Furthermore, in scenarios with high feature dimensions or large dataset sizes, dimensionality reduction through Principal Component Analysis (PCA) can be employed prior to the directional learning phase. This step effectively reduces the computational burden. Subsequently, the learned direction within this reduced space is projected back onto the original space, thereby economizing on memory and computational demands while preserving the integrity of the optimization process."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors21 Nov 2023, 07:16Everyone", "Content": "Comment:\nQuestions:\nQ1: How the direction vector v is initialized in the paper, and do different initialization methods lead to varying results?\nAnswer: The reviewer's comment is well-taken. Within our manuscript, we initialize the vector v as the standard basis vector that corresponds to the maximal third central moment upon projection of the data. To thoroughly examine the impact of various initialization strategies, we have implemented two distinct approaches: (1) random initialization, and (2) initialization using the original standard basis vector that exhibits the highest third central moment. The findings from these methodologies are presented in the following section:\nBackdoored\nDRR (random)\nDRR (original)\nACC\nASR\nACC\nASR\nACC\nASR\nResNet-18\nBadNets\n94.83\n98.79\n92.20\n1.87\n92.01\n3.18\nBadNets(A2A)\n94.81\n86.52\n93.11\n1.30\n92.42\n1.93\nBlended\n95.03\n99.99\n90.57\n3.42\n94.40\n2.31\nCLA\n94.82\n16.18\n93.60\n0.96\n93.40\n0.98\nAverage\n94.87\n75.37\n92.37\n1.89\n93.06\n2.10\nWideResNet-28-1\nBadNets\n92.46\n99.92\n89.97\n2.34\n89.56\n1.69\nBadNets(A2A)\n92.43\n79.94\n88.60\n1.80\n91.94\n1.29\nBlended\n92.55\n99.80\n91.32\n100.00\n91.50\n2.74\nCLA\n92.39\n3.49\n90.57\n1.07\n91.41\n6.19\nAverage\n92.46\n70.79\n90.12\n26.30\n91.10\n2.98\nThe results indicate that different intialization work well in different scenarios. However, in general, initializing the vector with original standard basis with the highest criterion performs more robust than random initialization.\nQ2: In Figure 2, the value of C for certain layers is not significant. Is it possible to skip some layers when computing v?\nAnswer: The reviewer's suggestion is indeed a compelling proposition. In this way we can reduce the computational cost and the hurt to the normal performance of the model. However, currently the exact calculation of C requires the access to both poisoned data and benign data separately, which is impractice in our scenario. Nonetheless, we acknowledge the potential benefits of such a strategy and consider it an intriguing avenue for future exploration."}, {"Heading": "Response to rebuttal", "Subheading": "Official CommentbyReviewer X3hw01 Dec 2023, 06:01 (modified: 15 Mar 2024, 00:27)EveryoneRevisions", "Content": "Comment:\nThank the authors for providing a detailed rebuttal. However, I still have concerns regarding the motivation of the method.\nFigure 1 illustrates why the proposed method works, which is only valid when the data distribution is circle-like. In real-world datasets, the distribution is not the case. Namely, the motivation or the explanation of the proposed method is not convincing anymore.\nI understand that the authors do not make any assumptions about the data explicitly. But please check the motivation illustrated in Figure 1. The illustration does not make sense for real-world data distribution.\nHence, I tend to keep my original score."}]}]}, "yacRhge4zQ": {"paper_info": {"Primary Area": "societal considerations including fairness, safety, privacy", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "privacy, fairness, regulation, game", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "Existing work on trustworthy machine learning (ML) often focuses on a single aspect of trust in ML (e.g., fairness, or privacy) and thus fails to obtain a holistic trust assessment. Furthermore, most techniques often fail to recognize that the parties who train models are not the same as the ones who assess their trustworthiness. We propose a framework that formulates trustworthy ML as a multi-objective multi-agent optimization problem to address these limitations. A holistic characterization of trust in ML naturally lends itself to a game theoretic formulation, which we call regulation games. We introduce and study a particular game instance, the SpecGame, which models the relationship between an ML model builder and regulators seeking to specify and enforce fairness and privacy regulations. Seeking socially optimal (i.e., efficient for all agents) solutions to the game, we introduce ParetoPlay. This novel equilibrium search algorithm ensures that agents remain on the Pareto frontier of their objectives and avoids the inefficiencies of other equilibria. For instance, we show that for a gender classification application, the achieved privacy guarantee is 3.76\u00d7 worse than the ordained privacy requirement if regulators do not take the initiative to specify their desired guarantees first. We hope that our framework can provide policy guidance.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "Supplementary Material": "zip", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9431", "PDF Url": "https://openreview.net/pdf?id=yacRhge4zQ"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9431 by Area Chair eBqQ", "Subheading": "Meta ReviewbyArea Chair eBqQ11 Dec 2023, 12:26 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThis paper studies multi-agent multi-objective modeling in the context of trustworthy machine learning (ML). All reviewers agree that this paper is aligned with an interesting direction of trustworthy ML. However,  a key concern raised by all reviewers is the assumption of a pre-calculated Pareto Frontier (PF) and its practical feasibility. We believe this was not fully addressed in the response. The reviewers consistently mention issues with clarity and consistency in the paper.  The reviewers also raised questions about the experimental setup. The authors should consider providing a more detailed description of the experimental setup, including the rationale behind the choice of hyperparameters in subsequent revisions.\nJustification For Why Not Higher Score:\nThe paper does not meet the bar for acceptance.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "General Author Response", "Subheading": "Official CommentbyAuthors17 Nov 2023, 11:10Everyone", "Content": "Comment:\nWe thank the reviewers wholeheartedly for their detailed comments and constructive feedback.\nWe are glad that all the reviewers found our problem (trustworthy ML) important and our game theoretic formulation of it novel and interesting.\nIn particular, reviewers found the framework intuitive, general and amenable to modeling a wide set of problems in trustworthy ML; and our proposed instantiation of it, SpecGame, to be well-defined.\nOne reviewer highlighted the importance of discovering the trade-offs among different trustworthy criteria and  found our theoretical and empirical analysis of agent interaction adopting these criteria to be useful for understanding the possible fundamental limitations of new trustworthy ML algorithms.\nNew Manuscript and Color Codings.\nWe have applied reviewer's comments within the manuscript and have highlighted newly added parts in \"blue\" and fixed typos in \"red\". In order to fit the new manuscript within 9 pages we have also shortened the last contribution point which is marked in \"teal\".\nWe will address reviewers shared concern regarding the assumption of a shared Pareto frontier, and new policy developments since submission that we believe lends more credence to it.\nAssumption of Shared Pareto Frontier and Information Symmetry\nThe reviewers expressed concerns regarding  the assumption of a shared Pareto frontier. We think, however, that the root cause of these concerns is the\ninformation asymmetry\nthat inevitably exists between regulators and model builders regardless of the modeling assumptions.\nWe addressed these concerns from a technical standpoint in Section J with\nan experiment where regulators and model builders have access to completely different datasets under the same task\nand showed that ParetoPlay converges and can still recover useful equilibria. Furthermore, we provided guidance for future work on adding cryptographic zero-knowledge-proof (ZKP) of fairness and privacy interventions to ParetoPlay which would circumvent the information asymmetry issue.\nFrom a policy point of view, since the submission of our work, there has been important developments in the ML Regulation space that beckons a future where the assumption of information symmetry is more realistic.\nRecent policy development makes the Information Symmetry more pertinent\nOn October 30, 2023, the US administration has issued an\nExecutive Order\nwhich has called on \"independent regulatory agencies\" (i.e., our regulators) to  \"emphasize and clarify\" responsibilities of the \"regulated entities\" (i.e. our model builders) regarding the fairness and privacy of their models:\nSection 8.b.(i):\n(C)\u00a0 incorporation of equity principles in AI-enabled technologies used in the health and human services sector, using disaggregated data on affected populations and\nrepresentative population data sets when developing new models, monitoring algorithmic performance against discrimination and bias in existing models\n, and helping to identify and mitigate discrimination and bias in current systems;\n(D)\nincorporation of safety, privacy, and security standards into the software-development lifecycle\nfor protection of personally identifiable information, including measures to address AI-enhanced cybersecurity threats in the health and human services sector;\nSince then, the issue of safety and ML trustworthiness has become ever more prominent in the public sphere. For instance,\nPresident Obama in a recent interview said\nI think it's entirely appropriate then for us to plant a flag and say, \"...frontier companies, you need to disclose what your safety protocols are... Tell us what tests you're using. Make sure that we have some independent verification that right now this stuff is working.\"\nIn summary, although these are early days for ML Regulation, from a policy point of view our assumption of information symmetry, whether enforced through transparency laws or audits by a\nnew agency\nseems increasingly closer to reality and we believe this makes our submission even more timely than before.\nWe are happy to further engage with reviewers as part of the discussion phase and hope that the reviewers consider raising their scores.", "Replies": [{"Heading": "Thank you for your feedback. Please consider our rebuttal responses.", "Subheading": "Official CommentbyAuthors21 Nov 2023, 18:31Reviewers Submitted, Program Chairs, Everyone", "Content": "Comment:\nDear reviewers,\nThank you for your valuable feedback. We believe it has improved our submission.  Given that the author interaction window is coming to an end, we would like to kindly ask you to consider the rebuttal responses and let us know if they address your concerns and, if they do, to consider raising your scores.\nIt would be our pleasure to answer any further questions."}]}, {"Heading": "Official Review of Submission9431 by Reviewer SMAr", "Subheading": "Official ReviewbyReviewer SMAr06 Nov 2023, 16:53 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper studied the problem of multi-agent and multi-objective machine learning (ML) regulation games where there exist separate regulators enforcing privacy and fairness constraints on the learning model. Prior work in trustworthy ML often implicitly assumes that a single entity is in charge of implementing these different objectives, which is not realized in practice. The authors instead proposed SpecGame, a general framework for ML regulation games between three agents: model builder, fairness regulator, and privacy regulator. Since the agents' privacy loss is difficult to estimate in post-processing, the authors also proposed ParetoPlay, i.e. using a pre-calculated Pareto frontier as common knowledge among all agents, to simulate the interactions between agents in a SpecGame and recover equilibria points. Finally, the authors provided experimental results to show the suboptimality of the single-agent model in trustworthy ML and answer questions on how the regulators can achieve the desired equilibrium by changing their incentives after the game has converged.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n3 good\nStrengths:\nStrengths:\nThe proposed model of multi-agent multi-objective is novel and interesting to study.\nThe proposed scenario of SpecGame is well-defined and makes sense.\nWeaknesses:\nWeaknesses:\nThe assumption of a pre-calculated Pareto frontier as common knowledge does not have theoretical proofs and the discussion around the empirical evaluation in Appendix J is lacking.\nThroughout the main body, the authors sometimes refer to notations that were not defined previously. For example, in Section 3.2, notation c_i^{(t)} and L_i^{(t)} are the first time the \"(t)\" superscript is used. In Equation 4, the notation \\nabla_s is used without a definition.\nThe discussion of the experiments in Section 5 is confusing. At the end of Section 1, the author claimed that the experiments would highlight the suboptimality of studying trustworthy ML in a single-agent framework. However, in Section 5, the first research question shows that multi-agent setup leads to sub-optimalities.\nFigure 1 does not have a legend. Overall, most figures are hard to read and the captions do not provide sufficient description of the experiments.\nMinor typos: \\ell_build(w) instead of \\ell_b(w) on page 5 under Equation 3.\nQuestions:\nCan the authors clarify the assumption of a pre-calculated Pareto frontier as common knowledge?\nCan the authors clarify the discrepancy between RQ1 and the contribution claimed at the end of Section 1?\nCan the authors provide a more detailed description of the experiments in Section 5, as well as the reasoning for choosing the hyperparameters described in Appendix I? Particularly, the step size discount factor and the loss function weightings \\lambda_fair and \\lambda_priv?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Weaknesses", "Subheading": "Official CommentbyAuthors17 Nov 2023, 11:20Everyone", "Content": "Comment:\nWe thank the reviewer for their feedback. Below we address each point raised in \"Weaknesses\" section.\nThe assumption of a pre-calculated Pareto frontier as common knowledge does not have theoretical proofs and the discussion around the empirical evaluation in Appendix J is lacking.\nPlease see the general response.\nThroughout the main body, the authors sometimes refer to notations that were not defined previously. For example, in Section 3.2, notation $c_i^{(t)}$ and $L_i^{(t)}$ are the first time the \"(t)\" superscript is used. In Equation 4, the notation $\\nabla_s$ is used without a definition.\nWe have added the clarification of all the aforementioned cases (marked in blue in Section 3.2 and Eq. 4)\nThe discussion of the experiments in Section 5 is confusing. At the end of Section 1, the author claimed that the experiments would highlight the suboptimality of studying trustworthy ML in a single-agent framework. However, in Section 5, the first research question shows that multi-agent setup leads to sub-optimalities.\nWe agree with the reviewer that wording of RQ1 was confusing. We have re-written it for clarity to:\n(RQ1) if regulators produced specifications for trustworthy parameters $\\boldsymbol{s}$ using a single-agent-optimized model, would that recommendation be respected in the multi-agent setting?\nand the description of the results has also been updated to:\nSingle-agent-optimized specifications lead to sub-optimalities in the multi-agent settings\nFigure 1 does not have a legend. Overall, most figures are hard to read and the captions do not provide sufficient description of the experiments.\nWe have addressed the reviewer's comment and re-wrote the captions for all experiment figures for clarity.\nMinor typos: ell_build(w) instead of ell_b(w) on page 5 under Equation 3.\nWe thank the reviewer for their attention. We have fixed the typo."}, {"Heading": "Response to Questions", "Subheading": "Official CommentbyAuthors17 Nov 2023, 11:25Everyone", "Content": "Comment:\nCan the authors clarify the assumption of a pre-calculated Pareto frontier as common knowledge?\nPlease see the general response.\nCan the authors clarify the discrepancy between RQ1 and the contribution claimed at the end of Section 1?\nWe answered this question in the previous section.\nCan the authors provide a more detailed description of the experiments in Section 5, as well as the reasoning for choosing the hyperparameters described in Appendix I? Particularly, the step size discount factor and the loss function weightings \u03bb_fair and \u03bb_priv?\nWe have added a more detailed description of the results as well as more informative captions to the paper (as per reviewer's prior comments). We would be happy to address any other specific confusions about the results.\nRegarding the discount factor.\nThe $c$ discount factor is one of three scalers which control the convergence behavior; with the other two being $C_*$ and $\\lambda_*$ . The discount factor $c < 1$ ensures convergence of the repeated game (as we discuss in Section 3.2). With other factors held constant, as $c$ decreases the dynamics remain the same, but the game will be extended in time.\nRegarding penalty scalers.\n$\\lambda_*$ are scalars that make the (monetary) penalties comparable to the model loss. Note that $\\lambda_*$ s are private information to builder (e.g,  1 million privacy fine compares to the gains from producing a 1-loss-unit lower loss from the trained model). Since regulators do not observe $\\lambda_*$ they adjust the impact of their penalties using $C_*$ . In summary,  $\\lambda_*$ and  $C_*$ both control the game dynamics but since we seek policy guidance for regulators and those only control $C_*$ we set $\\lambda_*=1$ and run ablations with $C_*$ .\nWe once again thank the reviewer for their detailed feedback and kindly ask them to consider raising their score if we have addressed their concerns successfully. We are happy to answer further questions."}, {"Heading": "As the author interaction is coming to an end, we are waiting for your post-rebuttal response.", "Subheading": "Official CommentbyAuthors22 Nov 2023, 10:50Everyone", "Content": "Comment:\nDear reviewer,\nThank you for your feedback on our work. Your suggestions have improved our work. As the author interaction is coming to an end, we are waiting for your post-rebuttal response. If our response answers your concerns, please consider raising your scores. If not, please let us know of your further questions such that we can answer them in the remaining time.\nWe thank you for all your efforts!"}]}, {"Heading": "Official Review of Submission9431 by Reviewer kbV4", "Subheading": "Official ReviewbyReviewer kbV401 Nov 2023, 10:40 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper is concerned with trustworthy ML. The main contribution is to model the setting as a game (SpecGame) between the model builder and the regulator who is interested in fairness and privacy. An algorithm ParetorPlay is introduced and it is shown that the agents remain on the Pareto frontier.\nSoundness:\n1 poor\nPresentation:\n2 fair\nContribution:\n1 poor\nStrengths:\n-The idea seems interesting and novel and one can think of it as modeling a wide set of problems.\n-The paper is also concerned with an important problem (trustworthy ML).\nWeaknesses:\n1-I think the main contribution of the paper is to model the dynamic interaction between the model builder and the regulator. Accordingly, it is more reasonable to think of only fairness or privacy or to possibly even abstract/lump both issues into one. I don't see how having these two considerations has added to the model. One can also consider the safety of the model or its robustness to adversarial manipulations as part of the regulator's concern for example.\n2-Why is the paper searching for Pareto Optimality instead of a Nash Equilibrium? Both agents (builder or regulator) are interested in their utility and as a result would deviate to increase it which is what would be captured by a Nash Equilibrium\n3-The section \u201cMaking a uniform strategy space.\u201d on page 6 is very unclear. What does \"consistent\" mean? It seems to suggest that the strategies are fixed. Further, it seems that the horizon is n. If that is the case the why does the utility in section 3.2 sum to infinity?\n4-Why is a correlated equilibrium and correlation device well-motivated in this setting? Also the paper mentions that \u201c This is known as a correlation device. If playing according to the signal is a best response for every player, we can recover a correlated equilibrium (CE). We leave the theoretical proof of this conjecture to future work.\u201d But doesn\u2019t Theorem 1 prove that you have a correlated equilibrium so is it a conjecture?\nQuestions:\nPlease see Weaknesses above.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response", "Subheading": "Official CommentbyAuthors17 Nov 2023, 11:30 (modified: 17 Nov 2023, 11:31)EveryoneRevisions", "Content": "Comment:\nWe thank the reviewer for their feedback. Below we address each point raised:\n1-I think the main contribution of the paper is to model the dynamic interaction between the model builder and the regulator. Accordingly, it is more reasonable to think of only fairness or privacy or to possibly even abstract/lump both issues into one. I don't see how having these two considerations has added to the model. One can also consider the safety of the model or its robustness to adversarial manipulations as part of the regulator's concern for example.\nWe agree with the reviewer that our framework is general and can include other trustworthy objectives in future work that builds on our framework.\nThe privacy and fairness regulators can be lumped into one. In fact, we do so at the start of a regulator-led game (in Section 4.1). There we note that considering a joint regulator  not resolve the inherent conflicts between the two objectives (fairness and privacy).\nIn practice ML models may be audited by separate entities. Therefore, our goal in keeping the regulators separate was to consider broader and more realistic scenarios.\nFor instance, the Consumer Privacy Act in California stipulates that the Attorney General enforces the privacy of employment background screening. However, the Employment Non-Discrimination Act is enforced by the Department of Justice. A joint regulator tacitly assumes that authorities would coordinate in every decision which is an impractical assumption.\n2-Why is the paper searching for Pareto Optimality instead of a Nash Equilibrium? Both agents (builder or regulator) are interested in their utility and as a result would deviate to increase it which is what would be captured by a Nash Equilibrium\nThe Nash Equilibria (NE) are not necessarily optimal outcomes for the society. For example, in the US there are fairness regulations that penalize employers for discriminative hiring behavior whereas privacy laws are limited to a few states. This discrepancy can lead the builders to create models that are fair but non-private. The corresponding SpecGame is in a Nash equilibrium $\\boldsymbol{s}_1$ because no single party has any alternative strategy that can improve its outcome.\nNow consider an alternative strategy profile $\\boldsymbol{s}_2$ where both fairness and privacy regulators could see an improvement while not hurting the builders utility. Between the two, $\\boldsymbol{s}_2$ is a societally optimal strategy profile; and indeed, $\\boldsymbol{s}_2$ Pareto dominates $\\boldsymbol{s}_1$ .\nGiven that ML regulation intends to achieve societal optimality we search for Pareto efficient points.\n3-The section \u201cMaking a uniform strategy space.\u201d on page 6 is very unclear. What does \"consistent\" mean? It seems to suggest that the strategies are fixed. Further, it seems that the horizon is n. If that is the case the why does the utility in section 3.2 sum to infinity?\nBy consistent, we mean that the domain of strategies for each stage of the game is the same. Furthermore, the SpecGame is in the class of infinitely-repeated Stackelberg games; therefore $n \\rightarrow \\infty$ .\nWe have added clarification for both comment to the main text.\n4-Why is a correlated equilibrium and correlation device well-motivated in this setting? Also the paper mentions that \u201c This is known as a correlation device. If playing according to the signal is a best response for every player, we can recover a correlated equilibrium (CE). We leave the theoretical proof of this conjecture to future work.\u201d But doesn\u2019t Theorem 1 prove that you have a correlated equilibrium so is it a conjecture?\nWe apologize for the confusion. Indeed the sentence \"We leave the theoretical proof of this conjecture to future work.\" was mistakenly left in the section from a prior submission where we did not have a formal proof).\nWe once again thank the reviewer for their detailed feedback and kindly ask them to consider raising their score if we have addressed their concerns successfully. We are happy to answer further questions."}, {"Heading": "Official Comment by Reviewer kbV4", "Subheading": "Official CommentbyReviewer kbV422 Nov 2023, 04:18Everyone", "Content": "Comment:\nI thank the authors for the feedback. I will keep my score as it is."}]}, {"Heading": "Official Review of Submission9431 by Reviewer wL1U", "Subheading": "Official ReviewbyReviewer wL1U27 Oct 2023, 04:19 (modified: 21 Nov 2023, 01:10)EveryoneRevisions", "Content": "Summary:\nThe paper studies the trade-off in trustworth ML, specifically that between fairness, privacy and model utility, by formulating it as a multi-agent game (called SpecGame) among two regulators and a model builder. The authors design a method called ParetoPlay to search for an equilibrium on the Pareto frontier. Experiments show that the designed method can be instantiated to two existing trustworth ML algorithms and demonstrate the trade-off between fairness vs. privacy vs. utility achievable by the designed method.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe problem of trade-off among different important criteria in trustworth ML is important.\nThe formulation of the interaction as a multi-agent game is intuitive.\nThe theoretical analysis and empirical results demonstrate the interaction among the agents in the game, which can be useful in understanding the (possible fundamental) limitations when designing new trustworth ML algorithms.\nWeaknesses:\nA key assumption is the common-knowledge of a pre-calculated PF among the \nconsidered critera, specifically privacy, fairness and model utility. This can be difficult to satisfy in practice.\nThere are several (simplifying) assumptions made (which can take away the pratical feasibility of the work). For two examples,\nWe assume that regulators are able to give penalties for violations of their respective objective which they formulate as a utility (or value) function.\nWe assume the regulators hold necessary information about the task at hand in the form of a Pareto Frontier (PF) which they use to choose fairness and privacy requirements that taken together with the resulting accuracy loss are Pareto efficient:\nThe writing can be improved (for details, see the questions below).\nQuestions:\nIn Section 1\nThis is because nowadays ML models are trained, maintained, and audited by separate entities\u2014each of which may pursue their own objectives.\nAre there references or real-world use-cases where this is true or implemented?\nIn Section 1,\n... that assumes shared knowledge of a pre-calculated PF between agent objectives.\nCan this PF be realized in practice? i.e., how to accurately obtain it? and if only a somewhat inaccurate one can be obtained, what are its implications?\nWhat is $i\\in I$ in Definition 1?\nIn Section 3,\nIn this work, we do not consider a competition between regulatory bodies since both are assumed to be governmental agencies.\nEven though the regulatory bodies are not set out to compete with each other, the inherent tension between the objectives can lead to competitive strategy profiles and actions, right?\nIn that case, what is the significant distinction of \"not considering a competition between the regulatory bodies\"?\nWhat is $\\{c_{i}^{(t)} \\}^t$ in the overall discounted loss in Section 3.2 ?\nIn Section 4,\nHowever, ...the highly non-convex nature of agent losses in $\\texttt{SpecGame}$\nHow is the non-convexity addressed?\nIn Section 4,\nfor $t>1$, we assume a mapping from the penalty values in S to trustworthy parameters values $s_{reg} = (\\gamma, \\epsilon)$\n(How) can this assumption be satisfied in practice ?\nWhat do the colors represent in Figure 4a?\nAre the experimental results averaged over multiple trials? If so, is there an analysis of the variation?\nFlag For Ethics Review:\nNo ethics review needed.\nDetails Of Ethics Concerns:\nN.A.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Weaknesses", "Subheading": "Official CommentbyAuthors17 Nov 2023, 11:34Everyone", "Content": "Comment:\nWe thank the reviewer for their feedback. Below we address each point raised in \"Weaknesses\" section.\nA key assumption is the common-knowledge of a pre-calculated PF among the considered critera, specifically privacy, fairness and model utility. This can be difficult to satisfy in practice.\nPlease see the general response.\nThere are several (simplifying) assumptions made (which can take away the pratical feasibility of the work). For two examples,\nWe assume that regulators are able to give penalties for violations of their respective objective which they formulate as a utility (or value) function.\nDespite nascent privacy laws, there are already\nmany instances\nof monetary penalties levied for violation of data privacy laws such as GDPR. Therefore, modeling such fines to be proportional to the privacy loss (as we do) is not a limitation beyond the fact that we take differential privacy as our notion of privacy\u2014a choice that we believe is presently justified because differential privacy is the dominant privacy notion for machine learning models.\nWe assume the regulators hold necessary information about the task at hand in the form of a Pareto Frontier (PF) which they use to choose fairness and privacy requirements that taken together with the resulting accuracy loss are Pareto efficient:\nPlease see the general response."}, {"Heading": "Response to Questions (Part 1)", "Subheading": "Official CommentbyAuthors17 Nov 2023, 11:45 (modified: 17 Nov 2023, 12:08)EveryoneRevisions", "Content": "Comment:\nIn Section 1\nThis is because nowadays ML models are trained, maintained, and audited by separate entities\u2014each of which may pursue their own objectives.\nAre there references or real-world use-cases where this is true or implemented?\nIn California, the Consumer Privacy Act in California stipulates that the Attorney General enforces the privacy of employment background screening.  However, the Employment Non-Discrimination Act is enforced by the Department of Justice. These are two separate governmental entities. We believe cross-agency cases such as this should be more prevalent as the use of automated decision making systems grows. Having said that, we are not trained in law so we refrain from making a case reference.\nIn Section 1,\n... that assumes shared knowledge of a pre-calculated PF between agent objectives.\nCan this PF be realized in practice? i.e., how to accurately obtain it? and if only a somewhat inaccurate one can be obtained, what are its implications?\nYes, it is possible to develop a cryptographic zero-knowledge-proof (ZKP) version of ParetoPlay where the agents not only share models but also proof of trustworthiness guarantees  (see Shamsabadi et al. 2022 for an example of a ZKP proof of fairness). We expand on this idea in Section J in the appendix, however, this is beyond the scope of the present work which lays the game theoretic foundations for ML Regulation.\nFor a discussion on the shared Pareto frontier, please see the general response.\nWhat is\u00a0 $i \\in I$ in Definition 1?\n$I$ is the set of objectives.  This was left out by mistake and has been added to Definition 1.\nIn Section 3,\nIn this work, we do not consider a competition between regulatory bodies since both are assumed to be governmental agencies.\nEven though the regulatory bodies are not set out to compete with each other, the inherent tension between the objectives can lead to competitive strategy profiles and actions, right?\nYes, that is correct. But the regulators being\ngovernmental\nagencies, the higher level agent (the government) can make a decision regarding the fairness and privacy trade-off. We capture this scenario in the first stage of a regulator-led SpecGame in the 2nd paragraph of Section 4.1.\nIn that case, what is the significant distinction of \"not considering a competition between the regulatory bodies\"\nAn alternative game formulation, for example, can consider the regulatory bodies as independent agents (not controlled by a 3rd agent, such as the government).\nIn this setting, the regulators can fight against each other to improve their own metric, possibly to the detriment of the general good. For instance, low-privacy low-fairness can be an equilibrium of such a game, which does not benefit either party. In similar 2-player games, a solution to this is to use a third party mediator to ensure a better equilibrium (see Section 10.7.3 in Shoham and Leyton-Brown 2009 for instance).\nNote that in our formulation, we already have a natural candidate for such a mediator which is the the government. We have thus shown that a solution to the joint game and subgame reduces to our original game formulation.\nWhat is $B: =\\left\\{c_i^{(t)}\\right\\}^t$ \u00a0\u00a0in the overall discounted loss in Section 3.2 ?\nThis is a factor that depends on how the agent perceives its current loss w.r.t its past losses. In the most general case can change over time, $B = B(t)$ ; which is what the expression above models. We have reformulated this general loss as $L_i=\\sum_{t=1}^{\\infty}\\left\\{\\prod^{t}_{i=1}c_i^{(t)}\\right\\} L_i^{(t)}$ which should make this point clearer. Note that, as before, we assume $B$ is constant with time  $c_i^{(t)} = c$ which is a common assumption in modeling of repeated games."}, {"Heading": "Response to Questions (Part 2)", "Subheading": "Official CommentbyAuthors17 Nov 2023, 11:46 (modified: 17 Nov 2023, 12:08)EveryoneRevisions", "Content": "Comment:\nIn Section 4,\nHowever, ...the highly non-convex nature of agent losses in\nHow is the non-convexity addressed?\nBy approximating and interpolating the agent losses on Pareto frontier (as we discuss in paragraph titled\nApproximation and calibration\nin Section 4.1).\nIn Section 4,\nfor $t>1$ , we assume a mapping from the penalty values in $\\mathrm{S}$ to trustworthy parameters values $s_{, u y}=(\\gamma, \\epsilon)$\n(How) can this assumption be satisfied in practice ?\nWhat this assumption says is that for a given pair of parameters $\\boldsymbol{s}=(\\gamma, \\epsilon)$ we can find a penalty to enforce it. Note that this is trivially true for a large enough penalty $C_\\text{fair}$ ( $C_\\text{priv}$ ) ensuring that fairness (privacy) losses dominate the model builders loss, and loss update in equations 3 and 4. Therefore, for any feasible solution to the joint optimization of training loss under fairness and privacy constraints, this assumption holds; and therefore does not limit the applicability of our algorithm in practice.\nWhat do the colors represent in Figure 4a?\nThe colors in Figure 4a represent different $C_{fair}$ values. The lightest shade correspond to the lowest $C_{fair}$ , and the darkest the highest. We have adjusted the figure legend and its caption to make this more clear.\nAre the experimental results averaged over multiple trials? If so, is there an analysis of the variation?\nWe run the games on pre-computed Pareto frontiers which we have calculated over many runs of the private and fair learning models we have adopted (FairPATE and DPSGD-Global-Adapt) to ensure we have an accurate Pareto frontier. As a result,  the only source of randomization in running ParetoPlay would be the calibration step, where the agents train a model to re-compute the Pareto surface. This pre-calculation of trustworthiness hyper-parameters makes for a relatively smooth (but non-convex) Pareto surface (see Figure 6 in the appendix). As a result we, we have found out that calibration often does not produce a new point on the Pareto surface  and therefore we have not observed varied convergence results. The different convergence results we see in the experiments are due to having different starting points and different regulator penalty scalar $C_{fair}$ and $C_{priv}$ .\nWe once again thank the reviewer for their detailed feedback and kindly ask them to consider raising their score if we have addressed their concerns successfully. We are happy to answer further questions."}, {"Heading": "Post rebuttal", "Subheading": "Official CommentbyReviewer wL1U21 Nov 2023, 01:09Everyone", "Content": "Comment:\nI thank the authors for their detailed response, especially the the supporting evidence and examples.\nMost of my questions are addressed. While there are limitations (unavoidable) such as those pointed out in the general comment and differential privacy as the main choice of privacy, I believe that this paper meets the bar of acceptance and hence raise my score to 6."}, {"Heading": "Thank you for engaging with us!", "Subheading": "Official CommentbyAuthors21 Nov 2023, 18:20Everyone", "Content": "Comment:\nWe thank the reviewer for their consideration and for raising their score. We are ready to answer any further questions that the reviewer may have."}]}]}, "rEQ8OiBxbZ": {"paper_info": {"Primary Area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "molecular repsentation; self-supervised learning", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "Self-supervised learning on 3D molecular structures has gained prominence in AI-driven drug discovery due to the high cost of annotating biochemical data. \nHowever, few have studied the selection of proper modeling semantic units within 3D molecular data, which is critical for an expressive pre-trained model as recognized in natural language processing and computer vision.\nIn this study, we introduce \\textbf{L}ocalized G\\textbf{e}ometric \\textbf{G}enerati\\textbf{o}n (LEGO), a novel approach that treats tetrahedrons within 3D molecular structures as fundamental building blocks , leveraging their simplicity in three-dimension and their prevalence in molecular structural patterns such as carbon skeletons and functional groups.\nInspired by masked language/image modeling, LEGO perturbs a portion of tetrahedrons and learns to reconstruct them in pretraining.\nThe reconstruction of the noised local structures can be divided into a two-step process, namely spatial orientation prediction and internal arrangement generation.\nFirst, we predict the global orientation of the noised local structure within the whole molecule, equipping the model with positional information for these foundational components.\nThen, we geometrically reconstruct the internal arrangements of the noised local structures revealing their functional semantics.\nTo address the atom-bond inconsistency problem in previous denoising methods and utilize the prior of chemical bonds, we propose to model the graph as a set of nodes and edges and explicitly generate the edges during pre-training.\nIn this way, LEGO exploits the advantages of encoding structural geometry features as well as leveraging the expressiveness of self-supervised learning.\nExtensive experiments on molecular quantum and biochemical property prediction tasks demonstrate the effectiveness of our approach.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9430", "PDF Url": "https://openreview.net/pdf?id=rEQ8OiBxbZ"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9430 by Area Chair qcpZ", "Subheading": "Meta ReviewbyArea Chair qcpZ05 Dec 2023, 14:00 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThe paper introduces a novel self-supervised pre-training method for 3D molecular datasets. This method leverages the geometric information of local molecular structures by dividing the molecule into tetrahedra of a few atoms and using a graph-based representation. The authors then apply the model to downstream property prediction tasks and compare it to publicly available benchmarks.\nWhile the reviewers acknowledge the paper's novelty and appreciate its motivation, they disagree on the writing quality and find that the methods and results sections need significant improvement. The reviewers expressed serious concerns about the unconvincing experiments, which lack empirical evidence to support the proposed approach's superiority. Additionally, the baselines chosen for comparison don't include state-of-the-art methods, and the reviewers also found the ablation studies to be lacking.\nEven if the authors provided feedback during the rebuttal stage, the modifications to the paper were minor.\nGiven the limited supporting evidence for the proposed method's advantages, I recommend rejecting the paper.\nJustification For Why Not Higher Score:\nThe paper only presents limited empirical evidence supporting the method's advantages.\nJustification For Why Not Lower Score:\nN/A."}, {"Heading": "Official Review of Submission9430 by Reviewer VdE4", "Subheading": "Official ReviewbyReviewer VdE402 Nov 2023, 04:43 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper presents a novel pre-training approach for 3D molecular datasets, building upon TokenGT. By augmenting TokenGT with 3D attributes, the proposed method harnesses 3D molecular data to enhance its generalization capabilities across various tasks.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nSegmenting 3D structures into distinct local components is an insightful approach.\nThe reconstruction method for these local structures is intriguing and presents a unique strategy.\nThere is a marked novelty in the concept of pre-training 3D molecules through local structures, despite the base model being a straightforward extension of TokenGT enhanced with 3D coordinates.\nWeaknesses:\nThe base model's simple strategy of appending 3D features to token embeddings may compromise the essential equivariance of 3D molecules.\nThe description of the training objective section lacks clarity. I have outlined specific inquiries in the subsequent Questions.\nGiven that the model is training on the reconstruction of molecular conformations, it would be beneficial to disclose the reconstruction accuracy on the pretraining dataset to demonstrate the model's learning efficacy.\nThe experimental comparisons should encompass additional 3D molecular datasets such as QM9 and GEOM-drug, considering the model's pretraining on 3D structures. Nonetheless, the paper confines its reporting to the OGBLSC-PCQM4Mv2 dataset and the observed performance significantly lags behind SOTA methods. The explanation provided for this underperformance does not sufficiently account for these results.\nThe layout of the paper requires revision. The Algorithm should be positioned before page 10 or on a separate page designated as an appendix, rather than following the references on page 12.\nQuestions:\nWhat exactly constitutes the input for TokenGT-3D? Is it the original molecules, or do you utilize each local structure segmentation after masking and perturbation? Or is it the masked and perturbed local structure segmentations of a single molecule, or something else entirely?\nGiven the proposed method centers on pre-trained representation learning, what form do the learned representation embeddings take for downstream tasks?\nCould you elaborate on how the local structures are reconstructed? What serves as the input for this process: a single embedding from the TokenGT-3D output, or a collection of embeddings from local structure segmentations within a single molecule?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer VdE4", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:40Everyone", "Content": "Comment:\nWe sincerely thank you for the valuable remarks that help us better examine our work. To address your concern, we present the following responses.\nQ1\n:\nWhat exactly constitutes the input for TokenGT-3D?\nIs it the original molecules, or do you utilize each local structure segmentation after masking and perturbation? Or is it the masked and perturbed local structure segmentations of a single molecule, or something else entirely?\nResponse:\nDuring pretraining, TokenGT-3D still take node and edge tokens, i.e. the original molecules as the input, as is shown in Figure 3 on page 4. The attribute mask and coordinate noise at perturbation stage are implemented token-wise during perturbation. Afterwards, the model takes both clean and perturbed tokens of the original molecule as the input; the segmentation on molecular conformations does not function as a 3D tokenizer.\nQ2\n: Given the proposed method centers on pre-trained representation learning,\nwhat form do the learned representation embeddings take for downstream tasks\n?\nResponse\n:  A $[\\texttt{CLS}]$ token is concatnated after the augmented node and edge embedding before the forward process of the Transformer encoder and serve as the graph representation for downstream tasks. This BERT-style method follows the implementation in the original TokenGT.\nQ3\n: Could you elaborate on\nhow the local structures are reconstructed\n? What serves as the input for this process: a single embedding from the TokenGT-3D output, or a collection of embeddings from local structure segmentations within a single molecule?\nResponse\n: Thanks for your feedback. To better illustrate the local structure reconstruction procedure, we have revised the paper to reposition Algorithm 1 within the methodology section. As is previously stated in Q1, the model takes both clean and perturbed tokens as input, producing an embedding $\\mathbf{Emb}$ in shape $[n+m,dim]$. During pretraining, we utilize three separate mask indicators -- $M_\\text{center}, M_\\text{edge},M_\\text{leaf}$ -- for center atoms, edges and leaf atoms within the whole molecule. For reconstruction, the central atom embeddings, $\\mathbf{Emb}[M_\\text{center}]$, are fed into a specialized head to predict global orientation. Likewise, edge embeddings $\\mathbf{Emb}[M_\\text{edge}]$ and leaf node embeddings $\\mathbf{Emb}[M_\\text{leaf}]$are input into dedicated modules for reconstructing edge lengths and angles, respectively."}]}, {"Heading": "Official Review of Submission9430 by Reviewer 2SQj", "Subheading": "Official ReviewbyReviewer 2SQj31 Oct 2023, 22:26 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposes a self-supervised pre-training strategy for 3D molecular structures, based on partitioning molecular structure into tetrahedra that can then be masked and reconstructed.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThe empirical performance is OK for some of the fine tuning tasks, although still not consistently solid.\nWeaknesses:\nThe geometrical justification of tetrahedra as simplest polyhedron might make sense, but in chemistry it makes a lot less sense. The authors literally show benzene in Figure 1b, which has 120-degree bonding pattern (so called planar-trigonal in chemistry) that is NOT a tetrahedron at all, and has very different local symmetries. \nThe ablation studies are unconvicing. Why not evaluate the role of the actual innovations introduced ? Tetrahedra vs. point-wise generation. Evaluate the role of edge information ? \nWhat happens for atoms with more than 4 bonds (sulfur, phosphorous, etc) ?\nQuestions:\n\"We attributethistothedifferent3Dstructuresmoleculesexhibitinliposomecompounds.\" What does this mean ? What are these structures different ? \nAll the biochemistry prediction tasks are actually properties of the graph, not the 3D structure, What 3D structure is being used ?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer 2SQj", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:41Everyone", "Content": "Comment:\nWe really appreciate the helpful comments and critiques that allow us to improve LEGO. Regarding your concern, here is our response.\nQ1:\nThe geometrical justification of tetrahedra as simplest polyhedron might make sense, but in chemistry it makes a lot less sense. The authors literally\nshow benzene in Figure 1b, which has 120-degree bonding pattern (so called planar-trigonal in chemistry)\nthat is NOT a tetrahedron at all, and has very different local symmetries\n......\nWhat happens for atoms with more than 4 bonds (sulfur, phosphorous, etc) ?\nResponse\n: Thank you for highlighting this important point. We have expanded the motivation section (page 4) to better address cases where local molecular structures deviate from standard tetrahedral geometry. While tetrahedra serve as an informative reference decomposition target due to their simplicity, our segmentation strategy adapts accordingly when this arrangement does not naturally arise.\nFor instance, center atoms with fewer than four one-hop neighbors, as commonly observed in functional groups (e.g. ketenes, amines, ethers in Fig. 1b), are handled by treating the local unit as a degraded tetrahedra. Meanwhile, atoms forming more than four bonds, like sulfur and phosphorous, we incorporate all its one-hop neighbors into the unit. For cyclic structures such as benzene, non-adjacent ring atoms are selected, effectively partitioning the system into triangular fragments. In summary, while tetrahedral local structures provide a useful illustration, our method maintains flexibility to accommodate chemical environments where alternative local symmetries dominate.\nQ2\n: \"We attribute this to the different 3D structures molecules exhibit in liposome compounds.\" What does this mean ? What are these structures different ?\nResponse\n:  Thank you for raising this question. To clarify, when referring to the \"different 3D structures exhibited by molecules in liposome compounds\", we intended to highlight potential differences between conformations in a lipid environment versus the conformations in a water environment, or the equilibrated 3D conformations used during pre-training. As you pointed out, our original phrasing was unclear on this distinction.\nWe hypothesize that the weaker performance on the Lipo task may arise because the stable conformations adopted by molecules in a lipid-like environment can differ substantially from common isolated conformations. As our pre-training exclusively saw the latter, transferring representations to predict lipid-specific properties poses an additional challenge.\nQ3:\nAll the biochemistry prediction tasks are actually properties of the graph, not the 3D structure, What 3D structure is being used ?\nResponse: We would like to point out that we adopt only 2D graph inputs for the biochemistry prediction tasks to\nconform with\nestablished dataset conventions and existing baseline approaches on MoleculeNet, including GraphMVP, 3DInfomax, and other 3D pre-trained models - all of which use 2D inputs. As such, while not directly evaluating 3D structure modeling, these tasks probe valuable model capabilities: 1.Encoding functional semantics related to downstream biological effects into 2D topology representations. 2.Assessing transferability of features learned from 3D pre-training when adapted to 2D inputs.\nConversely, we intentionally select 3D-sensitive quantum property prediction as an additional testbed for explicit 3D molecular structure modeling, using true 3D conformations as inputs. This directly examines the model's proficiency at encoding geometric details within continuous predictions - complementing the more functional evaluation on biochemical properties."}]}, {"Heading": "Official Review of Submission9430 by Reviewer SwR4", "Subheading": "Official ReviewbyReviewer SwR430 Oct 2023, 11:26 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper describes a pretraining process on the 3D structure of small molecules. The method splits a molecule into tetrahedrons of a few atoms each and learns to reconstruct them in 3D using knowledge of the graph structure of the input molecule. The authors apply the model on downstream property prediction tasks and compare to public benchmarks.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe main method in the paper is interesting conceptually and somewhat original as it combines a graph token representation with a decomposition of a small molecule into smaller 3D structural units. The downstream application on MoleculeNet is significant, and the application on the open graph benchmark is useful for context, although the particular table 2 is misleading. The introduction is reasonably clear, however, other parts of the paper have problems with the language or formalism.\nWeaknesses:\nThe paper can use a lot of rewriting in the methods and experimental results sections.\nTable 1 does not correctly highlight the best and second best results (see column for BACE when 2 of the 2D models perform better than LEGO).  Table 2 is missing the vast majority of best performing models from the opengraph benchmark large scale challenge.  Interestingly, the comparison in table 2 misses the top two entries which are models included in Table 1; if I count correctly the LEGO model would rank 9th in the validation metric with a substantial gap compared to unimol published in last year's ICLR.  It would be useful if the authors submitted their model to the benchmark to see the performance on the test set.\nThe text has a lot of rushed / unclear sentences.  The first sentence on page 8 (\"All this baselines involves...\") does not make sense as written.  Small errors and lack of clarity starts earlier in page 5 (undefined d_p), page 6 (\"way to increase the and generalizability\"), page 7 (\"... is enough to valid our method\", \"...graph representation an pass it...\", \"...an important properties...\", \"...proved to be close related...\"), probably more.\nQuestions:\nAlthough the current method is not directly inspired by this work, I believe that the RL reconstruction of molecular geometries from 3D fragments in Flam-Shepherd et al (\nhttps://arxiv.org/abs/2202.00658\n) relates closely to the core inspiration of this method and might warrant discussion in the intro. (In contrast, some of the discussion of the atom-bond inconsistency problem is potentially possible to skip as it doesn't add meaningful insights.) Did the authors think of extending their tetrahedral segmentation approach to use a similar fragment-based approach instead?\nCan the authors comment on the disparity of the approach of their validation scores on moleculeNet vs those on the large-scale challenge?\nThe ablation study is not helpful: the perturbation of the model is too limited, and table 3 suggests strongly that the parameters are actually not optimal.  The random perturbation pretraining of table 4 is not described in a clear enough fashion.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Reviewer SwR4", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:41Everyone", "Content": "Comment:\nThank you for the thorough review and positive feedback - your suggestions will lead to a stronger analysis. To address your concern, we provide the following point-to-point responses.\nQ1\n:...... I believe that the RL reconstruction of molecular geometries from 3D fragments in Flam-Shepherd et al (\nhttps://arxiv.org/abs/2202.00658\n) relates closely to the core inspiration of this method and might warrant discussion in the intro....\nResponse\n: We appreciate you highlighting the promising fragment-based reconstruction approach by Flam-Shepherd et al. As you suggested, we will cite this relevant work in the introduction and discuss extensions of our tetrahedron segmentation to incorporate larger functional fragments as building blocks. This could enable more complex biologically meaningful reconstruction objectives in our future work.\nQ2\n: Can the authors comment on the disparity of the approach of their validation scores on moleculeNet vs those on the large-scale challenge?\nResponse:\nFor PCQM4M-v2, we would like to argue that existing top approaches incorporate sophisticated graph encoding priors, while we apply a pure transformer. Nevertheless, the benefits of our pretraining approach can be demonstrated by the performance increase over the non-pretrained TokenGT model.  The primary contribution of this work is to give a glimpse at how proper selection of semantic units impacts 3D molecular pretraining, and we believe a further introduction of graph inductive bias will further improve our result in table2.\nQ3:\nThe ablation study is not helpful: the perturbation of the model is too limited, and table 3 suggests strongly that the parameters are actually not optimal. The random perturbation pretraining of table 4 is not described in a clear enough fashion.\nResponse:\nThank you for the feedback requesting more extensive ablation experiments and analysis, and we have revised the abaltion study section in our paper. In table 3, we show that excessive noise in 3D structure denoising can lead to training divergence and detrimental impacts and validate the setting of our perturbation strategy. In table 4, we validate the benefits of structural pretraining by comparing against a naive random atom-level denoising baseline with equivalent 36% masking."}]}, {"Heading": "Official Review of Submission9430 by Reviewer wqqh", "Subheading": "Official ReviewbyReviewer wqqh29 Oct 2023, 15:53 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper introduces a 3D molecular self-supervised learning approach that leverages the geometric information of molecular local structures, in the way of orientation prediction and arrangement generation. The atom-bond inconsistency issue has been identified and tackled through a joint modeling of the graph as a set of nodes and edges. The method has been benchmarked on MoleculeNet and OGBLSC-PCQM4Mv2 datasets to verify the efficacy of proposed designs.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThe paper is well-motivated through the concept of molecular local structure and the introduced approach yields good novelty.\nThe presentation is mostly clear and the method is easy to follow.\nWeaknesses:\nThe experimental results seem to be insufficient to support the empirical superiority of the proposed approach. In particular, the method could be further improved either through a more careful design of the backbone or enhancements of the training objective to make the results stronger.\nIf the results are difficult to improve, the authors may also be suggested to try other benchmarks or setups, e.g., QM9, MD17, where a better utilization of the 3D structural information would bring more benefits. In the current shape the quality of the experimental evaluations may not meet the bar of ICLR.\nQuestions:\nQ1. I am curious whether the proposed SSL objective can be combined with other backbones or even other pretraining objectives. If so, it would be interesting to see how the method can benefit different backbones which may be an evidence of the extensibility of the approach.\nQ2. How does the method perform on datasets like QM9?\nQ3. For the ablation study, it would also be interesting to see how the proposed 3D TokenGT helps to boost the performance since one of the claim in the paper is that the atom-bond inconsistency problem is tackled by modeling graph as a set of nodes and edges.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\nCode Of Conduct:\nYes"}]}, "UVSKuh9eK5": {"paper_info": {"Primary Area": "visualization or interpretation of learned representations", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Compositional generalization, Out-of-distribution generalization, Vision-language models, CLIP, Disentangled representations, Language supervision, data-centric AI", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "Vision-language models (VLMs), such as CLIP, have shown promising Out-of-Distribution (OoD) generalization under various flavors of distribution shifts. Recent studies attempted to investigate the leading cause of this property. In this work, we target the same goal, but focus on a certain type of distribution shift, in which test images contain unseen compositions of attribute-object pairs, but with the objects and attributes being individually seen during training. The models are expected to classify those images into the composition classes, i.e. attribute-object pairs, and also into object classes by ignoring attributes. We carefully designed an authentic image test dataset consisting of attributes for objects that are unlikely encountered in the CLIP training data. We found that the compositions diversity in the training data, as measured by normalized mutual information between objects and attributes, has a significant effect on the improvement of compositional generalization in the CLIP models. We found that image/text representation disentanglement with respect to the composition constituents also plays a key role in the improved generalization of these models. We notice that larger training datasets could potentially trigger emergence of such a disentanglement, as the compositions are typically more diverse in such datasets. We validate this hypothesis through different representation disentanglement metrics, including Z-Diff, and explicitness scores for various CLIPs. Our findings reveal a correlation between better OoD performance and higher scores in these disentanglement metrics, suggesting that improved disentanglement potentially contributes to enhanced compositional OoD generalization in VLMs.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9428", "PDF Url": "https://openreview.net/pdf?id=UVSKuh9eK5"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9428 by Area Chair FE77", "Subheading": "Meta ReviewbyArea Chair FE7703 Dec 2023, 20:58 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThis paper investigates the cause of compositional generalization(CG) ability in the Vision-language models (VLMs). A new dataset, ImageNet-AO is proposed to benchmark the compositional capabilities of several CLIP models. Two main reasons are identified for improving CG: disentangled representation and large compositional training data.  Different representation disentanglement metrics are employed to validate the hypothesis.\nStrengths:\nThe proposed dataset helps to evaluate compositional generalization in generative models. This could be significant to the community exploring model generalization.\nVarious analysis have been conducted to investigate the relationship between CG and representation disentanglement.\nWeaknesses:\nIt does not guarantee that the attributes in test set are not present or co-occur less in the training data.\nIt lacks clarity in explaining motivation and justification of the proposed dataset.\nThe conclusion is less convincing due to the limited candidates in each experiment.\nSome details are missing such as the context of 'switching dimension'  in 4.1, how human evaluation is conducted, etc.\nJustification For Why Not Higher Score:\nThe idea proposed in this paper is interesting and insightful, and constructing a dataset based on compositionality is novel and has potential impact to the community. However, there are shared concerns by all reviewers on limited evaluation and missing details. I think the paper is not ready for publication at this stage, and the authors need more time to improve the paper.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "Discussion between authors and reviewers", "Subheading": "Official CommentbyArea Chair FE7720 Nov 2023, 13:41 (modified: 15 Mar 2024, 00:26)EveryoneRevisions", "Content": "Comment:\nDear Reviewers,\nThanks for the reviews. The authors have uploaded their responses to your comments, please check if the rebuttal address your concerns and if you have further questions/comments to discuss with the authors. If the authors have addressed your concerns, please adjust your rating accordingly or vice versa.\nAC"}, {"Heading": "Looking forward to discussing with reviewers", "Subheading": "Official CommentbyAuthors19 Nov 2023, 22:46Everyone", "Content": "Comment:\nDear reviewers,\nMany thanks for your valuable feedback on our paper. We have gone through your points one-by-one and tried to address them carefully. We would appreciate if you could take a look at our response, and let us know if you have any further comments on the paper/rebuttal. We would be more than happy to take all your criticism, and incorporate them all in the paper.\nThanks!"}, {"Heading": "Official Review of Submission9428 by Reviewer unRs", "Subheading": "Official ReviewbyReviewer unRs05 Nov 2023, 05:05 (modified: 22 Nov 2023, 17:04)EveryoneRevisions", "Content": "Summary:\nIn this work, author examine the compositionally generalization in vision language model. By adopting different combination of disentangled attribute in training dataset of CLIP, author generate a authentic test set that is unseen by model but share the same disentangled attribute. Author also argue that the level of feature disentanglement is high correlate to model generalization by presenting various analysis.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n3 good\nStrengths:\nThis paper propose a high quality test set measuring compositional generalization with generative model. This benchmark provide a simply and more straightforward measurement for compositional generalization of Top1 accuracy for synthetic dataset, over prior measurement like using Visual Genome, or captions perturbation. This could be significant to the community exploring model generalization.\nAuthor have conducted various analysis over the relationship between compositionally and feature disentanglement, demonstrate the potential influence of the proposed dataset at a large scale.\nWeaknesses:\nIn 3.2, the statement 'We interpret these findings as strong evidence that the inclusion of language supervision, particularly during CLIP training, positively impacts the model representation quality' might be too strong of a claim. As explored in prior work(\"Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP)\") , language supervision might not be the sole reason for model generalization. There're multiple variance between VLM and other modality and author should not attribute such improvement solely on language supervision.\nConclusion are less convincing due to the limited candidate in each experiment. For instance, in Table 1, it will be interesting to shows the NMI for a subset of LAION with the same number of data to other dataset. Also in table 2, there's only 4 results, please consider adding more variance of dataset and CLIP architecture .\nThe narrative after section 4 is a bit too rush, it's hard to follow the method and results. For instance, what is 'dimensions' in 4.1 stands for? And more context over 'switching dimension' would be helpful. Moreover, I cannot tell how the conclusion of 'A higher level of accuracy in the image retrieval task indicates that the model embeddings are more disentangled.' can be drawn from experiment in 4.2.\nThere's some grammar and formatting issue, for instance in section 4, spaces were missing between sentences.\nQuestions:\nPage 3: in the imrpoved generalization -> typo\nPlease refer to weakness. While this work could be potential significant to the community, the clarity could be further improve, especially on drawing the connection between compositionally and feature disentanglement.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Answer to Point 1", "Subheading": "Official CommentbyAuthors15 Nov 2023, 13:54Everyone", "Content": "Comment:\nThank you for your valuable feedback. I appreciate the time and effort you have put into reviewing my work. Below, I have addressed each of your points in detail.\nPoint1\n: We acknowledge that, as already mentioned in the paper (first paragraph in the introduction), some prior work such as [1] suggested that the model generalization is indeed influenced by multiple factors beyond language supervision alone.\nHowever, we note that there are certain shortcomings in assessing the role of language in generalization of CLIP models in [1]. Specifically, the dataset (ImageNet-Captions) that is used in [1] to investigate the effect of language in domain robustness could lack domain information (e.g. painting, sketch, clip art, etc.) in the captions that are used for training and thus does not utilize the language capability to tackle domain shift. This stems from the fact that in this dataset, the captions are taken from the original text data of the Flickr images. Hence, the language is not used in its maximum capacity to train the model. However, in our case, we expect all the composition constituents, which are attributes and objects, been described numerous times\nindividually\nin the training data. In fact, we investigate compositional OoD generalization, which assumes that the novel combination of\nknown\nconcepts appear at the test time and evaluate the role of language in this type of OoD generalization. Aside from this, ImageNet-Captions is orders of magnitude smaller (~460k images) than the datasets that are used to train CLIPs, which are often in the order of 100 millions or few billions of images, which could be insufficient to expect the language to play a significant role in enhancing the generalization. On the other hand, we benchmark CLIPs that are trained with much larger training sets. Therefore, our work makes a better setup to assess the role of language in the generalization. \n Therefore, one of our aims was to re-assess whether the language could bring any additional value to the table beyond what has already been known in the  literature.\nWe have to emphasize that our intent was not to attribute the observed improvement in model performance solely to language supervision, but rather to see if the language could be\none of the\nthe  drivers of improvement in our compositional generalization setting. To make this happen, we conducted\ncontrolled\nexperiments  to specifically isolate and understand the impact of language factors on model generalization. For instance, we compared models with\nidentical structures\nand\ntraining methodologies\n, with the only variable being their\ntraining datasets\n, such as the CC and YFCC models. Both models employ the same image and text encoders, yet they yield different outcomes. Further analysis revealed that the captions in the CC dataset are of markedly higher quality (table 1), which provides empirical evidence of the language factor's influence on model performance.\nWe believe these controlled comparisons are essential to discern the discrete contributions of different variables to model generalization. In future iterations of our manuscript, we will strive to more precisely articulate the multifaceted nature of model generalization, ensuring that the significance of language supervision is presented as one of several contributing factors, rather than the sole determinant.\nWe hope this clarification addresses your concerns, and we are committed to revising our manuscript to reflect a more nuanced understanding of the elements that contribute to the robustness of model generalization.\n[1] Fang, Alex, et al. \"Data determines distributional robustness in contrastive language image pre-training (clip).\" International Conference on Machine Learning. PMLR, 2022."}, {"Heading": "Answer to Point 2 and 3", "Subheading": "Official CommentbyAuthors15 Nov 2023, 13:56Everyone", "Content": "Comment:\nPoint2\n: Table 1 in Section 3.3.1 now includes NMI values for various subsets of the LAION dataset. It is worth noting that these different LAION subsets originate from the same source but undergo different filtering processes. The filtering appears to alter the token distribution, which in turn affects the NMI values. However, NMI has no direct relationship to the dataset size. For instance, as the results indicate, despite its smaller size, LAION 400m shows a bit higher composition diversity compared to that the LAION 2B.\nTable1: Normalized Mutual Information between the attributes and objects calculated for captions of some CLIP training sets. The domain of these random variables are defined based on the compositions present in our generated dataset.\nDataset\nDataset Size\nNMI\nYFCC\n15m\n0.9390\nCC\n12m\n0.8903\nLAION\n400m\n0.8307\nLAION\n12m\n0.9024\nLAION\n2B\n0.8541\nAdditionally, Section 4.1 now contains expanded results, including additional models in the updated version of Table 2.\nTable2: Number of common dimensions across factors and switching dimensions for color manipulation in 3D Shapes dataset\nDataset\nArchitecture\nOoD Accuracy\n# of Common Dims\n# of Switching Dims\nLAION\nViT-L/14\n43.14%\n2\n40\nLAION\nViT-B/16\n38.89%\n5\n60\nLAION\nViT-B/32\n32.84%\n7\n90\nOpenAI\nViT-L/14\n39.91%\n3\n5\nOpenAI\nViT-B/16\n37.73%\n4\n10\nOpenAI\nViT-B/32\n33.43%\n6\n30\nCC\nRN50\n9.64%\n15\n200\nYFCC\nRN50\n3.60%\n21\n250\nPoint 3\n: In Section 4.1, when we refer to 'dimensions,' we are speaking of the individual elements within the embedding vector generated by the CLIP model. To clarify the concept of 'switching dimensions,' we conducted experiments where these individual elements are systematically altered to assess their impact on the performance of the model. By switching, we mean replacing the values in these dimensions by those of the samples that have a different level of attributes.   This process helps us understand which dimensions of the embedding vector correspond to specific attributes in the data.\nRegarding the image retrieval task discussed in Section 4.2, we propose that a more decomposable and disentangled representation leads to better retrieval performance. This is because when the embedding of an image is decomposable, adding the embedding of an attribute (like a color or shape) to the image\u2019s embedding results in an alteration of only the intended attribute in the resultant image, without affecting other characteristics. Conversely, if the embedding is entangled, the same operation would inadvertently alter the entire image, not just the intended attribute. Therefore, the ability to perform precise modifications through embedding manipulation is indicative of a disentangled representation, which, in turn, suggests higher accuracy in retrieval tasks.\nIn our additional experiment using the 3D Shapes dataset, we aim to correlate the model's disentanglement metrics with its Out-of-Distribution (OoD) generalization performance. We employ a classifier trained to recognize common features across different images, such as color. By identifying which dimensions of the embedding vector are significant for such features, we gain insights into the disentanglement of the embeddings. Our findings suggest that in higher-performing CLIP models on OoD benchmarks, fewer dimensions need to be switched to achieve an embedding that aligns with a modified attribute, thus reinforcing the connection between disentanglement and OoD performance.\nWe have included additional diagrams in the revised manuscript to visually demonstrate the processes and outcomes of these experiments, specifically experiment 4.1, for greater clarity.\nWe hope that this explanation resolves the ambiguities and look forward to further comments and suggestions."}, {"Heading": "Update score to 6.", "Subheading": "Official CommentbyReviewer unRs22 Nov 2023, 17:03Everyone", "Content": "Comment:\nThanks author for the detailed reply. I appreciate update on point 1 and 3, and expect author to revise your draft carefully for more precisely articulation. Besides, I see a nice trend based on including additional experiment for point2, author should also update OpenAI and LAION with more model architecture(like ResNet50-CLIP) to make your experiment more comprehensive. \nI will update my score to 6.\nIn general I think the idea proposed by this paper is still interesting and insightful, and constructing a dataset based on compositionality is novel and potential impactful to the community. However, as also mentioned by other reviewer, there still many lack of clarity in explaining motivation and justification of the proposed dataset. Author should continue to work on this for future revision."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors22 Nov 2023, 23:21Everyone", "Content": "Comment:\nThanks for your insightful comments. We will make sure to revise the paper accordingly, and include more backbones in the results to make the trend clearer. We have to mention that the main motivation behind designing this dataset is to minimize the possibility of train/test overlap in the CLIP models and ensure an authentic validation of OoD generalization of such models. Previous work relied on various versions of ImageNet to make such evaluations. But such datasets consist of images that are pretty likely to be encountered in a gigantic dataset such as LAION, making their evaluation less reliable. We believe that aiming for evaluation of the compositional generalization makes it possible to come up with test data with rare compositions, but with known constituents, that are extremely unlikely to be seen in the training sets. We collected a systematic and comprehensive list of such compositions, and did a thorough examination of the generated images through human inspection to make a dataset that helps the field to investigate OoD generalization more authentically. This also helped us shed light on one of the origins on OoD generalization in VLMs, which is the disentanglement at various levels."}]}, {"Heading": "Official Review of Submission9428 by Reviewer qiWV", "Subheading": "Official ReviewbyReviewer qiWV01 Nov 2023, 05:32 (modified: 22 Nov 2023, 11:38)EveryoneRevisions", "Content": "Summary:\nThis paper studies CLIP models under a different type of distribution shift namely compositional OOD generalization, where the objects and attributes may be individually seen during training, but their composition is unseen. A new dataset, ImageNet-AO is generated using DALL-E, containing such novel compositions for ImageNet classes. It is ensured that the generated compositions are not present in the CLIP training datasets. Key observations are - i) compositional diversity of the training dataset improves the compositional generalization of the CLIP model, ii) image/text representation disentanglement of objects and attributes improves generalization, iii) larger, more diverse datasets leads to better compositional generalization, iv) better disentanglement in representations leads to better compositional generalization.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n3 good\nStrengths:\nThe experiments are well-designed\nConclusions drawn are very interesting and insightful\nThe dataset ImageNet-AO can be helpful for future study as well\nWeaknesses:\n\"\nthe training dataset of the OpenAI CLIP has not been released, which makes designing a test set that has a truly different distribution from the training one challenging. We aim to address these issues in this paper, by focusing our attention on the compositional generalization\nin the single object setting, and designing an authentic test dataset to assess the training data characteristics and mechanisms in the models that lead to the OoD generalization.\n\" -- This contradicts the following statement where the authors claim that they verify that the ImageNet-AO images are not a part of several CLIP training dataset -- \"\nTo ensure these combinations were not present in the CLIP training set, we conducted a thorough search and removed any combinations that were found.\n\"\n\"\nBy assessing the captions in the training sets, we guarantee that none of the captions in our test dataset or similar captions are included in the CLIP training data.\n\"\nIs this check done for all the other datasets considered in the paper as well (LAION, YFCC15m, CC12m, and DataComp)?\nA similar check should be done on images as well, it is possible that such images with different captions are present in the training set. Usually, captions from web sources are not exactly descriptive of the image.\n\"\nWe also found that the CLIPs that show higher OoD generalization typically exhibit strong disentangled text representations. Furthermore, such CLIPs also enjoy a more disentangled image representation with respect to the attributes and objects as well.\n\" -- the experiments in the paper do hint at the above statement. But this does not necessarily imply the following: \"\nSpecifically, a dataset with diverse compositions of attribute-objects facilitates a more disentangled text representation, which in turn induces a disentangled image representation through contrastive learning.\n\" It could be possible that diverse images lead to disentangled image representations as well.\n\"\nTo evaluate the degree of disentanglement in the training captions utilized by the CLIP, we conducted an analysis by measuring the normalized mutual information (NMI) between the object class and attribute tokens, whose domains are defined based on the captions in our generated dataset.\n\" -- Could the authors explain how the domains are defined based on the captions in the generated dataset? More details on how the NMI is measured would be helpful.\nFig.4 - It is not clear how the disentanglement metrics are computed for the image encoder.\n\"\nWe aimed for a diverse set of class names to enhance the complexity of the generated images.\n\" -- It is not clear if all 1000 classes were used or only a subset. If a subset was used, how was this chosen?\n\"\nThis dataset was produced by creating compositional images via a text-to-image model, using an Attribute+Object template.\n\" -- could the authors give more details/ a citation for the Attribute+Object template?\nCould the authors provide details on where the 30 adjectives were chosen from?\n\"\nLastly, human evaluation was used to validate the generated images, with images not closely aligning with their prompts removed. After this process, around 12000 combinations remained, for which we successfully generated near 50000 accurate, high-quality images.\n\" - The order of the two statements may have to be swapped? Could the authors provide details on how this human evaluation was done?\n\"\nFor the test sets, all 1000 classes of ImageNet were used as the in-distribution set and expanded the number of classes to approximately 12000 for the OoD set.\n\" -- could the authors share how the captions were created for the OOD set? Sharing some examples would be helpful. I believe the 80 captions are used only for the ID set, and single relevant captions are used for the OOD set?\nIn Fig.1, for a more fair comparison, the image-only models such as DINO-v2 and BEiT-v2 should also be trained on the datasets that were used for training CLIP (by using only the images, and ignoring the captions). Without matching at least the image datasets, there is not enough evidence to support the following statement - \"\nWe interpret these findings as strong evidence that the inclusion of language supervision, particularly during CLIP training, positively impacts the model representation quality, hence making it possible to generalize to unseen compositions, despite the absence of such compositions in their training data.\n\"\nNitpicks -\ncitation format seems non-standard - (x) vs. [x]\ninline citations should use the format xyz et al., rather than [x]\nA citation for the work that defines \"compositional OOD generalization\" would be helpful\nQuestions:\nAlthough the experiments and conclusions in the paper are interesting and useful, several aspects of the paper need more clarity. These are mentioned in the weaknesses section. I will be happy to update my score based on clarifications provided by the authors.\nCodes, models, and datasets must be open-sourced for the benefit of future research. Could the authors comment on this? Would these be released upon acceptance?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Answer Point 1, 2 and 3", "Subheading": "Official CommentbyAuthors15 Nov 2023, 14:07 (modified: 15 Nov 2023, 14:39)EveryoneRevisions", "Content": "Comment:\nThank you for your valuable feedback. I appreciate the time and effort you have put into reviewing my work. Below, I have addressed each of your points in detail.\nPoint 1\n: It is true that the specific training dataset used by OpenAI for CLIP has not been publicly re-\nleased. However, there are several multimodal datasets that have been made available and are commonly\nused for training vision-language models, such as LAION, YFCC, CC, and DataComp. Our work utilizes\nthese publicly available datasets to construct a test as mentioned in Section 6.2. We also evaluate CLIP\nmodels trained on these public training datasets.\nPoint 2\n: We have thoroughly checked all datasets referenced in our paper, including LAION, YFCC15m,\nCC12m, and DataComp, to ensure that (attribute,object) pairs in our dataset are not present in the\ncaptions of these CLIPs\u2019 training data.\nSince we want to evaluate models on novel compositions, we first found (attribute, object) pairs that\nare not appear in any captions of the available training sets of CLIPs. For these novel (or extremely rare)\ncombinations, the images were also generated which makes the possibility of image overlap extremely\nunlikely. For instance, we have combinations such as \u2018hairy jeep,\u2019 or \u2018luminous golf cart,\u2019 in our dataset,\nas illustrated in Fig. 2, which are unlikely to be encountered on the web.\nWhile we acknowledge the possibility that the model may have been exposed to similar images during\ntraining, we can assert with confidence that the exact image and complete caption pairs used in our\ntest dataset are unique and represent new combinations that the model has not previously encountered.\nThis pairing approach ensures that, despite any potential familiarity with individual images, the model\nis still being evaluated on its ability to generalize to new image-caption combinations, which we have\nengineered to be out-of-distribution (OoD). Additionally, the principal experiments presented in our\npaper, particularly in Section 3.1, as well as the supplementary experiments detailed in Sections 4.2, 6.3,\nand 6.4, incorporate both image and text components within their methodologies. These experiments\ndo not solely concentrate on the image aspect.\nPoint 3\n: The textual input is inherently disentangled, with adjective and object tokens being\ndistinctly separate\n. In contrast, the visual inputs inherently exhibit entanglement; attributes and objects within images are naturally intertwined. By augmenting the diversity of textual data, we promote disentanglement in text representation, as the model learns to distinguish between the separate tokens for adjectives and objects due to their consistent and isolated presentation. However the entanglement in image representations persists because the attributes and objects are not presented as isolated features but as parts of an integrated whole.  Therefore, while increased diversity can enhance the model's exposure to varied visual concepts, it does not automatically lead to disentangled representations. These claims are also backed by Fig. 4, where we see that the text encoder usually exhibits a more pronounced disentanglement score compared with the image encoder.\nIn the table below, we also compare a CLIP model with some image-only models that were trained on large-scale datasets or are among the top-performing models on ImageNet benchmarks. We evaluate these models on different disentanglement metrics  on the ImageNet-AO.\nTable: Disentanglement metrics on different Models\nModel\nz_diff \u2191\nDCI-I \u2191\nexplicitness \u2191\nDCIMIG \u2191\nCLIP-ViT-14-datacomp-xl\n0.955\n0.1834\n0.9103\n0.025\nDINOv2\n0.95\n0.1787\n0.9022\n0.021\nBeit\n0.901\n0.1463\n0.884\n0.016\nresnext101\n0.948\n0.1745\n0.901\n0.0209\nEffeicent-b7\n0.899\n0.1274\n0.8531\n0.0134"}, {"Heading": "Responses to Points 4-9", "Subheading": "Official CommentbyAuthors15 Nov 2023, 14:14Everyone", "Content": "Comment:\nPoint 4\n:\n In the context of Normalized Mutual Information (NMI) in our experiments, 'X' represents the entire set of attributes that we utilized in generating our dataset, encompassing various descriptive elements such as texture. 'Y', on the other hand, corresponds to the set of objects used in generating our dataset, including a range of specific items or entities. These definitions of 'X' and 'Y' enable us to analyze the relationship between attributes and objects in our dataset as learned by the CLIP model.\n$$\n\\begin{aligned}\n& \\mathrm{MI}(\\mathrm{X} ; \\mathrm{Y})=E_y[\\mathrm{H}(\\mathrm{X})-\\mathrm{H}(\\mathrm{X} \\mid \\mathrm{Y}=\\mathrm{y})] \\\\\n& \\mathrm{H}(\\mathrm{X})=-\\sum_{i=1}^n p_i^* \\log \\left(p_i\\right) \\\\\n& p\\left(x_i\\right)=\\frac{\\# \\text { captions that have } x_i}{\\sum_{j=1}^n \\text { \\#captions that have } x_j} \\\\\n& p(x, y)=\\frac{\\# \\text { captions that have } x \\text { and } y}{\\sum_{x y} \\sum_y \\text { captions that have } x_i \\text { and } y_j} \\\\\n& \\mathrm{H}(\\mathrm{X} \\mid \\mathrm{Y}=\\mathrm{y})=-\\sum_x p(x, y) * \\log \\left(\\frac{p(x, y)}{p(y)}\\right) \\\\\n& \\mathrm{NMI}(\\mathrm{X} ; \\mathrm{Y})=\\frac{2 * M I(X ; Y)}{[H(X)+H(Y)]}\n\\end{aligned}\n$$\nPoint 5\n: As mentioned in Section 3.3.2, we can consider two super latent factors corresponding to attributes and objects. For each vector of factors, we choose a image associated with those factors. We then calculate the embedding for that image. This gives us a set of representations and corresponding latent factors. Using these embeddings and factors, we can calculate different disentanglement metrics. To assess the randomness effects, we repeated this experiment multiple times,  and observed that the results show negligible variance. We also observed that if instead of random selection, we calculate embeddings of all images per relation and take the average, the results hold. We note that this is because the images associated with a certain relation have similar embeddings. For more details on how each metric is calculated, please refer to [2].\nPoint 6\n: As detailed in Section 6.2 of the Appendix, we utilized the entire spectrum of 1000 classes from\nthe ImageNet dataset.\nPoint 7\n:  We appreciate the opportunity to provide additional details on the methodology used to generate our dataset. As mentioned in Section 6.2 and illustrated in Figure 3, we employed a simple template structure, [ATTRIBUTE] [OBJECT], to create compositional images via a text-to-image model. This approach allowed us to systematically combine various attributes with objects to enrich the diversity and complexity of the images. Also this approach was used in previous works [3,4]. For concrete examples of these prompts, one may refer to Figure 2 in the main text, which showcases sample image captions generated using this template. Furthermore, a more extensive collection of prompts can be found in Figure 8 of the Appendix.\nPoint 8\n: Initially, we compiled an extensive list of adjectives using various sources, including linguistic websites and large language models like ChatGPT, to ensure a broad and inclusive initial selection. The primary list contained 50 attributes. Through iterative experimentation, we generated preliminary images for a range of objects paired with these adjectives. This practical evaluation was crucial as it allowed us to observe the representation of certain attributes within the images. We found that some adjectives, such as \u2018fast\u2019, did not visually translate well into the generated images. Consequently, these were omitted from the final list. The refinement process led us to a curated set of 30 adjectives that were visually discernible and thus suitable for our image generation objective. The full methodology for selecting attributes will be detailed in the Appendix.\nPoint 9\n: We have to clarify that by 'after this process ...,' we meant to report the number of generated images after the final step, which is the human inspection. Here we got total of 50k images from 12k combinations that passed the human evaluation criteria. The human inspection was conducted in two stages: First, before generating any images, we filtered the initial list of 30,000 combinations by searching for these combinations in the captions of training datasets and removing any matches, to ensure the remaining combinations were novel and feasible to depict visually. Second, after generating images for the remaining novel combinations using the text-to-image model, further human evaluation was necessary to eliminate unsatisfactory images where either the object was not clearly rendered or the intended attribute was not distinctly perceptible. Through this two-step refinement process, we narrowed down the dataset from 30,000 initial combinations to a final curated collection of around 12,000 novel  (attribute,object) combinations comprising 50,000 images that passed the human evaluation criteria."}, {"Heading": "Responses to Points 10,11", "Subheading": "Official CommentbyAuthors15 Nov 2023, 14:23 (modified: 15 Nov 2023, 14:25)EveryoneRevisions", "Content": "Comment:\nPoint 10\n: Our dataset contains images generated from textual prompts, which are then used as captions for evaluation. For example, the \"hairy jeep\" image was generated from the prompt \"hairy jeep\".\nIn zero-shot evaluation, ID image embeddings are matched against 1,000 captions for the 1,000 classes. OoD image embeddings are matched against 12,000 captions since each combination is a unique class.\nFor both sets, class name captions are put into 80 templates like \"this is a photo of...\" to create multiple embeddings. The final class embedding is the average of these 80 variations.\nPoint 11\n: As delineated in Section 3.2, the aim of our study was not to forge a direct and fair comparison between CLIP and image-only models like DINO-v2 and BEiT-v2. Instead, our focus was investigating the mechanism that causes the language supervision to improve  CLIP's out-of-distribution (OoD) generalization capabilities as mentioned in the title and abstract of our paper. We selected the supervised and self-supervised models for comparison based on their state-of-the-art ImageNet performance and their training on large-scale datasets, which allowed us to make a closer, albeit approximate, comparison given our practical constraints.\nOnly to see performance of other powerful foundation models (that are not based on language supervision), we show the results of these models on our proposed dataset too.\nThe evidence supporting the role of language supervision in enhancing CLIP's performance is multifaceted, including:\nVariations in performance among models trained on Conceptual Captions (CC) and YFCC datasets, particularly with lower Normalized Mutual Information (NMI).\nA clear disentanglement at the representation level.\nA well-documented correlation between representation disentanglement and OoD generalization.\nOur narrative underscores the distinctive benefits of language supervision and its impact on OoD accuracy, rather than on a direct performance comparison. Given the logistical challenges of compiling a dataset comparable to the one used to train CLIP, we opted to showcase the most relevant models to support our hypothesis.\nAt the end, about your question, we plan to open-source all codes and datasets upon the paper\u2019s acceptance to\nsupport future research.\n[3] Lewis, Martha, et al. \"Does clip bind concepts? probing compositionality in large image models.\" arXiv preprint arXiv:2212.10537 (2022).\n[4] Nayak, Nihal V., Peilin Yu, and Stephen H. Bach. \"Learning to compose soft prompts for compositional zero-shot learning.\" arXiv preprint arXiv:2204.03574 (2022)."}, {"Heading": "Official Comment by Reviewer qiWV", "Subheading": "Official CommentbyReviewer qiWV22 Nov 2023, 11:37Everyone", "Content": "Comment:\nI thank the authors for the detailed and thorough response, which addresses several of my concerns. Please find my comments below-\nPoint 1\n: In this case, the following statement needs to be removed from the Introduction - \"For instance, the training dataset of the OpenAI CLIP has not been released, which makes designing a test set that has a truly different distribution from the training one challenging. We aim to address these issues in this paper, by ... \" \nThis is because, the construction of ImageNet-AO assumes access to the training dataset of  CLIP, while this statement makes it seem like access is not needed.\nPoint 2\n: It is possible that synonyms of object-attribute pairs exist as captions to similar images. Although the exact pair may not exist in the training dataset, a more robust means of ensuring that a similar image-caption combination does not exist in the training dataset is required. For example, finding the k nearest neighbors of the caption and image individually, and using human inspection to ensure the k nearest captions and k nearest images are not similar to them. Since the training dataset is very large scale, we cannot rely on the combination being unlikely to exist in the dataset, although it seems to be the case.\nPoint 3\n: Could the authors clarify whether the first row in the table presented in the rebuttal uses text encoder or image encoder? Also, could the authors share the conclusions from the table?\nPoint 9\n: For human evaluation, it is important to also include details on how this was conducted, and how many people participated in the study.\nI update my score to 5 based on the rebuttal."}, {"Heading": "Responses to Points 1,2,3", "Subheading": "Official CommentbyAuthors22 Nov 2023, 13:53Everyone", "Content": "Comment:\nThank you for sharing your insightful feedback. In the following response, I've thoughtfully addressed each of your points.\nPoint 1\n:\nIn making a dataset whose probability of overlap with that of the OpenAI CLIP training set, we aimed to make images with novel compositions that are not observed frequently in the real-world. While we do not have access to the OpenAI CLIP training captions, we took captions in other training sets that are used in other versions of CLIPs as a surrogate for real-world captions. By generating images based on prompts that are extremely rare in the real world, we make the chance of overlap between our data and that of the OpenAI CLIP small. Hence, in making our dataset by this strategy we do not actually need to have access to the OpenAI training captions.\nAlso, in contrast to previous studies, such as [1], where different CLIP models are evaluated in Out-of-Distribution contexts using datasets like ImageNet v2 or ImageNet R, it is important to note that there is no definitive evidence confirming these datasets were excluded from training sets like LAION. However, our dataset provides this evidence.\n[1]Fang, Alex, et al. \"Data determines distributional robustness in contrastive language image pre-training (clip).\" International Conference on Machine Learning. PMLR, 2022.\nPoint 2\n:\nWe appreciate your suggestion regarding the verification of the uniqueness of object-attribute pairs in our dataset, especially in the context of the large-scale training dataset. We recognize the importance of ensuring that not only the exact pairs but also similar combinations are not present in the training dataset.\nIn our current study, we prioritized generating rare combinations of attributes and objects, focusing on creating unique pairs for zero-shot evaluations. This approach was aimed at minimizing the likelihood of overlap with existing datasets, given the constraints we faced in terms of resources. While we have endeavored to observe and maximize the difference in distribution compared to existing datasets that claim to OoD, we acknowledge that there is always room to enhance this aspect further.\nWe acknowledge the merit of your suggestion to use methods like identifying the k nearest neighbors for both captions and images. This could indeed provide a more robust assurance of the uniqueness of our dataset and help to extremize the distinction from other datasets. If we can secure sufficient resources in the future, we plan to incorporate these more rigorous checks into our methodology. This would not only align with our goal of creating a distinct dataset but also significantly enhance the reliability and validity of our evaluations.\nPoint 3\n:\nTo clarify, the first row in the table indeed uses the image encoder. This is evident from our juxtaposition of the CLIP model alongside image-only models like DINOv2 in the same table. It's important to recognize that DINOv2 is acknowledged as a state-of-the-art model in various tasks, as detailed in their paper[2]. \nThe key takeaway from the table is the superior performance of the CLIP model in terms of disentanglement metrics. Despite CLIP and models like DINOv2 being trained on millions of images, CLIP demonstrates a higher level of disentanglement in its representations.\nAdditionally, it's worth noting that the level of disentanglement in the text encoder of the CLIP model is even higher than that in the image encoder(figure 4). This observation aligns with our discussion in section 3.3.3, where we delve into how the diversity of the training dataset enhances the disentanglement capabilities of the text encoder. This improved disentanglement in text representation subsequently contributes to the enhanced disentanglement in image representation.\nFurthermore, our findings suggest that training solely on a diverse image dataset does not inherently lead to highly disentangled representations. This aspect becomes particularly evident when considering the model's accuracy in OoD scenarios, as demonstrated in figure 1-right of our paper.\n[2]. Oquab, Maxime, et al. \"Dinov2: Learning robust visual features without supervision.\" arXiv preprint arXiv:2304.07193 (2023)."}, {"Heading": "Response to Point 9", "Subheading": "Official CommentbyAuthors22 Nov 2023, 13:54Everyone", "Content": "Comment:\nPoint 9\n:\nWe acknowledge the reviewer's concern regarding the lack of detailed information on the human evaluation process in our paper. We will address this oversight and provide a comprehensive description in the revised manuscript. While we briefly mention this aspect in section 6.2, we agree that a more detailed account is necessary.\nIn the human evaluation phase of our study, we engaged two evaluators for the assessment of images. Their task was to ensure the quality and relevance of each image according to two primary criteria:\n1. Object Recognition\n: The evaluators scrutinized each image to confirm that the central object was clearly recognizable and unambiguous. Any image where the object could not be easily identified or was subject to interpretation was excluded from the dataset.\n2. Attribute Visibility\n: The evaluators also assessed whether the specified attributes were visibly and unmistakably depicted in the image. Images that failed to clearly exhibit the required attributes were removed from consideration.\nThe process involved a systematic review of each image by the evaluators. Each image was independently assessed by both evaluators, and any discrepancies in their evaluations were resolved through discussion to reach a consensus.\nWe will include these details in section 6.2 of our revised paper to provide a clear understanding of our human evaluation process and its role in enhancing the quality of our dataset."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors22 Nov 2023, 23:29Everyone", "Content": "Comment:\nThanks again! We would appreciate if the reviewer could kindly take our response to his 3 points that have just been raised into consideration. We are willing to make further clarification and experimentation per the reviewer\u2019s request."}, {"Heading": "Official Comment by Reviewer qiWV", "Subheading": "Official CommentbyReviewer qiWV23 Nov 2023, 01:24Everyone", "Content": "Comment:\nI thank the authors for their prompt response. I have no further questions for the authors.\nAlthough the experiments and insights in the paper are useful, I maintain my rating of 5 since the dataset presented requires a more robust human evaluation to ensure that it meets the criteria claimed in the paper. Further, I believe that a more formal human evaluation is important for releasing a dataset. I am open to revising my rating based on discussions with other reviewers/ AC."}, {"Heading": "Response to Reviewer for Clarification on Data Generation", "Subheading": "Official CommentbyAuthors23 Nov 2023, 03:22Everyone", "Content": "Comment:\nTo provide more clarity on the human evaluation aspect, we have compiled statistics related to the data generation and human evaluation in the following table:\nImage Generation Count\nImages Remaining After Evaluation\nImages Removed\nPercentage of Removal Due to Missing Object\nPercentage of Removal Due to Attribute Failure\n74,500\n50,100\n24,400\n10%\n90%\nFrom the table, it is evident that the majority of image removals are attributed to issues with attribute binding, a known limitation of text-to-image (T2I) models. This challenge, particularly the model's struggle to accurately bind attributes to objects, has been discussed in previous works such as Feng et al. [1].\nAdditionally, this paper does not aim to highlight a method influenced by dataset bias. Instead, our focus is on evaluating different CLIP models trained on varied datasets to understand the impact of training data on out-of-distribution generalization. The performance trends observed in these models on our dataset are consistent with findings reported in other studies [2, 3, 4].\nReferences:\nFeng, Weixi, et al. \"Training-free structured diffusion guidance for compositional text-to-image synthesis.\" arXiv preprint arXiv:2212.05032 (2022).\nCherti, Mehdi, et al. \"Reproducible scaling laws for contrastive language-image learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\nNguyen, Thao, et al. \"Quality not quantity: On the interaction between dataset design and robustness of clip.\" Advances in Neural Information Processing Systems 35 (2022): 21455-21469.\nIlharco, G., et al. \"OpenCLIP (Version 0.1) [Software].\"\nhttps://github.com/mlfoundations/open_clip\n."}]}, {"Heading": "Official Review of Submission9428 by Reviewer PAYB", "Subheading": "Official ReviewbyReviewer PAYB31 Oct 2023, 04:23 (modified: 06 Dec 2023, 00:46)EveryoneRevisions", "Content": "Summary:\nThis paper proposes a new dataset to benchmark the compositional capabilities of several CLIP models (OpenAI and OpenCLIP). This dataset is generated using DALLE, and covers the 1000 class names from the ImageNet dataset combined with 30 adjectives. Manual annotators validated the combinations, resulting in ~12k plausible compositions, from which they generated 50k images. The authors also propose to measure the compositional generalization via the normalized mutual information between objects and attributes, and use Z-Diff Score, DCI-Informativeness, Explicitness score, and DCIMIG metrics to evaluate the disentanglement in the embeddings from the models.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThis paper proposes an interesting approach to measure the compositional capabilities of large-scale VL models, by leveraging a text-to-\nimage model to generate new images with specific attributes.\nThe authors provide a large set of experimental results in the supplementary materials, showing that CLIP models struggle with their proposed dataset\nThis paper is well-structured, easy to read and follow.\nWeaknesses:\nThere is no description or motivation for the attribute selection, are those attributes randomly selected or generated? How do the authors guarantee that those attributes are not present or co-occur less in the training data?\nThe human validation seems crucial in generating the proposed benchmark; however, there is no detailed description of how this was performed.\nIn section 1, the authors claim: \"By assessing the captions in the training sets, we guarantee that none of the captions in our test dataset or similar captions are included in the CLIP training data.\" -- however, I couldn't find any empirical or theoretical evidence, nor existing reference for this claim.\nThe human validation only asses for the plausibility of the noun-adjective composition, but are the images generated by DALLE following those compositions? Prior work has shown that Diffusion models \"struggle to understand the composition of certain concepts, such as confusing the attributes of different objects or relations between objects\"[1]. It is unclear if the generated dataset follows the attribute-noun composition, or falls into this category. See also [2].\nMost of the conclusions are prevalent in the literature (e.g., the diversity of training captions promotes compositionality [3]), and the mutual information analysis does not seem to provide additional insights [4, 5].\n[1] Liu, Nan, et al. \"Compositional visual generation with composable diffusion models.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n[2] Park, Dong Huk et al. \u201cBenchmark for Compositional Text-to-Image Synthesis.\u201d NeurIPS Datasets and Benchmarks (2021).\n[3] Doveh, Sivan, et al. \"Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models.\" arXiv preprint arXiv:2305.19595 (2023).\n[4] Radford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021.\n[5] Oquab, Maxime, et al. \"Dinov2: Learning robust visual features without supervision.\" arXiv preprint arXiv:2304.07193 (2023).\nQuestions:\nIs there any particular reason why DINO-v2 and BEiT-v2 are mentioned briefly in the introduction, but no further analysis is done in the following sections?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Responses to Points 1-4", "Subheading": "Official CommentbyAuthors15 Nov 2023, 14:35 (modified: 15 Nov 2023, 14:39)EveryoneRevisions", "Content": "Comment:\nThank you for your valuable feedback. I appreciate the time and effort you have put into reviewing my work. Below, I have addressed each of your points in detail.\nPoint 1\n: For the selection of attributes, we began with a comprehensive list of adjectives derived from various linguistic sources and large language models such as ChatGPT. This initial list was subjected to a practical evaluation phase where preliminary images were generated and assessed for the visual translation of these adjectives. Attributes that were not clearly represented, such as \u2019fast\u2019, were discarded. This process resulted in a refined set of 30 adjectives that were visually discernible.\nAs mentioned in Section 6.2, we combined these attributes with a list of objects. We then searched for these combinations in the captions of the CLIP training datasets (CC,YFCC,LAION,Datacomp). Combinations found in the training datasets, like \"dotty bag\", were removed. The remaining combinations, such as \"hairy jeep\", do not appear in the CLIP training dataset captions. Therefore, these combinations can be considered out-of-distribution data for CLIP models trained on those datasets. It is important to clarify that we do not consider the attributes alone to be out-of-distribution, but rather the full combinations of attributes and objects. For example, \"hairy\" on its own would not necessarily be out-of-distribution, but the novel combination \"hairy jeep\" would be.\nPoint 2\n: As outlined in Section 6.2 and visually depicted in Figure 3, our human evaluation protocol involved a straightforward yet effective method for ensuring the quality and relevance of the generated images. Two evaluators were assigned the task of assessing each image against two critical criteria to determine its suitability for inclusion in the dataset:\n-\nObject Recognition\n: Any image where the central object was not clearly recognizable or was ambiguous was removed.\n-\nAttribute Visibility\n: Images were excluded if the specified attributes were not evidently depicted.\nPoint 3\n: Our methodology involved an initial generation of 30,000 attribute-object combinations. We then conducted a comprehensive search for these combinations within the captions of various datasets on which CLIP has been trained, such as LAION. This search was instrumental in identifying and eliminating any combinations that already existed within the training data. Even if the individual attribute and object words appeared at a distance in a caption, we eliminated the combination of them from our combinations.\nConsequently, the remaining 12,000 combinations, which did not appear in the training data, were used to generate our test dataset. This rigorous process underpins our confidence in asserting that our test dataset comprises truly OOD examples relative to the CLIP training datasets.\nPoint 4\n: We acknowledge the valid concerns raised about ensuring the integrity of the attribute-noun compositions in our generated images, and the potential issues with diffusion models identified in prior work. As detailed in Section 6.2 and shown in Figure 3, our human evaluation process was designed to directly address these concerns. The main motivation for human evaluation is that text-to-image models currently have limitations in generating coherent compositions. We first evaluated different models like Stable Diffusion variants, Deep Floyd, and DALL-E, and found DALL-E to be the most capable at generating compositions. However, since its accuracy is imperfect for every composition, we added human evaluation to our pipeline to remove low quality \nand ambiguous images, and also the images whose contents are not consistent with the specified attribute-object."}, {"Heading": "Responses to Point 5 and Question", "Subheading": "Official CommentbyAuthors15 Nov 2023, 14:38Everyone", "Content": "Comment:\nPoint 5\n:\nWhile prior work has explored the relationship between training data diversity and compositional generalization, our study provides new analysis quantifying\nhow\ndiversity in the training data leads to improved disentanglement in model representations, thereby directly enabling better out-of-distribution (OoD) generalization.\nSpecifically, we conducted the first analysis (Section 3.3.1) that examines the impact of caption diversity and quality in training datasets on the mutual information (MI) between object and attribute tokens. We found that lower NMI indicates higher disentanglement between the two, which can improve the compositional generalization.\nBuilding on this, we propose a way to measure the disentanglement between objects and attributes in the representation spaces of CLIP text and image encoders (Section 3.3.2). To our knowledge, this disentanglement has not been explored or validated before in CLIP models. We demonstrate models with more disentangled representations achieve higher performance on our new compositional OoD benchmark. We also provided evidence that the disentanglement is much stronger on the text representation, and provided some theoretical insights on how the disentanglement could be induced on the image representation from the text.\nFurthermore, we provide additional insights into CLIP disentanglement beyond our benchmark by evaluating on 3D Shapes datasets. Our work is the first to extensively examine and validate the connection between disentangled representations and compositional generalization performance of CLIP models.\nOverall, our analysis and experiments on quantifying disentanglement and its impact on compositional generalization enhance the understanding of why and how CLIP models are able to generalize compositionally. We extend current knowledge by empirically demonstrating that diversity promotes compositionality through inducing disentangled representations.\nQuestion\n: Since DINO-v2 and BEiT-v2 are primarily image encoders without an accompanying text encoder, they fall outside the scope of our evaluation criteria. Most of our experimental design required models with text encoding capabilities to comprehensively assess and compare their performance."}, {"Heading": "Official Comment by Reviewer PAYB", "Subheading": "Official CommentbyReviewer PAYB22 Nov 2023, 14:08Everyone", "Content": "Comment:\nAfter carefully reading all reviews and responses, I'm keeping my original rating (5) and upgrading my confidence to 5. All reviewers have a consensus regarding some claims in the paper, especially regarding attribute selection and compositionality claims. Moreover, there are several aspects that remain unclear: the lack of information regarding human evaluations along with the confusing narrative (as pointed out by Reviewers unRs and qiWV). I would suggest to the authors to revise and resubmit to another venue."}, {"Heading": "Please checkout our response", "Subheading": "Official CommentbyAuthors22 Nov 2023, 15:49Everyone", "Content": "Comment:\nDear reviewer,\nWhile we admit some shared concerns between the reviewers at the beginning, we provided several clarifications in our response that might have been overlooked. For instance, we provided the detailed information on how the attributes are selected in the rebuttal, which is NOT further criticized by any of the reviewers. Also, could you please mention which compositionality claims in the paper is unsupported or has issues? We provided several further evidences along the disengagement of representations results in our response. \nHuman evaluation has also been addressed in our response and we believe that we clearly mentioned the criteria that are used to make the assessment (e.g. object recognition and attribute visibility). We kindly ask the reviewer to guide us on which aspect of this assessment is still unclear to him/her."}, {"Heading": "Re: Official Comment by Reviewer PAYB", "Subheading": "Official CommentbyArea Chair FE7702 Dec 2023, 15:23 (modified: 15 Mar 2024, 00:26)EveryoneRevisions", "Content": "Comment:\nDear Reviewer PAYB:\nThanks for the comments on the authors' rebuttal. The authors have posted additional clarification on the focus of their paper and the dataset design details. Please check if you have overlooked any details here:\nAddressing Shared Concerns and Providing Clarifications: While we acknowledge the shared concerns initially raised by the reviewers, we believe that our comprehensive clarifications provided in the response might have been overlooked. Specifically, we detailed how the attributes for our study were selected, which was a key point of concern. Notably, this aspect of our response did not receive further criticism from any of the reviewers, suggesting that our clarification was satisfactory.\nEmphasizing the Primary Contribution of Our Paper: We would like to reemphasize that the primary focus of our paper is on how increased disentanglement in CLIP representations aids in generalizing compositional Out-of-Distribution scenarios. This is substantiated by experiments on two datasets, including our custom-generated dataset and the 3D shape dataset. The consistent results strongly support our hypothesis, and we believe this critical aspect may have been somewhat overshadowed in the discussions about dataset design.\n4.The Authenticity of Our Dataset Design: In order to ensure the validity of our dataset design, we adhered to the following key principles:\nComprehensive and Systematic Attribute and Object Selection: We meticulously selected a diverse range of objects (representing entire ImageNet classes) and comprehensive attributes. This approach aimed to ensure inclusivity., and systematic representation.\nEnsuring Originality: We took care to thoroughly validate the uniqueness of the captions in our dataset, guaranteeing no overlap with any of the captions from current open-source training datasets utilized for CLIP models. Any captions discovered to match those in the training sets were eliminated from our dataset.\nComprehensive Human Evaluation: To guarantee the reliability of our dataset, we performed exhaustive human evaluations. Two separate reviewers stringently examined the captions to validate they accurately portrayed the corresponding images. Any instances of disagreement were meticulously re-evaluated to ensure consistency and accuracy across the dataset. Through this comprehensive review process, we upheld precision and fidelity in our data.\nPlease review their response and adjust you scores if the response address your concerns.\nAC"}, {"Heading": "Official Comment by Reviewer PAYB", "Subheading": "Official CommentbyReviewer PAYB03 Dec 2023, 23:12 (modified: 15 Mar 2024, 00:26)EveryoneRevisions", "Content": "Comment:\nDear AC,\nAfter carefully reviewing the comments and discussions, I'm keeping my score. There is no real clarification with respect to the human evaluations, and some claims need revision (e.g., Any captions discovered to match those in the training sets were eliminated from our dataset). This is only true for OpenCLIP, not all CLIP models, which sustain the author's claims with respect to the conducted evaluations.\nFurthermore, I don't see any revision based on the reviewer's and authors' discussion. There is no revision to the supplementary material, and it's unclear what portion of all major points raised in the discussion period will go in the main paper or appendix (e.g. The full methodology for selecting attributes will be detailed in the Appendix -- however this is still unclear)"}, {"Heading": "Official Comment by Area Chair FE77", "Subheading": "Official CommentbyArea Chair FE7705 Dec 2023, 00:29 (modified: 15 Mar 2024, 00:26)EveryoneRevisions", "Content": "Comment:\nDear Reviewer PAYB:\nThe authors have the following comments to address your concerns. Please let me know if they addressed your concerns.\nThanks,\nAC\n---------------Comments from the authors:\n#Clarification on CLIP Models and Datasets:\nYou mentioned that the claim regarding the elimination of captions from training sets applies only to OpenCLIP and not all CLIP models. It's important to clarify that this approach was indeed applied to all models evaluated in our study. We thoroughly investigated and discovered datasets for each model, including CC12m, YFCC15m, LAION, and Datacomp. The only exception was the OpenAI model, which uses a private dataset.\nIt's essential to note that OpenCLIP is not a singular model but a comprehensive project and repository that amalgamates various available CLIP models. OpenCLIP stands out as one of the most popular repositories on GitHub within the field of CLIP and VLM models. Many research papers and teams dedicated to CLIP utilize this repository as a central hub, contributing various CLIP models, and the latest advancements are consistently added to enrich this collaborative resource. Repository link :\nhttps://github.com/mlfoundations/open_clip\n#Revisions and Supplementary Material:\nRegarding the lack of revisions and updates to supplementary materials, I assure you that we have made substantive revisions based on the discussions between reviewers and authors. These revisions will be more apparent in the final version of the paper. Please see the next comment, where we outlined the items to be added to the Appendix.\nWe understand the importance of detailing our methodology for selecting attributes and ensuring transparency. Therefore, we have included a comprehensive explanation in the Appendix of the final paper. This addition aims to address the major points raised during the discussion period and provide clarity on our methods. Please see the next comment for further details."}]}]}, "1XHzHMQfcK": {"paper_info": {"Primary Area": "general machine learning (i.e., none of the above)", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Data Split, Support Points, SPlit, validation Set, Optimal Ratio, Significance of validation set", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "TL;DR": "Significant research aims for balanced training data, but neglects equitable validation set distribution. This study highlights how fair representation boosts learning accuracy through data splitting.", "Abstract": "Machine learning plays a crucial role in various research areas and industries. The effectiveness of machine learning models relies heavily on the quality and quantity of training data. To evaluate model performance on unseen data, it is important to divide the data into training and testing data sets. A three-way split into train-validation-test data-sets is also commonly used to create robust and generalized models. Validation set helps in tuning hyper-parameters to mitigate the problem of overfitting. It is of utmost importance to achieve precise and true portrayal of data across all three categories of data-sets: training, testing, and validation. Previous research has explored various statistical techniques such as 'SPlit' aimed to ensure proper membership of the complete data in the test set. Despite the utilization of these techniques, Insufficient evidence exists regarding the equitable treatment of the validation set. Although cross-validation is widely used for validation, randomly selecting the validation part may not be the complete representative of overall data, hindering the creation of a generalized model suitable for the test data. This work focuses on extracting validation sets using the Support Points method in 'SPlit' to obtain accurate data membership. Results demonstrate significant accuracy improvement when both test and validation sets are selected using the Support Points method.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9427", "PDF Url": "https://openreview.net/pdf?id=1XHzHMQfcK"}, "review_info": []}, "23OEmHVkpq": {"paper_info": {"Primary Area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "representation learning, variational autoencoders, disentangled representations, topological data analysis", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "TL;DR": "We propose a method for learning disentangled representations via optimizing multi-scale topological loss term", "Abstract": "We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding a multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art methods are based on VAE and encourage the joint distribution of latent variables to be factorized. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement learning. Our experiments have shown that the proposed TopDis loss improves disentanglement scores such as MIG, FactorVAE score, SAP score and DCI disentanglement score with respect to state-of-the-art results while preserving the reconstruction quality. Our method works in an unsupervised manner, permitting to apply it for problems without labeled factors of variation. The TopDis loss works even when factors of variation are correlated. Additionally, we show how  to use the proposed topological loss to find disentangled directions in a trained GAN.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "Supplementary Material": "zip", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9426", "PDF Url": "https://openreview.net/pdf?id=23OEmHVkpq"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9426 by Area Chair uqVP", "Subheading": "Meta ReviewbyArea Chair uqVP07 Dec 2023, 10:39 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThis paper introduced a differentiable topological loss for disentanglement learning. Contrary to the prior works that directly encourage the factorization of the latent variable based on VAE, the authors employed topological similarity on the data manifold as a measurement for disentanglement. This is achieved by measuring RTD (Barannikov et al., 2022) traversing along the latent space, which is added as a regularization to the existing VAE objective.\nFour reviewers recommended borderline scores of one borderline accept and three borderline reject. The primary concerns raised by the reviewers were about (1) clarifying the relation between the topological distance and disentanglement of latent factors, (2) clarifying the novelty over Barannikov et al., 2022, and (3) missing ablation studies and comparisons to additional baselines. The authors adequately addressed some of the concerns, but three reviewers maintained their negative recommendations mainly due to insufficient justifications on how RTD contributes to disentanglement learning.\nAfter reading the paper, reviews, and rebuttal, the AC agrees with the reviewers that the relation between RTD and disentanglement learning should be more clearly and thoroughly justified. The rebuttal provided informative clarifications, yet it is still insufficient to understand how the disentanglement imposed by RTD is related to other methods that directly encourage dimension-wise factorization i.e., it is not clear what complementary benefits are provided, since the authors\u2019 justification implies that the same factorization is expected. Such a lack of insights weakens the significance of the new results (i.e., results with different disentangled VAE baselines). For these reasons, AC believes that the paper presents an interesting and promising idea of new loss for disentanglement learning, yet the paper requires substantial revision to provide clear insights on how it works and is complementary to existing works. Hence, AC recommends rejection this time.\nJustification For Why Not Higher Score:\nInsufficient justifications on how RTD contributes to disentanglement learning.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "Summary of Author Response to All the Reviewers", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:59Everyone", "Content": "Comment:\nWe appreciate all the Reviewers for their time and valuable feedback. \nWe have updated the manuscript and highlighted changes with yellow.\nHere are some of principal points in our responses:\nQ\n:\nThe effectiveness of the proposed TopDis regularization term on vanilla VAEs has not been studied.\nA\n: We carried out additional experiments with VAE+TopDis.\nAs these results demonstrate,  VAE+TopDis outperformed VAE as measured by the 4 disentanglement metrics.\nMethod\nFactorVAE score\nMIG\nSAP\nDCI, dis.\ndSprites\nVAE\n0.781 \u00b1 0.016\n0.170 \u00b1 0.072\n0.057 \u00b1 0.039\n0.314 \u00b1 0.072\nVAE + TopDis (ours)\n0.833 \u00b1 0.068\n0.200 \u00b1 0.119\n0.065 \u00b1 0.009\n0.394 \u00b1 0.132\n3D Shapes\nVAE\n1.0 \u00b1 0.0\n0.729 \u00b1 0.070\n0.160 \u00b1 0.050\n0.952 \u00b1 0.023\nVAE + TopDis (ours)\n1.0 \u00b1 0.0\n0.835 \u00b1 0.012\n0.216 \u00b1 0.020\n0.977 \u00b1 0.023\n3D Faces\nVAE\n0.96 \u00b1 0.03\n0.525 \u00b1 0.051\n0.059 \u00b1 0.013\n0.813 \u00b1 0.063\nVAE + TopDis (ours)\n1.0 \u00b1 0.0\n0.539 \u00b1 0.037\n0.063 \u00b1 0.011\n0.831 \u00b1 0.023\nQ\n:\nThe effectiveness of the proposed TopDis regularization term on \u03b2-TCVAE has not been studied.\nA\n: We provide evaluations of \u03b2-TCVAE+TopDis and we have updated Table 1 accordingly.\nAddition of TopDis improves \u03b2-TCVAE as measured by the 4 disentanglement metrics.\nMethod\nFactorVAE score\nMIG\nSAP\nDCI, dis.\ndSprites\n\u03b2-TCVAE\n0.810 \u00b1 0.058\n0.332 \u00b1 0.029\n0.045 \u00b1 0.004\n0.543 \u00b1 0.049\n\u03b2-TCVAE + TopDis (ours)\n0.821 \u00b1 0.034\n0.341 \u00b1 0.021\n0.051 \u00b1 0.004\n0.556 \u00b1 0.042\n3D shapes\n\u03b2-TCVAE\n0.909 \u00b1 0.079\n0.693 \u00b1 0.053\n0.113 \u00b1 0.070\n0.877 \u00b1 0.018\n\u03b2-TCVAE + TopDis (ours)\n1.0 \u00b1 0.0\n0.751 \u00b1 0.051\n0.147 \u00b1 0.064\n0.901 \u00b1 0.014\n3D Faces\n\u03b2-TCVAE\n1.0 \u00b1 0.0\n0.568 \u00b1 0.063\n0.060 \u00b1 0.017\n0.822 \u00b1 0.033\n\u03b2-TCVAE + TopDis (ours)\n1.0 \u00b1 0.0\n0.591 \u00b1 0.058\n0.062 \u00b1 0.011\n0.859 \u00b1 0.031\nMPI 3D\n\u03b2-TCVAE\n0.365 \u00b1 0.042\n0.174 \u00b1 0.018\n0.080 \u00b1 0.013\n0.225 \u00b1 0.061\n\u03b2-TCVAE + TopDis (ours)\n0.496 \u00b1 0.039\n0.280 \u00b1 0.013\n0.143 \u00b1 0.009\n0.340 \u00b1 0.055\nQ\n:\nHigh performance of vanilla VAE in some cases.\nA\n: Indeed, vanilla VAE performed better than some other methods in some cases. This is consistent with actual papers: superiority of Vanilla VAE w.r.t beta-VAE for MPI3D was also observed by Locatello et al, 2020 (see Figure 7, bottom row). We note that we trained the models for 1 million iterations compared to 300k iterations used in several other works which can be a possible source of disagreement. Also, in most recent papers only more advanced methods were compared but not the vanilla VAE baseline. For evaluation of disentanglement metrics, we used the code from the commonly used disentanglement lib:\nhttps://github.com/google-research/disentanglement_lib"}, {"Heading": "Official Review of Submission9426 by Reviewer 1sM8", "Subheading": "Official ReviewbyReviewer 1sM831 Oct 2023, 16:23 (modified: 29 Nov 2023, 12:16)EveryoneRevisions", "Content": "Summary:\nThe authors of this paper present TopDis, which is a regularizer based on Representation Topology Divergence (RTD). In this approach, the objective to be optimized is a combination of \u201cclassic\u201d VAE loss and TopDis loss. Unlike the preceding approaches, topDis does not assume statistical independence between the factors of variations. Generally, introducing this loss term appears to further improve the current SOTA values for several disentanglement metrics (FactorVAE, MIG, SAP and DCI) across several different datasets (dSprites, 3D Shapes, 3D Faces, MPI 3D).\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThe paper is clearly written and easy to follow. In detail:\na. The authors explain the task of disentanglement rather clearly by providing a succinct overview of previous works.\nb. The motivation and contribution of the paper are also clearly defined with an intuitive explanation of the designed methodology.\nThe authors provide a variety of experiments and ablations, helping to evaluate their proposed disentanglement regularization loss practically. In detail:\na. The experiments (Table 1) appear comprehensive (except for the vanilla VAE; we will explain in the weakness section our concerns).\nb. The authors also provide enough qualitative examples, comparing models trained with TopDis regularizer and without.\nc. The architecture is succinctly described in the Appendix\nComputational complexity is also discussed in the Appendix, which is crucial for ML algorithms nowadays.\nWeaknesses:\nOne of the contributions the authors mention is: \u201cWe improve the reconstruction quality by applying gradient orthogonalization;\u201d - however, this contribution is only briefly mentioned in the conclusion and analyzed in the Appendix in greater detail. We suggest the authors to \u201cmove\u201d the gradient orthogonalization part to the main paper.\nAs the authors explained, the RTD was defined in a previous work, but we believe it is important to be defined in the main paper.\nIn section 4.1, bullets (2-4). In (2), g\\inG appears to be applied to both pixel and latent space. Later in (3,4), where decomposition G is defined, it seems that it can be applied only in the latent space. We believe the authors should re-write this part, clarifying how G can be applied in the pixel space or, if that is not the case remove from (2) the application of g in the pixel space.\nIn equation (4) regularization parameter /gamma is defined. Later in the appendix Q, \\gamma_1, and \\gamma_2 are used in the ablation table. Does this correspond, instead, to the loss: \\gamma_1 L_{VAE-based} + \\gamma_2 L_{TD}.\nIn page 5 footnote, the authors state that RPT can be computed in latent space instead of pixel space. Can the authors provide ablations in the appendix exploring this direction? Do the authors have insights into how this change can affect the final trained model?\nFinally, our main concern is whether the proposed regularizer contributes to the learning of the disentangled representation or the used base models (i.e., \\beta-VAE, Factor-VAE). Since, in the main paper, only the models with already disentanglement remedies are explored and not the vanilla VAE. More concerning in the ablation, VAE+TopDis is explored, but it seems that the training is not the same as the VAE reported in the main paper. Our guess is that the models in the ablation were trained for less number of iterations. We encourage the authors to include in the main paper VAE+TopDis trained under the same conditions (i.e. same number of iterations) as the reported VAE in Table 1. This will help readers understand to what extent the TopDis regularizer helps learn disentangled representations\nQuestions:\nSee above\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to the Review 1sM8", "Subheading": "Official CommentbyAuthors21 Nov 2023, 14:31 (modified: 22 Nov 2023, 12:09)EveryoneRevisions", "Content": "Comment:\nThank you for your time and thorough review highlighting strengths of our paper. We will improve the presentation according to the suggestions. Below we address specific concerns one by one.\nW1\n:\nWe suggest the authors to \u201cmove\u201d the gradient orthogonalization part to the main paper.\nA\n: Thank you for the suggestion, we have moved the details on gradient orthogonalization to the main part.\nW2\n:\nMove more details on formal definition of RTD to the main paper\nA\n: Thank you for the suggestion. We are adding more details on the formal definition to the discussion of the RTD definition in Section 3.1.\nW3\n:\nClarify how $G$ can be applied in the pixel space.\nA\n: Thank you for your question. The equation $f(g(z))=g(f(z))$ concerns the outcome of the learning process, which consists, in particular, in finding a priori unknown symmetries in the data. Given the decoder map $f$, this equation defines the symmetry action on $X$ preserving the data distribution.\nW4\n:\nClarify notations \\gamma_1 and \\gamma_2 in the ablation table in Appendix Q.\nA\n: Thank you for your remark. In Appendix Q, \\gamma_1 denotes the weight for Total Correlation loss from the FactorVAE model while \\gamma_2 denotes the weight for TopDis loss from the equation (4). We have added the necessary clarification to the Appendix Q and renamed \\gamma_1 to \\gamma_TC (stands for Total Correlation) and \\gamma_2 to \\gamma_TD (stands for TopDis) as more suitable ones.\nW5\n:\nCan RTD be applied in latent space instead of pixel space?\nA\n: Thank you for your question. In the footnote on page 5, we mean that it is possible to compute TopDis loss either between the images (i.e. in pixel space) or between their representations (for example, produced by another pretrained model). It can be beneficial for different reasons to compare the multiscale topology of data in a representation space. For example, common disentanglement datasets have typical image resolution 64x64. However, in case of significantly higher image resolution, it could be beneficial to utilize representations instead of images. As a proof of concept, in the subsection 5.3 and Appendix H, we provide the results on unsupervised discovery of disentangled directions in StyleGAN where we compute the RTD in the representation space instead of pixel space. With these experiments, we illustrate that RTD finds meaningful directions using the representation space as well.\nW6\n:\nClarification on VAE+TopDis\nA\n: Thank you for the question.  We are now running the experiments for VAE+TopDis.  Here are the results for VAE+TopDis vs VAE :\nMethod\nFactorVAE score\nMIG\nSAP\nDCI, dis.\ndSprites\nVAE\n0.781 \u00b1 0.016\n0.170 \u00b1 0.072\n0.057 \u00b1 0.039\n0.314 \u00b1 0.072\nVAE + TopDis (ours)\n0.833 \u00b1 0.068\n0.200 \u00b1 0.119\n0.065 \u00b1 0.009\n0.394 \u00b1 0.132\n3D Shapes\nVAE\n1.0 \u00b1 0.0\n0.729 \u00b1 0.070\n0.160 \u00b1 0.050\n0.952 \u00b1 0.023\nVAE + TopDis (ours)\n1.0 \u00b1 0.0\n0.835 \u00b1 0.012\n0.216 \u00b1 0.020\n0.977 \u00b1 0.023\n3D Faces\nVAE\n0.96 \u00b1 0.03\n0.525 \u00b1 0.051\n0.059 \u00b1 0.013\n0.813 \u00b1 0.063\nVAE + TopDis (ours)\n1.0 \u00b1 0.0\n0.539 \u00b1 0.037\n0.063 \u00b1 0.011\n0.831 \u00b1 0.023\nThe reconstruction loss for VAE+TopDis: 9.54\u00b10.19 (dSprites), 3489.533\u00b11.502 (3DShapes), 1376.218\u00b10.316 (3DFaces)\nAs we see, VAE+TopDis outperformed VAE as measured by the 4 disentanglement metrics.\nIn the experiments in Table 6 in Appendix G the training procedure was identical to that of Table 1 in the main paper, except that we have used a slightly smaller architecture in Table 6 for dSprites for both VAE and VAE+TopDis-C compared with the experiments from Table 1. For this reason, the metrics values differed in Table 1 and Table 6 for VAE at dSprites dataset. However, we highlight that the experimental setup for 3D Shapes is identical in both Table 1 and Table 6. And that our setup is always consistent when we compare a base model with and without TopDis loss.\nAlso, our main objective was to improve the SOTA methods, and that is what we achieved. The TopDis loss  have consistently improved the performance of \u03b2-VAE, \u03b2-TCVAE, ControlVAE, FactorVAE and DAVA models in terms of disentanglement scores (MIG, FactorVAE score, SAP score, DCI disentanglement score) while preserving the reconstruction quality.\nConcluding remarks\n. Please respond to our post to let us know if the clarifications above suitably address your concerns about our work. We are happy to address any remaining points during the discussion phase; if the responses above are sufficient, we kindly ask that you consider raising your score."}]}, {"Heading": "Official Review of Submission9426 by Reviewer VGnt", "Subheading": "Official ReviewbyReviewer VGnt30 Oct 2023, 22:51 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper proposed a disentanglement regularization term based on topology, to constrain the manifold relation between the latent points of original images and shifted images. The authors provided extensive experiments on VAE-based methods and showed the effectiveness of the proposed methods.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nIt is important to explore the constrain in the manifold of latent space for disentanglement, due to the statistical arguments of Locatello et al. (2019). The paper explored a way from topology and proposed a regularization term, which can be easily optimized.\nThe paper provided a good formulation of the TopDis loss and how to optimize it in the VAE framework.\nWeaknesses:\nThe relation between the constrain on latent space and disentanglement is still unclear, the TopDis is based on VAE-framework, which is based on Probability, and the paper referred to the definition of disentanglement based Group. And the paper failed to connect the above two framework, and making the proposed TopDis only kind of an intuitive necessary condition, as shown in Figure 3.\nFrom Appendix L, the best performance hyperparameters are quite different across different methods and different datasets, is there any guidance or criterion to choose the hyper-parameter?\nQuestions:\nMy main concern is the relation between the proposed TopDis and disentanglement, is there any theoretical guarantee or deduction?\nThe authors applied the proposed TopDis to infer disentangled directions in a pretrained style-GAN, is there some quantitative results? Then dose the method can be applied to other disentangled methods?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to the Reviewer VGnt, #1", "Subheading": "Official CommentbyAuthors23 Nov 2023, 05:01 (modified: 23 Nov 2023, 05:14)EveryoneRevisions", "Content": "Comment:\nThank you for your time and thorough review. We will improve the presentation according to the suggestions. Below we address specific concerns one by one.\nW1\n:\nThe relation between the constrain on latent space and disentanglement is still unclear, the TopDis is based on VAE-framework, which is based on Probability, and the paper referred to the definition of disentanglement based Group. And the paper failed to connect the above two framework,and making the proposed TopDis only kind of an intuitive necessary condition,as shown in Figure3.\nA\n: Thank you for the valuable feedback.\n1.We are adding a clarification on the relation of the probability based definition of disentanglement and the group based definition. Let $q(z)$ be the  aggregate posterior distribution over a latent space, aggregated over the whole dataset $X$. And let $q(z_i)$ be the similar aggregate distribution over the latent code $z_i$. The formula (1) is  valid and defines symmetry group(oid) shifts if we replace the standard normal distribution by any distribution over the real line, we use it with the distribution $q(z_i)$ over the $i-$th latent codes.\nProposition.\na) If the distribution $q(z)$ is factorized into product $q(z)=\\prod_i q(z_i)$, then the shift defined by the formula (1) and acting on a single latent code $z_i$ and leaving other latent codes fixed,  preserves the latent space distribution $q(z)$. This defines the $G_i$ groupoid action on $z$ for any $i$, whose action is then extended to points of the initial dataset $X$ with the help of the decoder-encoder.  b) Conversely, if $q(z)$ is preserved for any $i$ by the shifts acting on $z_i$ and defined via formula (1) from the distribution $q(z_i),$ then $q(z)=\\prod_i q(z_i)$.\nProof.\na) The shift defined by (1) for the distribution $q(z_i)$ acting on the latent space, preserves also any $q(z_j)$ for $ j\\neq i$. b) The result follows from the case of an arbitrary distribution over a pair of random variables $z_1, z_2$. For two variables, it follows from the Bayes formula that the shifts of $z_1$ preserve the conditional $q(z_2\\vert z_1)$. Since the group(oid) action is transitive it follows that the conditional does not depend on $z_1$, and hence $q(z_1,z_2)=q(z_1)q(z_2)$.\nTo the best of our knowledge, we are the first to impose the preservation of the distributions under the symmetry groupoid action condition in the standard normal distribution variational autoencoder   framework. On the one hand, this implies that such symmetry action is necessarily defined not by a group but by the groupoid and also using the specific shifts as in equation (1). On the other hand, this distribution preservation by groupoid is the main ingredient making the two definitions of disentanglement compatible. \nWe are adding this result clarifying the correspondence between the two definitions to the paper.\n2.We are adding a clarification deducing the smallness of RTD from the disentanglement conditions and properties of symmetry action on manifolds.Briefly, the Lie group(oid) symmetry $g$ action on the support of data distribution is continuous and invertible. This implies that for any subset of the support of data distribution, the image of the subset under $g$ has the same homology or the same group of topological features. The preservation of topological features at multiple scales can be tested with the help of the representation topology divergence (RTD). If RTD is small between a sample from $X$ and its symmetry shift, then the groups of topological features at multiple scales are preserved.\nAlso the smallness of RTD implies the smallness of the disentanglement measure from (Zhou et al. 2020) based on the geometry scores of data subsets conditioned to a fixed value of a latent code. Such subsets for different fixed values of the latent code are also related via the symmetry shift action, and if RTD between them is small, the distance between their persistence diagrams and hence the metric from loc cit is small as well.\nW2\n:\nFrom Appendix L, the best performance hyperparameters are quite different across different methods and different datasets, is there any guidance or criterion to choose the hyper-parameter?\nA\n:  In our experiments, we used the following greedy procedure to tune the weight $\\gamma$ of the TopDis loss. First, we tune hyperparameters of baseline approaches (i.e. beta-VAE, FactorVAE, ControlVAE, etc.), by selecting ones having the high disentanglement quality while keeping reasonable reconstruction error. A minor exception is the DAVA model, where the hyperparameters are tuned adaptively during training. Then, we select the best $\\gamma$ weight for TopDis from the range [1, 6] by the disentanglement quality. We observe that optimal $\\gamma$ often coincide for different datasets.\n(cont'd below)"}, {"Heading": "Response to the Reviewer VGnt, #2", "Subheading": "Official CommentbyAuthors23 Nov 2023, 05:01 (modified: 23 Nov 2023, 05:22)EveryoneRevisions", "Content": "Comment:\nQ1\n:\nMy main concern is the relation between the proposed TopDis and disentanglement, is there any theoretical guarantee or deduction?\nA\n: Thank you for the question. (See the answer to W1.)  Firstly, we add the proposition establishing the relation of the probability based definition of disentanglement and the group based definition. Secondly, we add the clarification deducing the TopDis minimization from the disentanglement conditions and properties of symmetry groupoid action, see the answer to W1.\nQ2\n:\nThe authors applied the proposed TopDis to infer disentangled directions in a pretrained style-GAN, is there some quantitative results? Then dose the method can be applied to other disentangled methods?\nA\n: Comparison of methods dedicated to the unsupervised discovery of disentangled directions in StyleGAN is qualitative since the FFHQ dataset doesn't have labels. Our goal is to demonstrate the applicability of the TopDis loss for this problem. See section 5.3 for details.\nConcluding remarks\n. Please respond to our post to let us know if the clarifications above suitably address your concerns about our work. We are happy to address any remaining points during the discussion phase; if the responses above are sufficient, we kindly ask that you consider raising your score."}, {"Heading": "Official Comment", "Subheading": "Official Commentby23 Nov 2023, 05:05 (modified: 23 Nov 2023, 05:11)EveryoneRevisions", "Content": "[Deleted]"}]}, {"Heading": "Official Review of Submission9426 by Reviewer 2nX5", "Subheading": "Official ReviewbyReviewer 2nX530 Oct 2023, 14:31 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposed a novel Topological Disentanglement loss (TopDis loss) that can be added to any VAE-type loss to improve the disentanglement by encouraging the preservation of topological similarity in the generated samples with shifted latent space. Experiments demonstrated the proposed TopDis loss increases the disentanglement performance of several SOTA methods for various disentanglement metrics and datasets.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n3 good\nStrengths:\n(1) Inspired by [1], the proposed differentiable Representation Topology Divergence (RTD) as a loss for the VAE-framework looks promising to improve the disentanglement.\n(2) Rich experiments are conducted to evaluate the performance of the proposed TopDis loss for various VAE-based methods.\n[1] Barannikov, Serguei, et al. \"Representation topology divergence: A method for comparing neural network representations.\" ICML 2022.\nWeaknesses:\n(1) It is unclear how the hyper-parameters in Eqn (4) affect the performance. There are \u03b3_1 and \u03b3_2 in Table 9 (appendix N), but there is only one \u03b3 in Eqn (4).\n(2) In Table 1, it seems that some advanced disentanglement methods performed significantly worse than the vanilla VAE (e.g. FactorVAE on 3dshapes, and \u03b2-TCVAE on MPI3D, etc), making it a little suspicious for the experimental results and/or the model selections of baselines. Besides, two important evaluations of VAE+TopDis and \u03b2-TCVAE+TopDis are missing.\n(3) The evaluation of how the proposed methods handle the tradeoff between disentanglement and reconstruction is limited. Besides Table 4 and Table 8, the authors are encouraged to report the reconstruction errors of the proposed method with and without \"gradient orthogonalization\" for a complete comparison with the baselines. Did the \"gradient orthogonalization\" apply to the baselines as well?\nQuestions:\n(1) The authors are encouraged to respond to the concerns above.\n(2) How the \u03b3 should be selected for different VAE-based methods? Does TopDis improve disentanglement when \u03b2 is already very large? How does the TopDis loss affect the optimization of the original disentanglement loss in those baselines (like the total correction in TC-VAE and FactorVAE)?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to the Reviewer 2nX5, #1", "Subheading": "Official CommentbyAuthors22 Nov 2023, 11:28 (modified: 22 Nov 2023, 13:47)EveryoneRevisions", "Content": "Comment:\nW1\n:\nIt is unclear how the hyper-parameters in Eqn (4) affect the performance. There are \u03b3_1 and \u03b3_2 in Table 9 (appendix N), but there is only one \u03b3 in Eqn (4).\nA\n: Thank you for your remark. The sensitivity w.r.t. to $\\gamma$ in Eqn (4) is provided in Appendix Q.\nIn Appendix Q, $\\gamma_1$ denotes the weight for Total Correlation loss from the FactorVAE model while $\\gamma_2$ denotes the weight for TopDis loss from the equation (4). We have added the necessary clarification to the Appendix Q and renamed $\\gamma_1$ to $\\gamma_{TC}$ (stands for Total Correlation) and $\\gamma_2$ to $\\gamma_{TD}$ (stands for TopDis) as more suitable ones.\nW2\n:\nIn Table 1, vanilla VAE performed better than some other methods (e.g. FactorVAE on 3d Shapes, and \u03b2-TCVAE on MPI3D, etc), evaluations of VAE+TopDis and \u03b2-TCVAE+TopDis are not included in Table 1.\nA\n:  Thank you for your remarks.\nWe provide evaluations of \u03b2-TCVAE+TopDis and we have updated Table 1 accordingly.\nAddition of TopDis improves \u03b2-TCVAE as measured by the 4 disentanglement metrics.\nMethod\nFactorVAE score\nMIG\nSAP\nDCI, dis.\ndSprites\n\u03b2-TCVAE\n0.810 \u00b1 0.058\n0.332 \u00b1 0.029\n0.045 \u00b1 0.004\n0.543 \u00b1 0.049\n\u03b2-TCVAE + TopDis (ours)\n0.821 \u00b1 0.034\n0.341 \u00b1 0.021\n0.051 \u00b1 0.004\n0.556 \u00b1 0.042\n3D shapes\n\u03b2-TCVAE\n0.909 \u00b1 0.079\n0.693 \u00b1 0.053\n0.113 \u00b1 0.070\n0.877 \u00b1 0.018\n\u03b2-TCVAE + TopDis (ours)\n1.0 \u00b1 0.0\n0.751 \u00b1 0.051\n0.147 \u00b1 0.064\n0.901 \u00b1 0.014\n3D Faces\n\u03b2-TCVAE\n1.0 \u00b1 0.0\n0.568 \u00b1 0.063\n0.060 \u00b1 0.017\n0.822 \u00b1 0.033\n\u03b2-TCVAE + TopDis (ours)\n1.0 \u00b1 0.0\n0.591 \u00b1 0.058\n0.062 \u00b1 0.011\n0.859 \u00b1 0.031\nMPI 3D\n\u03b2-TCVAE\n0.365 \u00b1 0.042\n0.174 \u00b1 0.018\n0.080 \u00b1 0.013\n0.225 \u00b1 0.061\n\u03b2-TCVAE + TopDis (ours)\n0.496 \u00b1 0.039\n0.280 \u00b1 0.013\n0.143 \u00b1 0.009\n0.340 \u00b1 0.055\nAlso, we are running experiments with VAE+TopDis, please find the results below.\nWe see that VAE+TopDis outperformed VAE as measured by the 4 disentanglement metrics.\nMethod\nFactorVAE score\nMIG\nSAP\nDCI, dis.\ndSprites\nVAE\n0.781 \u00b1 0.016\n0.170 \u00b1 0.072\n0.057 \u00b1 0.039\n0.314 \u00b1 0.072\nVAE + TopDis (ours)\n0.833 \u00b1 0.068\n0.200 \u00b1 0.119\n0.065 \u00b1 0.009\n0.394 \u00b1 0.132\n3D Shapes\nVAE\n1.0 \u00b1 0.0\n0.729 \u00b1 0.070\n0.160 \u00b1 0.050\n0.952 \u00b1 0.023\nVAE + TopDis (ours)\n1.0 \u00b1 0.0\n0.835 \u00b1 0.012\n0.216 \u00b1 0.020\n0.977 \u00b1 0.023\n3D Faces\nVAE\n0.96 \u00b1 0.03\n0.525 \u00b1 0.051\n0.059 \u00b1 0.013\n0.813 \u00b1 0.063\nVAE + TopDis (ours)\n1.0 \u00b1 0.0\n0.539 \u00b1 0.037\n0.063 \u00b1 0.011\n0.831 \u00b1 0.023\nIndeed, vanilla VAE performed better than some other methods in some cases. This is consistent with actual papers: superiority of Vanilla VAE w.r.t beta-VAE for MPI3D was also observed by Locatello et al, 2020 (see Figure 7, bottom row). We have decided to double-check them and will report results in Appendix, as in most recent papers only more advanced methods were used but not the vanilla VAE baseline. Also we trained the models for 1 million iterations compared to 300 000 iterations used in several other works which can be a possible source of disagreement.\nWe would like to emphasize that our main objective was to improve the SOTA methods, and that is what we achieved. The TopDis loss have consistently improved the performance of \u03b2-VAE, \u03b2-TCVAE, ControlVAE, FactorVAE and DAVA models in terms of disentanglement scores (MIG, FactorVAE score, SAP score, DCI disentanglement score) while preserving the reconstruction quality."}, {"Heading": "Response to the Reviewer 2nX5, #2", "Subheading": "Official CommentbyAuthors22 Nov 2023, 11:31Everyone", "Content": "Comment:\nW3\n:\nThe evaluation of how the proposed methods handle the tradeoff between disentanglement and reconstruction is limited. Besides Table 4 and Table 8, the authors are encouraged to report the reconstruction errors of the proposed method with and without \"gradient orthogonalization\" for a complete comparison with the baselines. Did the \"gradient orthogonalization\" apply to the baselines as well?\nA\n: Thank you for your question and remark. Figure 17 in Appendix illustrates the positive effect of gradient orthogonalization on reconstruction error during training. We do not apply gradient orthogonalization in each experiment but only when the proposed loss can slightly hinder the reconstruction objective.. In our experiments, we orthogonalize the gradient of the proposed TopDis loss w.r.t. the reconstruction loss. Since there is a known tradeoff between reconstruction quality and disentanglement, our primary goal is to develop an additional loss that would enhance the underlying base model but would not result in increase of reconstruction error. While in our experiments we do not apply gradient orthogonalization separately for the baseline models, it is possible to apply the gradient orthogonalization of disentanglement-promoting loss (e.g. Total Correlation for FactorVAE, additional KL-divergence for BetaVAE, etc.) for the baseline model. While a similar technique is known, to the best of our knowledge, it has not been used in the context of learning disentangled representations before.\nQ2\n:\nHow the \u03b3 should be selected for different VAE-based methods? Does TopDis improve disentanglement when \u03b2 is already very large?\nA\n: Thank you for your questions.\nIn our experiments, we used the following greedy procedure to tune the weight $\\gamma$ of the TopDis loss. First, we tune hyperparameters of baseline approaches (i.e. beta-VAE, FactorVAE, ControlVAE, etc.), by selecting ones having both high disentanglement quality and reasonable reconstruction error. A minor exception is the DAVA model, where the hyperparameters are tuned adaptively during training. Then, we select the best $\\gamma$ from the range [1, 6] by the disentanglement quality. We observe that optimal $\\gamma$ often coincide for different datasets.\nHigh values of $\\beta$ typically lead to high reconstruction error, we do not explore this setup in our experiments. Our goal is to enhance the underlying model to provide both good disentanglement and reconstruction quality.\nConcluding remarks\n. Please respond to our post to let us know if the clarifications above suitably address your concerns about our work. We are happy to address any remaining points during the discussion phase; if the responses above are sufficient, we kindly ask that you consider raising your score."}]}, {"Heading": "Official Review of Submission9426 by Reviewer T3Gf", "Subheading": "Official ReviewbyReviewer T3Gf29 Oct 2023, 06:56 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nIn this paper, the authors propose a method, named TopDis (Topological Disentanglement), for learning disentangled representations via adding a multi-scale topological loss term. The experiments results show that the proposed TopDis loss improves disentanglement scores such as MIG, FactorVAE score, SAP score and DCI disentanglement score with respect to state-of-the-art results while preserving the reconstruction quality.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThis paper is the first to introduce the use of a topological regularization term in the field of disentangled representation learning.\nThe topological regularization term is shown to be effective across multiple VAE models and metrics.\nThe regularization term proposed in this paper is also demonstrated to be effective for discovering pre-trained StyleGAN models.\nWeaknesses:\nThe paper lacks a clear reasonable explanation as to why topological constraints are meaningful/effective for disentanglement representation learning.\nThe new loss function was already proposed in a 2022 ICML paper [a]. The main contribution of this work is applying it to disentanglement, making the explanation of the above issue crucial for this paper.\nThe experiments focus on models with some disentanglement capabilities, but the effectiveness of this regularization term on vanilla VAEs has not been studied.\nThe performance of vanilla VAEs presented in this paper show high DCI performance, but other papers [b] report poor performance instead. A reasonable explanation is needed, and it would be helpful to include evaluation code in the supplementary materials.\n[a] Representation Topology Divergence: A method for comparing neural network representations.\n[b] \u03b2-VAE: LEARNING BASIC VISUAL CONCEPTS WITH A CONSTRAINED VARIATIONAL FRAMEWORK\n[c] Disentangling by Factorising\nQuestions:\nSee weakness\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to the Review T3Gf #1", "Subheading": "Official CommentbyAuthors22 Nov 2023, 23:14 (modified: 22 Nov 2023, 23:21)EveryoneRevisions", "Content": "Comment:\nThank you for your time and the thorough review. We will improve the presentation according to the suggestions. Below we address specific concerns one by one.\nW1\n:\nThe paper lacks a clear reasonable explanation as to why topological constraints are meaningful/effective for disentanglement representation learning.\nA\n: Thank you for the feedback. We are adding a clarification deducing the smallness of RTD from the  disentanglement conditions and topological properties of symmetry action.\nHere is the principal explanation.  The Lie group(oid) symmetry $G$ action on the support of data distribution is continuous and invertible. This implies that for any subset of the support of data distribution, the image of the subset under symmetry  $g\\in G$ has the same homology or the same group of topological features. The preservation of topological features at multiple scales can be tested with the help of the representation topology divergence (RTD). If RTD is small between a sample from $X$ and its symmetry shift, then the groups of topological features at multiple scales under such symmetry $G$ action are preserved.\nAlso the smallness of RTD implies the smallness of the disentanglement topological measure from (Zhou et al 2021) based on the geometry scores of data subsets conditioned to a fixed value of a latent code. Such subsets for different fixed values of the latent code are also related via the symmetry shift action, and if RTD between them is small, then the distance between their persistence diagrams and hence the metric from loc cit is small as well.\nW2\n:\nThe new loss function was already proposed in a 2022 ICML paper [a]. The main contribution of this work is applying it to disentanglement, making the explanation of the above issue crucial for this paper.\nA\n: Thank you for the remark. As we mentioned in Appendix, the paper [a]  describes briefly an application  of topological metric to evaluation of interpretable directions in a simple synthetic dataset. They compare topological dissimilarity in data submanifolds corresponding to slices in the latent space conditioned by a fixed latent code value.\nIn our work, we propose the new differentiable loss function TopDis (Eqn. 3), which measures the topological discrepancy, as evaluated by RTD, between an arbitrary data sample from decoder and its symmetry shift obtained via group(oid) action shifts preserving the Gaussian distribution.\nW3\n:\nThe experiments focus on models with some disentanglement capabilities, but the effectiveness of this regularization term on vanilla VAEs has not been studied.\nA\n: Thank you for the valuable feedback. Here are the results for VAE+TopDis.\nAs these results demonstrate,  VAE+TopDis outperformed VAE as measured by the 4 disentanglement metrics.\nMethod\nFactorVAE score\nMIG\nSAP\nDCI, dis.\ndSprites\nVAE\n0.781 \u00b1 0.016\n0.170 \u00b1 0.072\n0.057 \u00b1 0.039\n0.314 \u00b1 0.072\nVAE + TopDis (ours)\n0.833 \u00b1 0.068\n0.200 \u00b1 0.119\n0.065 \u00b1 0.009\n0.394 \u00b1 0.132\n3D Shapes\nVAE\n1.0 \u00b1 0.0\n0.729 \u00b1 0.070\n0.160 \u00b1 0.050\n0.952 \u00b1 0.023\nVAE + TopDis (ours)\n1.0 \u00b1 0.0\n0.835 \u00b1 0.012\n0.216 \u00b1 0.020\n0.977 \u00b1 0.023\n3D Faces\nVAE\n0.96 \u00b1 0.03\n0.525 \u00b1 0.051\n0.059 \u00b1 0.013\n0.813 \u00b1 0.063\nVAE + TopDis (ours)\n1.0 \u00b1 0.0\n0.539 \u00b1 0.037\n0.063 \u00b1 0.011\n0.831 \u00b1 0.023\nWe had also described the experiments for VAE vs VAE+TopDis-C models in Appendix in Table 6.\n(cont'd below)"}, {"Heading": "Response to the Review T3Gf #2", "Subheading": "Official CommentbyAuthors22 Nov 2023, 23:17Everyone", "Content": "Comment:\nW4\n:\nThe performance of vanilla VAEs presented in this paper show high DCI performance, but other papers [b] report poor performance instead. A reasonable explanation is needed, and it would be helpful to include evaluation code in the supplementary materials.\nA\n: Thanks for the constructive comment. Our results are somewhat consistent with the literature: the superiority of Vanilla VAE w.r.t beta-VAE for MPI3D was also observed by Locatello et al, 2020 (see Figure 7, bottom row). In [b], beta-VAE performed better than VAE on dSprites which is also consistent with our results. \nWe note that our experimental setup is different from what is used in some other works. In particular, we train the models for 1 million iterations compared to 300 000 iterations used in other works.  We are going to double check experimental results concerning vanilla VAE on MPI3D.\nWe would like to emphasize that our main objective was to improve the SOTA methods, and that is what we achieved. The TopDis loss have consistently improved the performance of \u03b2-VAE, \u03b2-TCVAE, ControlVAE, FactorVAE and DAVA models in terms of disentanglement scores (MIG, FactorVAE score, SAP score, DCI disentanglement score) while preserving the reconstruction quality.\nConcluding remarks\n. Please respond to our post to let us know if the clarifications above suitably address your concerns about our work. We are happy to address any remaining points during the discussion phase; if the responses above are sufficient, we kindly ask that you consider raising your score."}, {"Heading": "Official Comment by Reviewer T3Gf", "Subheading": "Official CommentbyReviewer T3Gf23 Nov 2023, 03:06Everyone", "Content": "Comment:\nThank you for your response, some of the concerns are addressed. However, I still have the following questions:\nEqn. 3 is defined as the RTD between the sampled images between the original data and the shifted data, what is the difference between RTD in this paper and in [1]?\nThe DCI performance of the vanilla VAE on 3D Shapes is very high (0.95), which outperforms most of the method proposed recently. I have concern in the evaluation of DCI metric.\nDoes the clarification:\nA: Thank you for the feedback. We are adding a clarification deducing the smallness of RTD from the disentanglement co...\nadd in the main paper? I think it is important to add this part into the main paper."}, {"Heading": "Response to the Review T3Gf", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:02 (modified: 23 Nov 2023, 06:37)EveryoneRevisions", "Content": "Comment:\nThank you for your valuable feedback.\nIn [1], the topological metrics is measured between samples whose latent codes necessarily belong to a hyperplane conditioned on the fixed value of one of $z_i$. During the training, the latent codes of data points obtained from the encoder never lie on a hyperplane, so this approach is not applicable for the optimization of the topological loss term. Instead, we work with an arbitrary data points sample obtained from the encoder on which acts the symmetry group(oid) shift.\nFor DCI calculation, we used the evaluation code from the commonly used disentanglement lib:\nhttps://github.com/google-research/disentanglement_lib\n.\nWe note that we trained the models for 1 million iterations compared to 300k iterations used in several other works which can be a possible source of disagreement.\nAlso, in most recent papers only more advanced methods were compared but not the vanilla VAE baseline.\nWe have added this and other improvements to the main text, the additions are highlighted by yellow in the revision. We are also adding, as a byproduct, the proposition clarifying the relation between the two approaches to the definition of disentanglement, the factor-independence based and the symmetry based.\nConcluding remark\n. Please respond to our post to let us know if the clarifications above suitably address your concerns about our work. We are happy to address any remaining points; if the responses above are sufficient, we kindly ask that you consider raising your score."}]}]}, "iS7c9lkXuF": {"paper_info": {"Keywords": "NeRF, robust learning", "TL;DR": "Enhancing robustness in NeRF through the use of influence functions, a classic technique in robust learning, and segmentation method to improve the precision of noise detection.", "Abstract": "Neural Radiance Fields (NeRF) is a method for 3D scene modeling that employs fully-connected networks to learn 3D geometric information and synthesizes high-quality novel views. However, NeRF exhibits vulnerability when confronted with distractors in the training images, such as the presence of moving objects like pedestrians or different weather conditions within specific views. Given the difficulty of data curation in NeRF compared to other domains, training a robust model that maintains 3D consistency is an important and timely challenge. Previous approaches have attempted to differentiate distractors by using loss values, but there is a fundamental limitation that hard-to-learn pixels like high-frequency details also show high loss values. In this paper, we propose a noise pruning framework via influence functions to effectively filter out noisy pixels, ultimately enhancing the robustness of NeRF. Furthermore, we improve the precision of detection by incorporating segmentation techniques to refine pixel-level predictions. Our method demonstrates superior performance on benchmark datasets, including synthetic and natural scenes, showcasing its effectiveness across various environments and proficiency in dataset pruning.", "Primary Area": "representation learning for computer vision, audio, language, and other modalities", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9421", "PDF Url": "https://openreview.net/pdf?id=iS7c9lkXuF"}, "review_info": [{"Heading": "Official Review of Submission9421 by Reviewer yQMB", "Subheading": "Official ReviewbyReviewer yQMB31 Oct 2023, 00:12 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nAs there would be dynamic distractors in the scene which usually occlude the objects-of-interest, NeRF models could not reconstruct the 3D scene correctly. This paper adopts Influence Functions to evaluate each pixel in the training set and finds out the distractor pixels. To further improve the consistency in each image plane, it also integrates a segmentation method, i.e., SAM. The experimental results verify the feasibility of the proposed method.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n1 poor\nStrengths:\nThis paper firstly adopts Influence Functions to find the distractors in the data for NeRF reconstruction, which provides a new method to alleviate this difficulty.\nThe paper is easy to understand and the presentation is acceptable.\nWeaknesses:\n-- The novelties are limited. As the key idea of this paper is adopting an existing method, i.e., Influence Functions, to improve the robustness of NeRF reconstruction, it is acceptable if the performance gain is remarkable. Unfortunately, this is not true according to the experiments.\n-- There are many confusing results and settings in the experiments, which significantly reduce their credibility. In Tab. 1, there are no results of RobustNeRF. Acctually, according to the RobustNeRF paper, it achieves better performance overall. In Tab. 4, the results of RobustNeRF are not consistent with the results reported by the RobustNeRF paper. Moreover, the results of Crab scene are missed.\nQuestions:\nWhat are the principles of the experimental settings? Why are some results missed or inconsistent?\nThis paper has not reported the computation cost of the method. It seems really expensive to compute the Hessian matrix.\nI have not found the appendix.\nFlag For Ethics Review:\nNo ethics review needed.\nDetails Of Ethics Concerns:\nNone.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9421 by Reviewer RkM8", "Subheading": "Official ReviewbyReviewer RkM830 Oct 2023, 14:49 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposes a noise pruning pipeline for NeRF learning, aimed at training NeRF on images with distortion. Specifically, it proposes to first use inference functions to determine the potential pixels that are considered as distortion. Then, it segments out regions that include pixels considered as distortion using the SAM model. Finally, it removes the segmented-out pixels from the training images to reduce the inference of distortion. The paper then conducts experiments on the RobustNeRF dataset and a synthetic dataset.\nSoundness:\n2 fair\nPresentation:\n1 poor\nContribution:\n2 fair\nStrengths:\nLearning NeRF in a robust manner is important and useful.\nUsing the inference function to determine the distortion seems reasonable.\nWeaknesses:\nI believe the proposed pipeline may have limitations in real-world applications. In real-world scenarios, noise and distortion can originate from various sources, such as inaccurate camera calibration, motion blur, out-of-focus blur, JPEG compression noise, and more. Simply discarding the information from these inconsistent pixels by segmenting out the entire region may result in significant information loss, especially in cases like inaccurate camera calibration and motion blur. I would recommend that the author reconsider this approach and seek to improve it.\nThe presentation of the results is limited. It is challenging to assess the effectiveness of the approach based on the content in the current version of the paper. I suggest that the author include more qualitative results, as there is more than half a page remaining empty.\nThe Method section is not well-written, containing numerous typos and mistakes. For example, the line following equation 5 states \"loss is defined as,\" but it proceeds to show how theta is computed. Additionally, the notion of I(r, r) appears to be unscientific.\nQuestions:\nHow does the SAM is actually prompted? Is there a non-maximum suppression process used on top of the selected pixels?\nWhat will happen if this approach is applied to clean images without any noise? Since it seems a fixed number of pixels are considered as noise in stage 1.\nWhat if this approach is applied to JPEG compression noise?\nFlag For Ethics Review:\nNo ethics review needed.\nDetails Of Ethics Concerns:\nNo concerns.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9421 by Reviewer hbFT", "Subheading": "Official ReviewbyReviewer hbFT29 Oct 2023, 17:43 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper presents a method to identify distractors during the NeRF reconstruction. With the help of SAM segmentation, it can effectively group the segments belonging to the same objects while ignoring the distractors. Experimental results shown that the proposed method is effective in improving the NeRF reconstruction quality.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThe proposed method is simple, and it demonstrated better results on multiple NeRF dataset.\nWeaknesses:\nThe proposed method is highly incremental and it is a straight forward extension of RobustNeRF with SAM segmentation. There are not much analyses why there are distractors and how sensitive is the proposed method against the segmentation accuracy. It basically assume the SAM segmentation is good enough such that the segments belong to the same object can be grouped successfully using the proposed method in Sec. 3. Although it demonstrate some good results, the scene are relatively simple which has only one dominant object. I am not fully convinced that the method proposed in Sec. 3 is general enough for handling complex scene with many objects. I am also not convinced that the proposed method can handle tiny objects effectively since the tiny objects are likely to be filtered out by the proposed method as distractors.\nConsidering the limited technical novelty and the potential problems on complex scene and tiny objects, I am not convinced that the submission has reach the bar of acceptance.\nQuestions:\nPlease provide additional experiments or examples to convince that the proposed method is effective in handling complex scenes and tiny objects.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}]}, "E5CMyG6jl0": {"paper_info": {"Primary Area": "generative models", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Large Language Model, Alignment, Point-wise preference", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "Language model alignment is a cutting-edge technique in large language model training to align the model output to user's intent, e.g., being helpful and harmless. Recent alignment framework consists of two steps: supervised fine-tuning with demonstration data and preference learning with human preference data. Previous preference learning methods, such as RLHF and DPO, mainly focus on pair-wise preference data. However, in many real-world scenarios where human feedbacks are intrinsically point-wise, e.g., upvotes number or binary criterion, effective model alignment to user preference is under explored. In this paper, we fill this gap by developing a simplified tuning method for point-wise preference data. Further revelation on the connection between supervised fine-tuning and point-wise preference learning enables us to develop a unified framework for both human demonstration and point-wise preference data, which sheds new light on the construction of preference dataset. Extensive experiments demonstrate the superior performance and efficiency of our proposed methods. A new dataset with high-quality demonstration samples on harmlessness are constructed and made publicly available.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9420", "PDF Url": "https://openreview.net/pdf?id=E5CMyG6jl0"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9420 by Area Chair 9zti", "Subheading": "Meta ReviewbyArea Chair 9zti05 Dec 2023, 20:33 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThe paper presents a novel approach to align language models with point-wise preference data, integrating supervised fine-tuning and preference learning into a unified framework. Strengths include the originality of handling point-wise preference data and unifying alignment frameworks, as well as solid experimental results showing improvements over existing methods. Weaknesses involve limitations in generalization to more complex metrics and a need for broader dataset application. Some concerns about the justification of designed components, remain partially unresolved.\nJustification For Why Not Higher Score:\nThe paper is not recommended for a higher score primarily due to unresolved issues raised by reviewers UxYc and 33oF. While the authors made significant improvements in response to feedback, there remain gaps in the generalization to various metrics and datasets. Moreover, the clarity and justification of certain key components require further elaboration.\nJustification For Why Not Lower Score:\nDespite its limitations, the paper merits acceptance at its current score rather than a lower one. The research presents significant advancements in model alignment with point-wise preference data and demonstrates empirical improvements over existing methods. The issues raised, while pertinent, do not significantly detract from the overall value and novelty of the work."}, {"Heading": "Eagerly look forward to knowing your update", "Subheading": "Official CommentbyAuthors21 Nov 2023, 18:55Everyone", "Content": "Comment:\nRespected Reviewers,\nThe discussion phase is drawing to a close. We eagerly look forward to knowing your update after the initial author response.\nWe are wondering whether your concerns have been well addressed. If you have any additional questions, it would be great if you could let us know. We are readily prepared to address them."}, {"Heading": "Official Review of Submission9420 by Reviewer veNV", "Subheading": "Official ReviewbyReviewer veNV01 Nov 2023, 14:09 (modified: 20 Nov 2023, 10:50)EveryoneRevisions", "Content": "Summary:\nLanguage model alignment is a significant technique to align inference output to human preference and help performance improvement. Currently, alignment mainly involves two steps: supervised fine-tuning with designed instructions and then preference learning with pair-wise samples such as RLHF and DPO method. However, most of the existing preference data in the world are not just pair-wise but more fine-grained, i.e., preference data are voted by scores. In this paper, the authors propose a new DPO method to align LLM with point-wise preference data. Standing on the proposed point-wise DPO method, they incorporate supervised fine-tuning, unifie the whole alignment framework, and solve it as a one-step alignment problem. In their experiments, they compare with RLHF and vanilla DPO and validate the effectiveness of their proposed framework by achieving lower perplexity scores and higher preference scores.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nOriginality: Several existing works to align LLM outputs to human preference have been proposed, such as RLHF and DPO. Standing on DPO, this paper devises a new approach for point-wise preference data to make alignments. Besides, they unify the alignment framework with supervised fine-tuning stage. These two contributions enhances paper\u2019s strength on originality.\nQuality: Numbers in the experiments are solid and look promising, especially the improvements in complexity and preference score (harmful) compared to baseline RLHF.\nClarity: The presentation in this paper is easy to follow and well-organized.\nSignificance: A typical way to do preference learning is to treat generated samples with pair-wise binary relation which losses the granular information on voting scores, rankings, or preference levels. To fill the gap, this paper proposes a new DPO method to align LLM with point-wise preference data. They study the gradients between supervised fine-tuning and their proposed method then propose a novel unified framework to learn human preference. Empirically, their results validate the framework\u2019s effectiveness and show the significance of this work.\nWeaknesses:\nThough the experimental results look promising to demonstrate framework\u2019s effectiveness, more human preference datasets to align LLM should be included, such as datasets provided and used in [1] and [2].\nThe proposed framework should be able to be generalized to more complex metrics (such as the discussion to handle continuous labels) but the datasets used in the experiment are only in binary classes, which is not enough to support the capability of its generalization.\nThe generalization to other metrics with positive and negative samples needs further description in details.\n[1]\nhttps://arxiv.org/abs/2112.09332\n[2]\nhttps://proceedings.mlr.press/v162/ethayarajh22a.html\nQuestions:\nWhat\u2019s the objective loss of ULMA for continuous preference labels?\nIn this case, how does the framework deal with positive and negative samples?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Author Response", "Subheading": "Official CommentbyAuthors19 Nov 2023, 11:23Everyone", "Content": "Comment:\nThank you for the helpful review! We now address your concerns and suggestions as follows.\nQ1:\n\"more human preference datasets to align LLM should be included\"\nA1:\nWe have added two more datasets\nQA-feedback\nand\nred-team\nin the revision. The former dataset is a point-wise dataset with binary labels, and the second dataset is a point-wise dataset with continuous labels. The new empirical results are presented in Table 1 and Table 2 (together with the original results on\nHH\nand\nGolden HH\n). Please refer to the revised manuscript for detailed descriptions and empirical results.\nQ2:\n\"The proposed framework should be able to be generalized to more complex metrics (such as the discussion to handle continuous labels)... the dataset is not enough to support the capability of its generalization\"\nA2:\nIn fact, the core concept of our proposed ULMA framework, i.e., using a hybrid objective formulation for different types of data, is capable of generalizing to more general metrics. On the new\nred-team\ndataset in the revision, we provide an example of how ULMA can be generalized to handle continuous labels.\nSpecifically,\nred-team\nis a point-wise dataset on LLM's robustness to red teaming attacks, which consists of samples scoring from 0 to 4. The scores are rated by human and indicate how successful the attacks are, among which those rated 0 can be considered as high quality demonstration data. To better exploit these data, we introduce a hybrid loss by adding SFT loss of the samples rated 0 to the original MSE loss of all samples.\nIn experiment, we observe that ULMA performs better than Unlikelihood and point-wise RLHF on\nred-team\n(note that pair-wise methods such as RLHF and DPO cannot be applied to this dataset since each prompt only has a single sample), which supports the capability of ULMA's generalization to more complex metrics.\nQ3:\n\"The generalization to other metrics with positive and negative samples needs further description in details\"\nA3:\nGood suggestion! We have detailedly discussed how to generalize ULMA to datasets with continuous labels in Section 4.3 in the revised manuscript (at the end of page 6). Specifically, for these datasets, there is no direct separation of positive and negative samples. To apply ULMA, we can specify some sample as high quality data (e.g., the most helpful or harmless ones) and treat these high quality samples as \"positive\" demonstration data. Then we can adopt a hybrid loss by combining the SFT loss on these demonstration data with the original loss (e.g., MSE) on preference data. A concrete example on the\nred-team\ndataset can be seen in\nA2\n.\nQ4:\n\"What\u2019s the objective loss of ULMA for continuous preference labels? In this case, how does the framework deal with positive and negative samples?\"\nA4:\nThe objective loss for continuous preference labels is a hybrid of SFT loss on high quality demonstration data and the original loss (e.g., MSE) on preference data. In this case, some samples in the dataset (e.g., the most helpful or harmless ones) are treated as the \"positive\" demonstration data and applied to the SFT loss. An example on the\nred-team\ndataset is discussed and empirical evaluated in the revised manuscript. Please refer to our response in\nA2\nand\nA3\nfor more details."}, {"Heading": "Official Comment by Reviewer veNV", "Subheading": "Official CommentbyReviewer veNV20 Nov 2023, 10:49Everyone", "Content": "Comment:\nThanks for addressing my questions and adding more results! Two new datasets and their results look great and promising to me. Especially, the discussion about how this framework extends to continuous label spaces is important. I will raise my score to 6 and support this paper to be accepted."}]}, {"Heading": "Official Review of Submission9420 by Reviewer UxYc", "Subheading": "Official ReviewbyReviewer UxYc01 Nov 2023, 07:58 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nIn this paper, the authors propose a unified language model alignment approach. Their main idea is to address point-wise human preference. I like the idea of studying point-wise human preference. My main concern is that in many cases, there can be a mapping function between pair-wise preference and point-wise preference.\nSoundness:\n3 good\nPresentation:\n4 excellent\nContribution:\n3 good\nStrengths:\nIt is interesting to study the alignments on the point-wise human preference.\nIt is great to compare the existing approach.\nReleasing more datasets is always great for the community.\nWeaknesses:\nIn many cases, there can be a mapping function between pair-wise preference and point-wise preference. The authors do not discuss these cases.\nIt would be great to have more experimental results in terms of more LLM-based tasks.\nThere is no significance test in the tables.\nQuestions:\nI like the idea of studying point-wise human preference. However, one essential issue is that in many cases, there can be a mapping function between pair-wise preference and point-wise preference. For example, from pair-wise -> point-wise: you can directly enumerate how many positive preferences have been received for each document, and then rank the documents according to the numbers and assign a ranking score to each document. Or, a simpler way is to use the number of (positive num \u2013 negative num) preferences as the point-wise preference. Therefore, to verify the idea of studying the point-wise human preference. Similarly, point-wise -> pair-wise, one document with higher scores can receive the positive preference. One must prove that these rule-based methods can not work well for the LLM. Also, to test the performance of an LLM, there are many evaluation metrics and many LLM-based tasks. Therefore, I expect the authors to test the LLM for more tasks and metrics. Also, more LLMs are expected. For the reported tables, many numbers are quite close, and it is necessary to have a significance test to see whether the proposed method is better (or you can report the mean and std for multiple runs). Overall, I like this idea, but this version may not be ready for publication. If you can answer the above question or point out my misunderstanding, I will be happy to raise my score.\nFlag For Ethics Review:\nNo ethics review needed.\nDetails Of Ethics Concerns:\nNA\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Author Response", "Subheading": "Official CommentbyAuthors19 Nov 2023, 11:26Everyone", "Content": "Comment:\nThank you for the insightful review! We acknowledge your valuable suggestions and concerns, which we will address in the following.\nQ1:\n\"In many cases, there can be a mapping function between pair-wise preference and point-wise preference\"\nA1:\nAdmittedly, there exists some relation between pair-wise data and point-wise data as you have suggested, but we shall note that it is\nnot\na one-to-one correspondence, and the transformation may suffer from information loss. Now we explicate the above point in two folds.\n(i) \"Pair-wise to point-wise\" transformation may lose some pair-wise comparison information. For example, consider two pair-wise datasets $\\mathcal D_1=\\{x_1\\succ x_2, x_2\\succ x_3, x_3\\succ x_4\\}$ and $\\mathcal D_2=\\{x_1\\succ x_3, x_3\\succ x_2, x_2\\succ x_4\\}$. Obviously, the two datasets differ from the pair-wise relation between $x_2$ and $x_3$. These two datasets will be transformed to the same point-wise dataset $\\mathcal D_3=\\{(x_1,1), (x_2,0), (x_3,0), (x_4,-1)\\}$, in which $x_2$ and $x_3$ have the same score. In this case, the transformation drops the relation between $x_2$ and $x_3$, which incurs loss in information.\n(ii) \"Point-wise to pair-wise\" transformation is unsuitable to some dataset where no comparison can be made, e.g., all samples have the same score or there is only one sample for some prompt. One example is the\nred-team\ndataset, which only has one sample for each prompt. This dataset cannot be transformed into a pair-wise dataset, and pair-wise methods such as RLHF and DPO are inapplicable. Please see the revised manuscript for more detailed descriptions and empirical results.\nTherefore, the conversion between pair-wise and point-wise datasets is not always possible, and may suffer from information loss in many cases. In the revised manuscript, we have compared the performance of point-wise DPO and point-wise DPO on pair-wise dataset\nGolden HH\nand point-wise dataset\nQA-feedback\n, respectively. We observe that pair-wise DPO performs slightly worse than point-wise DPO on point-wise dataset, and vice versa, which supports our above claim.\nIn addition to the above difference, we shall emphasize that\nour investigation on point-wise dataset has motivated the design of more flexible tuning method\n. Specifically, the absolute labeling on point-wise dataset motivates us to treat different types of samples (e.g., high quality positive data and noisy negative data) in different ways, which results in the ULMA framework. Our investigation on point-wise dataset and proposed new dataset may motivate more elaborate algorithmic design for point-wise preference learning.\nQ2:\n\"more experimental results in terms of more LLM-based tasks\"\nA2:\nWe have added two more datasets\nQA-feedback\nand\nred-team\nin the revision. The former dataset is a point-wise dataset with binary labels, and the second dataset is a point-wise dataset with continuous labels. In particular,\nred-team\nonly has a single answer for each prompt and no comparison can be made, hence it cannot be transformed into a pair-wise dataset and pair-wise methods such as RLHF and DPO are inapplicable. The empirical results are presented in Table 1 and Table 2 in the updated manuscript. Please refer to the manuscript for more details.\nQ3:\n\"There is no significance test in the tables\"\nA3:\nIn the revised manuscript, for each  harmful/helpful score presented in Table 1-3, we report its 95-percent confidence interval after repeating the model training procedure for three times beyond its mean value. The results show that, on the\nred-team\nand\nQA-feedback\ndatasets, ULMA performs\nsignificantly\nbetter than other methods; on the\nHH\nand\nGolden HH\ndatasets, though the confidence intervals are overlapped, the mean value of ULMA is still better. Please refer to the manuscript for detailed empirical results."}]}, {"Heading": "Official Review of Submission9420 by Reviewer iHby", "Subheading": "Official ReviewbyReviewer iHby01 Nov 2023, 00:25 (modified: 24 Nov 2023, 08:41)EveryoneRevisions", "Content": "Summary:\nThis paper introduces a novel model alignment technique to user preferences. The authors have developed a simplified tuning method for point-wise preference data as well as human demonstration.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n4 excellent\nStrengths:\nDetailed background and preliminaries section which serves as a refresher of the main LLM methodologies. This serves as a solid base and leads very well to the proposed methodology.\nDetailed mathematical explanation of the concept.\nWeaknesses:\nThe experiments section is not very detailed. Expanding the methodology to more datasets would be nice.\nQuestions:\nNone\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n8: accept, good paper\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Author Response", "Subheading": "Official CommentbyAuthors19 Nov 2023, 11:28Everyone", "Content": "Comment:\nThank you for the valuable review! We appreciate your positive assessment and will address your concerns in the following.\nQ1:\n\"The experiments section is not very detailed. Expanding the methodology to more datasets would be nice.\"\nA1:\nWe have added two more datasets\nQA-feedback\nand\nred-team\nin the revision. The former dataset is a point-wise dataset with binary labels, and the second dataset is a point-wise dataset with continuous labels. The empirical results are presented in Table 1 and Table 2 in the updated manuscript. Please refer to the manuscript for empirical results and detailed discussions.\nIn particular, on the\nred-team\ndataset, we give an example of expanding the methodology of ULMA, i.e., using a hybrid objective formulation for different types of data, to handle point-wise dataset with continuous labels. Specifically,\nred-team\nis a point-wise dataset on LLM's robustness to red teaming attacks. It consists of samples scoring from 0 to 4, among which those rated 0 can be considered as high quality demonstration data. To better exploit these data, we introduce a hybrid loss by adding SFT loss of the samples rated 0 to the original MSE loss of all samples."}]}, {"Heading": "Official Review of Submission9420 by Reviewer 33oF", "Subheading": "Official ReviewbyReviewer 33oF28 Oct 2023, 03:31 (modified: 20 Nov 2023, 09:10)EveryoneRevisions", "Content": "Summary:\nThis paper presents a unified framework that integrates the two traditionally separate processes in LLM alignment: SFT on demonstration data and preference learning on preference data. The framework is structured for point-wise preference learning, considering the intrinsic characteristics of real-world preference data distribution. Specifically, the authors treat the positive and negative samples differently, applying SFT loss to the former and adding an additional KL regularizer for the latter. They also justify this formulation via gradient analysis and comparison with DPO. Since the method enhances learning from the positive samples, the authors extend the Anthropic's Helpful and Harmless dataset by refining the positive responses using GPT-4, thus boosting their method with high-quality data.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nReward modeling is a crucial and challenging part in LLM alignment. Conventionally, preference data is collected through rankings and used as preferred\u2013dispreferred pairs for learning, as the human-annotated scalar scores on individual samples can be uncalibrated and noisy. However, the ranking-based reward model may fail to impose correct penalty, since it is trained based on binary relative signals, potentially compromising its precision on individual samples. In this case, I agree with the authors that pair-wise RM may inadequately capture the nuances of real-world preference data distribution, especially on the data where preferences are obviously polarized and scoring quality against specific criteria is unambiguous. Therefore, I think it is important to explore the point-wise RM for better preference learning in LLM alignment.\nWeaknesses:\nWhile preference learning from pair-wise data is challenging (as I briefly discussed above), it still applies to most cases in the real world. For example, toxicity is not a strictly binary metric as we can categorize samples to be\ntoxic\n,\nvery toxic\n, or just\npose risks of toxic content generation\n[1]. Also for verifiability, there can be labels such as\nunhedged correct\n,\nhedged correct\n, and\nuninformative\n. So I don\u2019t think the authors made a convincing argument regarding the superiority of their point-wise preference learning over the pair-wise methods. In fact, the binary signals could result in significant information loss, since the learning can only capture the data polarity, omitting the nuanced levels present in practice.\nAdditionally, the paper lacks empirical analysis with limited experimental results to justify the design of each component in Equation (9). It is hard to interpret how the win rates evaluated by GPT-4 correlate with human judgment or the actual quality. For example, if I understand it correctly, the baseline to compare against is the chosen answer in the dataset, which can be considered as the golden samples for preference learning. This makes the numbers of win-rates in Tables 1&2 somewhat weird and vague since there should be a big proportion of tie cases as indicated in previous works [2]. It\u2019d help to report on metrics that are consistent with existing works for clear and interpretable result comparison. It is also important to extend the evaluation to other benchmarks,\ne.g.\n, RealToxicityPrompts [1], to compare their effectiveness at least in the domain of harmlessness.\n[1] Gehman et al. Evaluating Neural Toxic Degeneration in Language Models. Findings of EMNLP 2020\n[2] Rafailov et al. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. 2023\nQuestions:\nA. Could the authors elaborate more on the design of point-wise preference learning, particularly regarding harmlessness and helpfulness? For example, how to deal with potential information loss when simplifing the label to be strictly binary?\nB. The win-rates, especially in Golden HH, are close to $100$%. Could you elaborate on the reasons behind these statistics and also provide information on the corresponding lose- and tie-rates?\nC. How did the authors obtain and evaluate the baseline results? For example, there isn\u2019t an official implementation of DPO, how did the authors ensure that their version of DPO is consistent with the original one, and how does their result align with the reported one in the DPO paper?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment", "Subheading": "Official Commentby19 Nov 2023, 11:31 (modified: 19 Nov 2023, 11:34)EveryoneRevisions", "Content": "[Deleted]"}, {"Heading": "Author Response (Part 1)", "Subheading": "Official CommentbyAuthors19 Nov 2023, 11:33Everyone", "Content": "Comment:\nThank you for the detailed review! In the following, we will address your questions and give clarification on various aspects of this paper.\nQ1:\n\"the binary signals could result in significant information loss, since the learning can only capture the data polarity, omitting the nuanced levels present in practice\"\nA1:\nAdmittedly, as you have pointed out, many datasets on toxicity and verifiability are not binary labelled. Our original statement regarding binary labeling on toxicity and verifiability in Section 4.1 is only meant for providing an example to illustrate the limitation of pair-wise preference learning methods on point-wise preference datasets.\nWe shall clarify that such a statement does not mean that our proposed ULMA is restricted to binary point-wise datasets. In fact,\nwe do not necessarily need to transform continuous labels into binary labels when applying ULMA\n. The core concept of ULMA, i.e., using a hybrid objective formulation to treat the high-quality demonstration data and possibly noisy preference data differently, is capable of generalizing to continuous labels to characterize the different levels.\nSpecifically, for these datasets, there is no direct separation of positive or negative samples as in the binary case. To apply ULMA, we can specify some sample as high quality data (e.g., the most helpful or harmless ones) and treat these high quality samples as \"positive\" demonstration data. Then we can adopt a hybrid loss by combining the SFT loss on these demonstration data with the original loss (e.g., MSE) on preference data.\nIn the revised manuscript, we add the above clarification and also provide a concrete example on the\nred-team\ndataset with continuous labels.\nRed-team\nis a point-wise dataset on LLM's robustness to red teaming attacks. In this dataset, each sample has a score ranging from 0 to 4, which is rated by human and indicates how successful the attack is to the LLM (in Likert scale; an attack with a higher level is more successful).\nRed-team\nis very similar to your mentioned datasets on toxic content generation or verifiability. However, this dataset only has a single sample for each prompt, hence no comparison can be made and pair-wise methods such as RLHF and DPO are inapplicable; we can only apply point-wise methods such as our proposed point-wise DPO.\nTo further improve the performance of point-wise DPO on\nred-team\n, inspired by the ULMA framework for binary cases, we treat the samples of level 0 as the high quality demonstration data. Then we introduce a hybrid loss by adding SFT loss of the samples rated 0 to the original MSE loss of all samples. As presented in Table 1, empirical results show that our proposed ULMA outperforms other methods on\nred-team\n. Please refer to the revised manuscript for more detailed description on the dataset and empirical results.\nQ2:\n\"lacks empirical analysis with limited experimental results to justify the design of each component\"\nA2:\nIn the revised manuscript, we have conducted an ablation study to verify the design of the hybrid objective formulation in ULMA. Specifically, we first use positive samples as demonstration data to compare ULMA with point-wise DPO (which adopts KL regularization for these samples) to evaluate the effectiveness of ULMA for learning from high quality (positive) demonstration data. Then we use negative samples as dispreferred demonstration data to compare ULMA with the counterpart algorithm without KL regularization (i.e., Unlearning) to evaluate the ability of ULMA for learning from possibly noisy (negative) preference data. As presented in Table 3, the results show that ULMA can better exploit the high quality positive samples by applying SFT loss to these samples, while preventing from overfitting to possibly noisy negative samples via adding regularization to negative samples. Please refer to the revised manuscript for more detailed empirical results."}, {"Heading": "Author Response (Part 2)", "Subheading": "Official CommentbyAuthors19 Nov 2023, 11:34Everyone", "Content": "Comment:\nQ3:\n\"the numbers of win-rates in Tables 1 and 2 are somewhat weird and vague... The win-rates, especially in Golden HH, are close to 100 percents\"\nA3:\nThe numbers reported in Table 1 and 2 are actually the\nwin & tie\nrate [1, 2] of the harmful metric. We had made a typo in the original captions of Table 1 and 2. Sorry for causing your confusion! In the revised manuscript, beyond the win & tie rate presented in Table 1 and 2, we also report the detailed win, tie and loss rates respectively in Figure 1 and Figure 2 in Appendix D.\nWe also note that it is reasonable that the win & tie rates on\nGolden HH\nare close to 100%. Here we explain the reason. Since\nGolden HH\nis constructed based on\nHH\n, to make the results on\nGolden HH\nand\nHH\ncomparable, the generated answers are\ncompared to the chosen sample in the original\nHH\ndataset\nwhen evaluating the harmful score on\nGolden HH\n. However, the quality of the chosen samples in\nHH\nis not very high. When employing SFT on\nGolden HH\nwhere the new chosen data are of much higher quality, the answers generated on\nGolden HH\nis very likely to be at least no worse than the original chosen data in\nHH\n. As a consequence, the win & tie rates on\nGolden HH\nare close to 100%. Note that the above phenomenon actually implies that the quality of the chosen samples in\nGolden HH\nis much higher than that in\nHH\n(also see Table 5 in Appendix C for more detailed comparison of both datasets).\nQ4:\n\"It is also important to extend the evaluation to other benchmarks\"\nA4:\nWe have added two more datasets\nQA-feedback\nand\nred-team\nin the revised manuscript. The first dataset\nQA-feedback\nis a point-wise QA dataset with binary labels, in which the perplexity and the helpful score are evaluated. The second dataset\nred-team\nis a point-wise dataset with continuous labels on robustness of LLMs to red teaming attacks, in which we report the perplexity and the harmful score. The empirical results are presented in Table 1 and Table 2 in the updated manuscript, from which we also observe that our proposed ULMA method outperforms other compared methods on both datasets. Please refer to the manuscript for empirical results and detailed discussions.\nQ5:\n\"there isn\u2019t an official implementation of DPO... how does their result align with the reported one in the DPO paper\"\nA5:\nIndeed, in our experiments, we had followed the official implelentation of DPO in\nhttps://github.com/eric-mitchell/direct-preference-optimization\n. The reported numbers are a bit different from those in the DPO's paper, because we adopt the harmful score as the metric on the\nHH\ndataset, which is different from the helpful score adopted by the DPO's paper.\n[1] Duan et al. BOTCHAT: Evaluating LLMs\u2019 Capabilities of Having Multi-Turn Dialogues. ArXiv, abs/2310.13650, 2023.\n[2] Xu et al. Superclue: A Comprehensive Chinese Large Language Model Benchmark. ArXiv, abs/2307.15020, 2023."}, {"Heading": "Official Comment by Reviewer 33oF", "Subheading": "Official CommentbyReviewer 33oF20 Nov 2023, 09:10Everyone", "Content": "Comment:\nThanks for clarifying my concerns and adding new results!\nMy additional comments:\nI noticed that the point-wise DPO presents comparable performance as ULMA. As the authors discussed, ULMA is better since it removes the regularization to enhance high-quality data learning. But from my understanding, the improvement actually depends on the quality of the positive samples and how much they can deviate from the reference model distribution. In the paper, the authors revise the HH data to be Golden HH to have better positive samples. But I am wondering whether this is generalizable to cases where we only have relatively high-quality samples or limited high-quality samples. For example, samples rated 0 by different people may have various qualities when the ratings are subjective. In this case, how would ULMA be impacted by the positive data quality? And how would it perform if given unbalanced positive & negative samples?\nIn Table 3, the GPT4 evaluation score of Unlearning on QA-feedback is $<0.5$. Does this mean that it makes things worse compared with the reference model (training starting point)?"}, {"Heading": "Response to your additional comments", "Subheading": "Official CommentbyAuthors20 Nov 2023, 22:12Everyone", "Content": "Comment:\nThank you for your valuable feedbacks! We would like to address your additional comments as follows.\nQ1:\n\"whether this is generalizable to cases where we only have relatively high-quality samples or limited high-quality samples.\"\nA1:\nAdmittedly, the performance gain of ULMA compared to point-wise DPO increases as the quality of the chosen data gets higher. However, it does not mean that ULMA is restricted to samples of very high quality; the gain still exists on relatively high-quality samples. To empirically verify this point, we construct a new dataset with relatively high-quality samples by replacing\nhalf of the chosen data\nin\nHH\nby\nGolden HH\n, and then compare the performance of point-wise DPO and ULMA on the new dataset. We find that ULMA (win & tie rate 0.93 for the harmful metric) still outperforms point-wise DPO (win & tie rate 0.89 for the harmful metric) on such dataset, which is consistent with the previous results on\nHH\nand\nGolden HH\nin Table 1.\nAs for the case of limited positive samples, we just conducted a new experiment on the red team dataset by discarding two thirds of the positive samples. In this way, the ratio of the positive samples drops from 42.4% to 19.6%, and we find that ULMA (win&tie rate 0.89 for the harmful metric) still outperforms the point-wise DPO (win&tie rate 0.88 for the harmful metric). The above results show that ULMA still has advantage with relatively high-quality samples limited high-quality samples. We will add the above results and discussions in the revision.\nQ2:\n: \"Does this mean that Unlearning makes things worse compared with the reference model (training starting point)\"\nA2:\n: Indeed, in Table 3, the unlearning method performs worse than the reference model. Here we explain the reason. For the purpose of ablation study, in our experiments, the unlearning method does not include pre-training via SFT; it only uses negative samples to fine-tune the pre-trained model. In other words, unlearning does not use high-quality positive samples in preference dataset for model alignment. As the negative samples in the datasets are noisy and of relatively low quality, unlearning without regularization may suffer from\ncatastrophic unlearning\n[2, Fig 3 of 1]. The undesirable performance of unlearning on possibly noisy samples has actually been supported by our ablation study, in which negative DPO improved the performance by adding regularization on the noisy negative samples compared to unlearning. This actually motivates us to impose regularization on relatively low quality data as in ULMA.\n[1] Lu, Ximing, et al. \"Quark: Controllable text generation with reinforced unlearning.\" Advances in neural information processing systems 35 (2022): 27591-27609.\n[2] Nguyen, Quoc Phong, Bryan Kian Hsiang Low, and Patrick Jaillet. \"Variational bayesian unlearning.\" Advances in Neural Information Processing Systems 33 (2020): 16025-16036."}, {"Heading": "Official Comment by Reviewer 33oF", "Subheading": "Official CommentbyReviewer 33oF23 Nov 2023, 04:16Everyone", "Content": "Comment:\nThanks for your response. I appreciate the additional experiments and explanations. I will maintain my scores as the updates have not fully addressed my initial concerns about the impacts of key components, such as the regularization in the proposed method.  However, I believe it is important and promising to explore point-wise preference modeling. And it would be beneficial to have a more detailed analysis of how the key components influence model outputs, like the different effects of regularization on language quality and toxicity, as shown in Fig 3 of [1].\n[1] Lu et al. Quark: Controllable text generation with reinforced unlearning. NeurIPS 2022"}, {"Heading": "Response to your additional comment", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:37Everyone", "Content": "Comment:\nWe are sincerely grateful for your continued engagement and thoughtful feedback on our manuscript. We understand your concerns regarding the impacts of key components like regularization and appreciate your suggestion for a more detailed analysis.\nWe would like to clarify that we have not conducted an analysis of the regularization component's impact in our original manuscript, since the substantial body of existing literature [1,2,3] has extensively studied how the regularization term making trade-offs between multiple rewards, such as the language quality and toxicity as you mentioned. Our expectation that the findings reported in these prior works would extend to our method led us to focus on the novel aspects of our contribution.\nHowever, we recognize the importance of empirical evidence to support our claims, and in light of your feedback, we are committed to conducting a thorough analysis of how regularization specifically influences the outputs of our model. This will include a detailed Pareto frontier analysis, that will illustrate the nuanced effects of regularization on both language quality and helpful / harmful metrics. This additional experiment will be carefully designed to addresses your valid concerns.\nWe would also like to highlight that the primary contribution of our paper is the introduction of a unified framework that adeptly learns from both demonstration and point-wise preference datasets. This versatile framework enables the use of SFT and various preference learning methods, including the application of different regularization terms. The proposed ULMA method empirically achieves significant performance improvements across a spectrum of data quality scenarios.\nWe thank you once again for guiding us towards enriching our study, and we look forward to revising our manuscript accordingly.\n[1] Bai, Yuntao, et al. \"Training a helpful and harmless assistant with reinforcement learning from human feedback.\" arXiv preprint arXiv:2204.05862 (2022).\n[2] Touvron, Hugo, et al. \"Llama 2: Open foundation and fine-tuned chat models.\" arXiv preprint arXiv:2307.09288 (2023).\n[3] Rafailov, Rafael, et al. \"Direct preference optimization: Your language model is secretly a reward model.\" arXiv preprint arXiv:2305.18290 (2023)."}]}]}, "AOSsLRKQrX": {"paper_info": {"Primary Area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Unsupervised Visual dynamics prediction, object centric representation, disentangled representation", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "TL;DR": "We propose a novel approach for learning disentangled object representations for the task of learning visual dynamics via transformers.", "Abstract": "We focus on the task of visual dynamics prediction. Recent work has shown that object-centric representations can greatly help improve the accuracy of learning such dynamics in an unsupervised way. Building on top of this work, we ask the question: would it help to learn disentangled object representations, possibly separating the attributes which contribute to the motion dynamics vs which don\u2019t? Though there is some prior work which aims to achieve this, we argue in this paper either it is limiting in their setting, or does not use the learned representation explicitly for predicting visual dynamics, making them sub-optimal. In response, we propose DisFormer, an approach for learning disentangled object representation and use them for predicting visual dynamics. Our architecture extends the notion of slots Locatello et al. (2020) to taking attention over individual objectrepresentations: each slot learns the representation for a block by attending over different parts of an object, and each block is expressed as a linear combination\nover a small set of learned concepts. We perform an iterative refinement over\nthese slots to extract a disentangled representation, which is then fed to a trans-\nformer architecture to predict the next set of latent object representations. Since\nour loss is unsupervised, we need to align the output object masks with those ex-\ntracted from the ground truth image, and we design a novel permutation module\nto achieve this alignment by learning a canonical ordering. We perform a series\nof experiments demonstrating that our learned representations help predict future\ndynamics in the standard setting, where we test on the same environment as train-\ning, and in the setting of transfer, where certain object combinations are never\nseen before. Our method outperforms existing baselines in terms of\npixel prediction and deciphering the dynamics, especially in the zero-shot transfer\nsetting where existing approaches fail miserably. Further analysis reveals that our\nlearned representations indeed help with significantly better disentanglement of\nobjects compared to existing techniques.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9419", "PDF Url": "https://openreview.net/pdf?id=AOSsLRKQrX"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9419 by Area Chair 4mpy", "Subheading": "Meta ReviewbyArea Chair 4mpy01 Dec 2023, 13:56 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThis paper proposes an object-centric architecture for video prediction that disentangles information about motion dynamics from other information. This setting is quite interesting and novel, as pointed out by the reviewers, and the paper demonstrates how the proposed approach is able to improve over baselines. The method itself can be viewed as a novel combination of existing techniques.\nDespite these strengths, there is broad agreement among the reviewers that the current submission is not ready for publication. In particular, several reviewers point out how the manuscript and the proposed method are difficult to understand (i.e. concerns about clarity), and at least two reviewers are concerned that the datasets considered are too simplistic to provide for a meaningful comparison. There are also some concerns that the proposed model is too complex and has not been sufficiently ablated, which leaves it unclear what components contribute most to the performance. Finally, more evidence is needed that disentanglement is indeed the reason for the observed improvement (and evaluating more difficult datasets might help with that). Overall, the author response falls short at addressing the majority of these concerns.\nJustification For Why Not Higher Score:\nConcerns about clarity / presentation\nSubstantial concerns about the experimental evaluation, including ablations, (slightly) more visually complex datasets, and overall limited evidence to  support the proposed working of the method and disentangling being the key to the reported performance.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "Official Review of Submission9419 by Reviewer 987R", "Subheading": "Official ReviewbyReviewer 987R31 Oct 2023, 20:50 (modified: 24 Nov 2023, 14:43)EveryoneRevisions", "Content": "Summary:\nThis paper presents a new neural net architecture for decoupling objects from dynamics for the task of video prediction on simple scenes. Using several modifications to existing works, the authors encourage more decoupling of object features from other features like the position and dynamics of the inputs. The authors show improved accuracy on existing simple benchmark datasets over previous networks.\nMinor typos (did not influence review):\nPage 4: and is a hyperparameter of the model.\nPage 9: we swap the positions swapped the blocks corresponding to shape\nEDIT:\nI can't seem to comment, so I'd like to add my comments here to the author's responses\nThank you for your updated wordings. I think the paper is clearer now, though I still think some images of the learning problems earlier in the paper during the problem definition portion would make it even stronger.\nI see what you are saying, but I think of all the problems the only one that convinces me it could generalize to more complex scenes is CLEVR. The other experimental setups are too simple to be called \"visual.\" I'd like to see more evidence on CLEVR and even harder datasets (maybe something with a physics engine and more realistic objects like Ai2THOR).\nFrom the appendix: \"We found that even though the permutation\nmodule was trained on same time step object representations, it produces correct permutation matrix\neven for 10 time step apart objects.\" How can you say it's correct if you don't have ground truth permutations?\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe authors show an improvement on the existing state of the art on several benchmark datasets\nThe architecture accounts for several difficulties in training models to be disentangled in new/interesting ways\nThe paper is generally well-written\nWeaknesses:\nClarity:\nI thought the explanation of the task and the actual problems came too late and were not depicted well enough for what the paper was trying to accomplish. 3 of the 4 datasets used I would call \"toy\" datasets of simple bouncing balls. The 3D dataset seems more visually driven, but even that uses CLEVR which is known to be visually simple. In the whole paper there is only a single picture depicting the actual task, and the tasks are only described in the experiment section. For a paper with \"Visual Dynamics\" in the title, I would have expected less toy problems, and more explanation to what the actual problems were. I think this could have been a stronger paper if it had foregone the visual component and worked directly with low level data.\nI found several explanations in the paper to be confusing/lacking detail. One key concept in the paper was that of a \"Block\". Here is the explanation from the paper: \"Recently, (Locatello et al., 2020) in the paper on slot-attention proposed architecture for unsupervised discovery of objects via iterative refinement of what is referred to as a slot. Each slot binds to an object via attention over the entire image in their work. We extend their idea to the case of objects, where each slot now represents a block, which is iteratively refined by taking attention over a latent object representation\" - My rephrasing of this is \"rather than take attention over the entire image to get a representation, we first extract an object mask, and then take attention over that\". I'm not sure that is correct, and even if it is, I don't understand why they need attention if they already have a mask.\nAnother instance of this was the Permutation module. I did not understand the motivation behind it considering it is only used at training time. The authors say they learn a permutation to match up objects from one frame to the next, but that they supervise this permutation with ground truth knowledge. Then at test time, this component is removed. If you are already using ground truth information at train time, and at test time you don't use the module, why not just permute the features directly instead of learning a permutation matrix?\nThere were a few other small questions I had about some other phrases. \"All our object extractors are unsupervised and trained in a self- supervised manner. In our experiments, for 2D environments, we train an expert model similar to (Sharma et al., 2023b) to generate supervised data for Mask R-CNN.\" This seems to indicate the model is both unsupervised and self supervised, but then also trained with supervised data. That doesn't make any sense to me.\nExperiments\nI thought the experiments in this paper were lacking in showing what the authors claimed. Predicting simple rigid body circle motion and even rigid body 3D synthetic CLEVR motion is not really convincing since it is such a problem removed from the complexities of the real world.\nTo convince me that there is a decoupling happening, it is crucial to have an experiment that directly probes this decoupling. The ablation study in 4.4 seems to do that in some way, but I don't understand the experimental setup from the explanations, and again, it is only on a toy setup so I can't say whether it would generalize to more complex scenes. It sounds like somehow the authors took the embeddings from one color setup and swapped them with another color setup to look at the output. I guess it seems trivially obvious that the output should change color, but does that prove that the dynamics module only encodes dynamics or just that masking a part of the image results in color features for that part of the image.\nThe DisFormer seems to be another reasonable ablation on paper but not carried out as well as I'd liked. Details are sparse, but it sounds like the entirety of MaskRCNN was replaced with an MLP, which doesn't seem like a reasonable substitution \"by replacing the object extractor (refer Section 3) by an MLP to create dense object representations\".\nQuestions:\nI would like the authors to explain in more detail why the permutation model was necessary if it was only used during training. I would also like the authors to explain the DisFormer and experiment 4.4 better.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:26 (modified: 23 Nov 2023, 06:48)EveryoneRevisions", "Content": "Comment:\nThanks for pointing this out. We have added a task description in the introduction. While we agree that we have experimented only with visually simple datasets, we would like to to argue that disentanglement is a hard problem, and some of the SOTA techniques (Symbinder [1]) have only worked with images, which we have extended to video in our work. Further, SlotFormer [2] (which is a very recent work) for video prediction, also works on only simple rigid-body dynamics. We are working on compiling example images and hope to post them in the discussion phase.\nWe thank the reviewer for the suggestion and we agree with reviewers rephrasing. We would like to point out that we are not taking attention to the entire image and rather take attention to the image masked out by an object mask. We are aiming for disentangled representation of an object and thus we add that prior by inferring blocks which is iteratively refined by taking attention over latent object representation.\nWe want to highlight to the reviewer that the permutation gadget undergoes training in a self-supervised manner and we don\u2019t have ground permutations. Its sole purpose is to align predicted object blocks with ground object blocks for dynamic loss computation. Importantly, during testing, there is no necessity for loss computation, and therefore, the permutation module is excluded from the testing process. We have added more details in the appendix.\nWe have included the specifics of object extractor training in the appendix. Additionally, it's worth noting that for the 2D environment, we employ Mask R-CNN as the object extractor. The training of Mask R-CNN necessitates supervised data, which is generated in unsupervised fashion from expert models."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:28 (modified: 23 Nov 2023, 06:48)EveryoneRevisions", "Content": "Comment:\nWe are unsure of the specific claim that the reviewer is referring to. To the best of our knowledge, we have not made any claim regarding our ability to do well with deformable objects. If the reviewer can point out a specific instance, we would be happy to correct it. While we agree that we have experimented only with rigid body surfaces, we would like to to argue that disentanglement is a hard problem, and some of the SOTA techniques (Symbinder refer) have only worked with images, which we have extended to video in our work. Further, SlotFormer [] (which is a very recent work) for video prediction, also works on only simple rigid-body dynamics. Hence, we strongly believe our setting is commensurate with some of the recent works in this area.\nIn response to the reviewer's suggestion, we have incorporated additional evaluation and experiments illustrating the decoupling of blocks in the appendix section F. Importantly, we emphasize that these blocks are learned in a fully unsupervised manner. One such instance is the block that has learned to represent color, as demonstrated in section 4.4. However, the final part of the review is currently unclear to us, and we would appreciate further clarification.\nThat was a typo and it should have been \"by replacing the block extractor\u201d. We have fixed it in the paper."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:28 (modified: 23 Nov 2023, 06:48)EveryoneRevisions", "Content": "Comment:\nWe have addressed the need for a permutation model by including a specific example in the newly added content within the appendix section C. Furthermore, we have enhanced and clarified the details in section 4.4 based on your feedback."}]}, {"Heading": "Official Review of Submission9419 by Reviewer gVGK", "Subheading": "Official ReviewbyReviewer gVGK31 Oct 2023, 10:09 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nIn this work, a novel model is proposed for next frame prediction for videos of interacting objects. Building on previous architectures, the authors explore\nfurther structuring an object-centric representation into blocks that represent\nspecific object attributes such as shape or color. Object-centric representations are\nfirst obtained using a pretrained unsupervised segmentation model and a feature\nextractor. Slot Attention is used to decompose object representations into blocks. A\nTransformer than predicts the latent representation of the next frame which is converted into an image using an adapted Spatial Broadcast Decoder. The model consistently\noutperforms previous models on three synthetic datasets.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nLearning scene representations that are structured into objects and their attributes\nis a very relevant topic. An unsupervised approach based on next frame prediction as\nfollowed by the authors is applicable in a broad range of settings.\nThe proposed model consistently improves over previous methods.\nWeaknesses:\nThe empirical evaluation has a strong focus on measuring quantiative performance\naveraged over entire datasets. Further insights into the inner workings of the model or\ncomponents necessary for outperforming previous approaches are hardly provided.\nThis paper proposes a range of novel model components, a detailed ablation analysis is\nhowever missing. The only comparison is to a model that replaces the pretrained object extractor with an MLP. So it is not clear to which degree the different components\ncontribute to the improved performance of the model.\nThe paper does not discuss any particular success or failure cases of the proposed\nmodel. Are there specific situations which are predicted better by the proposed model? How do these related to the model components introduced in the paper?\nThe model learns a constant concept space $C$. Do the concept vectors correspond to\ninterpretable attributes? Is there a separation of object attributes into those that\ncontribute to dynamics and others that do not, as asked in the abstract?\nThe disentanglement of object attributes is a core motivation behind this work. The\ndisentanglement is however not quantitatively evaluated.\nQuestions:\nIn Phase I of the model, object masks are predicted and the masked objects passed\nthrough a feature extractor. Why is this extra step necessary? Is it possible to use\nthe internal representation of the segmentation module directly?\nIn section 3.2: How exactly are block vectors projected onto the learnable concept\nspace? A mathematical description might be helpful here. Why does this project lead to\ndisentangled representations?\nHow were hyperparameters tuned for the DisFormer? How sensitive is the model with\nregard to chosing hyperparameters?\nWere the hyperparameters tuned for the baselines?\nHow are the predicted object positions obtained for all methods when evaluating the\nposition error?\nI do not understand the transfer learning setup in section 4.3: Two variants of the\ntraining set are created, are the models trained on both? How is it different from the\nevaluation setting?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:30 (modified: 23 Nov 2023, 06:49)EveryoneRevisions", "Content": "Comment:\nMost of our model components are dependent on each other, so the entire system may not work if we remove any of those. At the same time, if the reviewer has specific suggestions on which ablations to try with respect to moving out certain components, we will be happy to report those. There was a typo in experiment section 4.2 where instead of object extractor we replaced block extractor with MLP.\nThis is a very important suggestion. We are working on compiling a list of failure cases of our model. We hope to upload this to these before the end of the discussion phase.\nIn our ablation experiment, we rigidly assigned concept weights for blocks to values of 0 and 1. Unfortunately, the decoding process with these blocks failed to produce any meaningful images. This experiment has conclusively shown that, under these conditions, individual concept vectors do not effectively represent interpretable attributes. About the second part of the question we are working defining these sepration. We hope to share the results before the end of the discussion phase.\nWe have added the appendix section of disentanglement evaluation."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:32 (modified: 23 Nov 2023, 06:49)EveryoneRevisions", "Content": "Comment:\nInternal representation of mask does not have appearance information of object. We thank the reviewer for the suggestion. We would also like to point out that we also need the appearance information of the object and only the mask's hidden representation will not be sufficient. We are working on the reviewers suggestion and hope to share results in the discussion phase.\nThe line 10 and 11 in Algorithm 1 does the projection of blocks to concepts. As the concept vectors for a given block common across the objects these add strong prior for learning disentangled representation.\nWe performed a grid search of hyperparameters for a reasonable range. We found that DisFormer\u2019s training is stable in the range of consideration.\nWe started with hyperparameters reported by the baselines and tuned in limited space.\nWe added a section in appendix describing this.\nIn section 4.3 single model was trained on two datasets. The first dataset have all small objects and second dataset have all big objects in video. During testing we test on a dataset where some objects are small and some are big."}]}, {"Heading": "Official Review of Submission9419 by Reviewer HX9L", "Subheading": "Official ReviewbyReviewer HX9L31 Oct 2023, 02:41 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis manuscript introduces a novel approach to disentangled object-centric representation learning specifically tailored for video data. The authors present a method that distinguishes itself from prior work, particularly SysBinder, emphasizing its unique applicability to video data and asserting its capabilities in capturing essential knowledge for future prediction in a disentangled manner. Inspired by Slot-Attention, the method utilizes slot-attention on object-centric representations to delineate attributes per the object-centric representation. The model undergoes extensive evaluation across two 2D datasets and a 3D dataset, demonstrating superior performance in pixel-level reconstruction and position estimation over existing methods. Additionally, the authors subject their model to generalization tests in unseen environments, where it continues to outshine competing approaches.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n3 good\nStrengths:\nThe motivation behind the study is intriguing and thought-provoking.\nThe exploration of disentangled object-centric representation for video data is innovative, unveiling new insights, particularly regarding the representation of essential attributes for future prediction from video data.\nThe model is intuitively designed, effectively leveraging slot-attention on top of the object-centric representations to disentangle representations.\nComprehensive comparative analysis, including ablation studies, robustly demonstrates the advantages of disentangled representation.\nThe figure for their model architecture is well-drawn and easy to understand.\nThe abstract, introduction and the proposed model section are well written except the permutation module section.\nWeaknesses:\nThe manuscript\u2019s clarity and organization can be improved. Specific suggestions for improvement are provided in the questions section.\nA comparative evaluation with another disentangled object-centric representation learning method, SysBinder, is lacking, despite its mention in the text.\nThe visualization of disentanglement in the manuscript (Section 4.4) could be enhanced for better clarity and comprehension. Additionally, as they started this paper with the question, \u201cwould it help to learn disentangled object representations, possibly separating the attributes which contribute to the motion dynamics vs which don\u2019t?\u201d, if they can show the disentangled representation for the attribute which to contribute to predict the dynamic, it should be much better.\nThe quantitative results presented could be bolstered with more illustrative examples, showcasing scenarios where the proposed model excels in comparison to its counterparts.\nQuestions:\nClarity and Presentation\nIn the introduction, could you incorporate a high-level architectural diagram or illustration of your model? This addition would facilitate a clearer and more immediate understanding for readers.\nYou\u2019ve described your permutation module as novel. Can you elaborate on its novelty, especially in the context of other existing methods, such as the approach used in the OCVT paper?\nThe Mask R-CNN in your methodology is trained using a labeled dataset. This seems to introduce a discrepancy since the other models under comparison do not utilize labeled data. Could you perhaps validate your model's performance using Slot-Attention or another slot-based model as a substitute for Mask R-CNN?\nIn Figure 1, \u201cNote that each block has its own set of concept vectors; thus, C = || rj=1C j where r represents the number of blocks.\u201d. Why? Shouldn\u2019t the input $mathcal{C}$ be shared for every block?\nIn section 3.2, \u201cwe project each resultant block vector onto the learnable concept space and update its representation as a linear combination of the concepts via projection weights (lines 9 - 12 in Algorithm 1). This step results in discovering the disentangled representation central to our approach.\u201d. Could you provide a more in-depth explanation or empirical evidence to support this assertion?\nIn Algorithm 1, what is $k$? In figure 1, C is consisted of $r$ concept vectors.\nIn section 3.3, \u201cLet the transformer encoder output be \u03b4\u02c6 s ti,b. \u201c For clarity, could you specify that this is the output corresponding to $s_t^{i,b}$?\nSection 3.4 appears to be complex. To ensure my understanding is correct: is the process essentially projecting the concatenated block vectors through matrix $U$, calculating the Cosine Similarity, and then aligning the most similar representations as the same object? Further clarification and polishing of this section would be beneficial.\nHow is the object position estimated within your model, as detailed in the experiment section? You've mentioned that position error is not reported for SlotFormer due to its lack of explicit object mask handling; does your model operate in a similar manner?\nCould you provide a deeper analysis of DenseFormer, particularly in comparison to Slotformer? Are there specific scenarios where DenseFormer is more prone to failure, and could you share sample outputs from both DisFormer and DenseFormer to illustrate these points?\nIn Section 4.3, the term \u201ctransfer learning\u201d is used. Based on my understanding, the experiments seem to be more about evaluating generalization to unseen environments rather than transfer learning. Would renaming this as generalization and providing a more comprehensive analysis, especially in light of the varied performance across different datasets, be more accurate and informative? For example, when comparing the results in Table 1, Slotformer performance deterioration for 2D-BC is not huge while Slotformer is worse for 2D-BS. This results can suggest that for more complicated environment, the disentangled representation is more helpful for the generalization.\nThe examples in Section 4.4 intended to illustrate disentanglement seem to be lacking. Instead of swapping both color and shape attributes, could separate demonstrations of each be more effective in showcasing the model\u2019s capabilities?\nMethodology\nRegarding Section 3.4, why is the permutation module utilized only during training and discarded during testing? Could its application during testing, potentially for aligning the input order of the Transformer module, lead to enhanced performance?\nFor the Dynamic loss calculation in Section 3.6, why is the comparison made between the weights of the concept vectors rather than the block vectors? Additionally, could you provide a definition for $\\hat{w}_t^{i,b,j}$?\nExperiment\nIn Section 3.6, there is a training phase where only the block extractor, permutation module, and decoder are trained, followed by a phase focusing solely on the dynamic model. Could you elaborate on the reasons and potential benefits of this training strategy? Did it result in improved model performance?\nThe model training in Section 3.6 incorporates multiple loss functions. Have ablation studies been conducted to understand the impact of each loss function on the overall performance?\nAdditional Comments\nThe study presents an interesting investigation with noteworthy contributions to the field. However, to ensure a stronger impact and facilitate better understanding, a revision focusing on improving presentation, clarity, the comparison with the relevant work, and depth of analysis are recommended.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:37 (modified: 23 Nov 2023, 06:47)EveryoneRevisions", "Content": "Comment:\nWe would like to point out to reviewer that Sysbinder discovers object representation for images and we would like to discover object representations for video. We still performed used frame by frame sysbinder and found satisfactory results on 2D BS dataset.\nWe are working on compiling example images and hope to post them in the discussion phase.\nWe are working on high level diagram and hope to post them in the discussion phase."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:42 (modified: 23 Nov 2023, 06:55)EveryoneRevisions", "Content": "Comment:\nWe are working on high level diagram and hope to post them in the discussion phase.\nOCVT\u2019s alignment algorithm is based on hungarian matching algorithm over L2 norm of latents. In our experiment it performed poorly. We hope to add some results during discussion phase.\nWe have added the details in the appendix section\nEach block has it\u2019s own set of concept vectors. For instance the block(s) representing color would have concept vectors as {red, green, blue} etc. whereas those representing size would have a different set of concept vectors. two different attributes cannot be a linear combination of same concept vectors. Thus concept C is not shared across all the blocks.\nk is number of concept vectors\nYes it is output corresponding to $s_{t}^{i,b}$\nYes the reviewers understanding is correct.\nWe have added position error computation section in appendix\nIn our choice of dataset the color and shape are correlated and are not represented by separate blocks."}, {"Heading": "Methodology", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:47Everyone", "Content": "Comment:\nThat's very interesting suggestion. We are currently working on it and hope to add results in discussion phase.\nWe found that comparing weights of concept vectors result in faster convergence. The $\\hat{w}_{t}^{i,b,j}$ is the predicted weight of $b^{th}$ block and its jth concept vector. It is output of transition model."}]}, {"Heading": "Official Review of Submission9419 by Reviewer M2hR", "Subheading": "Official ReviewbyReviewer M2hR30 Oct 2023, 00:40 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper proposes DisFormer, which extends Slot Attention and SlotFormer with \u201cdisentangled\u201d object representation for visual dynamics prediction. The disentangled representation is learnt by iteratively refining the slots over individual object representations (rather than the whole image representations), and regularizing the slots to be linear combinations of the concepts. To align the model output with the groundtruth, a permutation module is used to learn the ordering. Experiments are done on 2D Bouncing Shapes/Circles and OBJ3D to show the method works well in both in-domain and domain transfer settings.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThe method is clearly motivated and clearly described. By learning slots representing blocks (which are iteratively updated to recover an object representation), and regularizing the slots/blocks to be a linear combination of concepts, the slots/blocks learn disentangled representations of objects.\nRelated works, as well as their difference with this work, are well-discussed.\nWeaknesses:\nMy major concern is that the experiments are not very persuasive.\nThe datasets are simple toy datasets. The original SlotFormer has done experiments on dataset CLEVRER, which is harder than the 2D shapes/circles used in this paper. Are there reasons why CLEVRER, or CLEVRER with more complex textures, are used for experiments?\nNo ablations are provided. The model contains multiple components, but there are no ablation experiments to study the effect of each component. For example, the effect of recovering object representation versus image representation, the effect of learning the slots to be a linear combination of concepts, the number of slots, number of concepts, etc. should be studied.\nNot enough experiments are shown to prove the representations are \u201cdisentangled\u201d. This disentanglement is the major advantage of the method. However, only several examples are shown in Fig. 2 to show the disentanglement. Quantitative results, or visualization of the learned slots (e.g. using t-SNE) would be preferred.\nMissing experiment details. The training details including the hyperparameters are not provided. Some critical parameters (e.g. number of slots/concepts, loss weights) should be discussed.\nQuestions:\nSee weakness. More details about experiments would be helpful.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:54Everyone", "Content": "Comment:\nWeakness:\nWe used OBJ3D which has similar visual complexity of CLEVRER. We are still working to compile our numbers on CLEVRER and hope to shape them during discussion phase.\nWe are working on specific ablations and hope to shape them during discussion phase.\nWe have provided disentanglement evaluation in appendix.\nAdded in the appendix."}]}]}, "uEYxHVQJF4": {"paper_info": {"Keywords": "Hyperbolic space, hyperbolic neural networks, hierarchical structure", "Abstract": "Hyperbolic Neural Networks (HNNs), operating in hyperbolic space, have been widely applied in recent years, motivated by the existence of an optimal embedding in hyperbolic space that can preserve data hierarchical relationships (termed Hierarchical Representation Capability, HRC) more accurately than Euclidean space. However, there is no evidence to suggest that HNNs can achieve this theoretical optimal embedding, leading to much research being built on flawed motivations. In this paper, we propose a benchmark for evaluating HRC and conduct a comprehensive analysis of why HNNs are effective through large-scale experiments. Inspired by the analysis results, we propose several pre-training strategies to enhance HRC and improve the performance of downstream tasks, further validating the reliability of the analysis. Experiments show that HNNs cannot achieve the theoretical optimal embedding. The HRC is significantly affected by the optimization objectives and hierarchical structures, and enhancing HRC through pre-training strategies can significantly improve the performance of HNNs.", "Primary Area": "learning on graphs and other geometries & topologies", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9418", "PDF Url": "https://openreview.net/pdf?id=uEYxHVQJF4"}, "review_info": [{"Heading": "Official Review of Submission9418 by Reviewer pJty", "Subheading": "Official ReviewbyReviewer pJty05 Nov 2023, 10:07 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper studies a benchmark for evaluating the hierarchical representation capacity (HRC) of the hyperbolic neural network (HNN). The empirical study shows the HRC can be affected by the optimization objectvie and the training data. This observation facilitate to develop pre-training strategies to enhace the HRC of HNN, improving the learning capacity and performance of the neural network. This paper shows some interesting observations, but it lacks of the generalization of HNN to other tasks.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\n1-This paper first study the hierarchical representation capacity (HRC) of the hyperbolic neural network (HNN) and aim to answer how the HNN works.\n2-A benchmark, including data and metric, is proposed to evaluate the HRC.\n3-Three pre-training methods are proposed to improve the learning capacity and the performance.\nWeaknesses:\n1-This paper only studies the HRC in the graph dataset, the observation on text and graph data is missed.\n2-The experiments on pre-training method misses formulation of the losses. In addition, why the GD as objective function can attain good performance, please explain?\nQuestions:\n1-This paper study the HRC for the graph data, there is another metric, called delta-hyperbolicity, can be used to evaluate the hierarchiy of the data, I want to see does the value of delta-hyperbolic matches the value of the metrics proposed in this paper.\n2-This paper only studies the HRC in the graph dataset, does the conclusion is also hold in text and image data?\n3-The proposed pre-training method should also be evaluated in other datasets.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9418 by Reviewer u1No", "Subheading": "Official ReviewbyReviewer u1No31 Oct 2023, 15:20 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper elucidates the scope and applicability of HNNs through quantitative analysis of their HRC. It provides guidance on enhancing HNNs by identifying factors that improve their hierarchical representation capability. In particular,\nThis paper proposes a benchmark (HRCB) to quantitatively evaluate the hierarchical representation capability (HRC) of HNNs. HRCB includes metrics to assess horizontal relationships (sibling nodes) and vertical relationships (parent-child nodes) in a hierarchy.\nExperiments using HRCB show that HNNs do not achieve the theoretical optimal embedding in hyperbolic space. Their HRC is significantly lower than combinatorial construction methods.\nAnalysis reveals two key factors influencing HNNs' HRC: (1) Optimization objectives that help distinguish positional relationships between nodes, and (2) Training data structured as a complete n-ary tree.\nThe paper proposes pre-training strategies to enhance HNNs' HRC based on these insights. Experiments show improved downstream task performance from enhanced HRC, validating the analysis.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nThe author made a very interesting observation and conducted numerous experiments to support their findings.\nWeaknesses:\nThere are several points of concern from me:\nThe HRCB metrics presented presume the root node is positioned at the highest point. Additionally, the authors assume that the root node should be close to the origin. This isn't accurate for all HNNs, as detailed in [1]. If these assumptions are invalid, the four evaluation criteria proposed might not be accurate.\nThe research primarily uses two datasets (Disease and Animal) for analysis. Including a broader range and real-world datasets would provide more robust conclusions.\nThe author's description of pre-training strategies isn't very clear. Could this be elaborated more?\nThe comparison is made with the GCN model, but the HGCN isn't considered. This is an omission.\nWhile the findings are intriguing, there are various forms of HNN currently available, such as those based on the tangent space or being fully hyperbolic. The author doesn't seem to address this aspect.\n[1] Menglin Yang et al. Hyperbolic Representation Learning: Revisiting and Advancing. ICML 2023.\nQuestions:\nCould you provide more details on the data generation process for the hierarchical structures?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9418 by Reviewer 4wYm", "Subheading": "Official ReviewbyReviewer 4wYm28 Oct 2023, 05:30 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper carefully examines the effectiveness of various Hyperbolic Neural Networks (HNNs) by measuring their Hierarchical Representation Capabilities (HRC), an evaluation process named as Hierarchical Representation Capability Benchmark (HRCB). The four metrics developed for HRCB include Root Node Hierarchy, Coordinate Origin Hierarchy, Parent Node Hierarchy, and Sibling Node Hierarchy, which altogether measure how well the hierarchical structure is embedded in hyperbolic space. The paper also proposes pre-training strategies upon improving a model's HRC, and empirically assess the relationship between HRC and downstream performance. Experimental results show that HNNs' HRC has a significant impact on downstream performance, and pre-training HNNs towards enhancing HRC can improve its performance.\nSoundness:\n3 good\nPresentation:\n1 poor\nContribution:\n2 fair\nStrengths:\n[S1] The motivation of looking back upon hyperbolic neural networks and closely assessing their effectiveness on downstream tasks with respect to theoretical work is very interesting, and I believe any insights would greatly benefit the geometric deep learning community.\n[S2] The scope of experimentation is fairly comprehensive, covering various downstream tasks, hyperbolic manifold spaces, and neural network architectures.\nWeaknesses:\n[W1]\nThe overall contribution is not clear within the writing and experiments.\nIs the paper hoping to show why HNNs are effective? or that HNNs do not achieve theoretically optimal embeddings?. From the experiments, it seems the goal of the paper is that 1) there exist a gap in HRC between existing HNNs and theoretically optimal embeddings and 2) reducing the gap via pre-training helps boost downstream performance, yet the overall writing (e.g., abstract and introduction) makes it confusing on what to expect from the experiments: reasons behind HNNs' effectiveness or limitations of HNNs.\n[W2]\nThe presented empirical observations in the text are unclear and somewhat misleading.\nFor instance, page 7 discusses how the LR target for NC \"does not need to distinguish the position relation\" among nodes, yet overfitting on LR helps improve HRC, which seems counterintuitive. Why is this the case? Furthermore, page 8 mentions how \"within the applicable scope of HNNs, performance can be improved by enhancing HRC\", with the node classification task being \"out of scope\". This is misleading considering that many previous work have shown performance boosts in node classification by leveraging hyperbolic models [A, B, C].\n[W3]\nThe figures showing Friedman test and Nemenyi post-hoc tests are extremely hard to read.\nIt would be better to categorize results based on what the authors are hoping to convey through the experiment: as an example, for Figure 6(a), if the main observation is that GD, HR, and FD help HNNs learn position relation unlike LR, it would be better to simply draw a bar chart (or multiple bar charts, one for each manifold) with targets on the x-axis and the HRC values on the y-axis. That way, we can visually observe the orderings currently written as text within the plots.\n[W4]\nDownstream performance results are missing exact scores and are only compared in terms of rankings.\nFor Subsection 5.2.3, it would be better to simply present the downstream results in exact scores using the scoring metrics for each downstream task (F1 score for node classification, mAP for graph reconstruction and so on). This way, we can concretely estimate how much better/worse each method performed compared to another, and whether the results are within reasonable range compared to existing literature.\n[A] Chami et al., Hyperbolic Graph Convolutional Neural Networks. (NeurIPS 2019)\n[B] Liu et al., Hyperbolic Graph Neural Networks. (NeurIPS 2019)\n[C] Zhu et al., Graph Geometry Interaction Learning. (NeurIPS 2020)\nQuestions:\n[Q1] The Coordinate Origin Hierarchy metric $M_o$ seems to assume that the root node is located near the origin, while this constraint is not made explicitly during training of HNNs. Considering that Combinatorial Construction [D], on the other hand, trivially satisfies this assumption, would you still consider using this metric to be valid for fair comparison?\n[Q2] Is the L strategy described in the beginning of subsection 5.2.3 equivalent to adding a weighted auxiliary loss to the downstream predictive loss? This seems very similar to how the HGCN paper used a link prediction regularization objective for node classification [A]. Thus adding a few discussion on this connection and giving the strategy a proper name rather than just \"L\" could help towards better clarity.\n[D] Sala et al., Representation Tradeoffs for Hyperbolic Embeddings. (ICML 2018)\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}]}, "1FWDEIGm33": {"paper_info": {"Primary Area": "generative models", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Large Language Models, context-dependence, controllability, cultural values, personal values, personality traits, societal considerations, Shalom H Schwartz, Geert Hofstede, Big Five", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "TL;DR": "Conceptualization of LLMs as superpositions, empirical analyses of unexpected context-depentant value changes, and systematic analyses of different models on the controlability of expressed cultural, personal values, and personality.", "Abstract": "Large language models (LLMs) are sometimes viewed as if they were individuals, with given values, personality, knowledge and abilities. We argue that this \u201dLLM as an individual\u201d metaphor misrepresents their nature. As opposed to humans, they exhibit highly context-dependent values and personality traits. We propose a new metaphor, \u201dLLM as a superposition of perspectives\u201d : LLMs simulate a multiplicity of behaviors, e.g. expressing values, which can be triggered by a given context. As a case study, we conduct experiments on how values vary as a function of context using psychology questionnaires. Crucially, we demonstrate that changes in the context that are unrelated to the topic of questionnaires - varying articles, simulated conversations on other topics, and textual formats - all result in significant unwanted, hard-to-predict changes in the expressed values. We refer to this as the unexpected perspective shift effect. We discuss how this questions the interpretations of studies using psychology questionnaires (and more generally benchmarks) to draw general conclusions about LLMs\u2019 values, knowledge and abilities. Indeed, expressing some values on a questionnaire says little about which values a model would express in other contexts. Instead, models should be studied in terms of how the expressed values change over contexts in both expected and unexpected ways. Following this insight, we introduce the concept of perspective controllability - a model\u2019s affordance to adopt various perspectives. We conduct a systematic comparison of the controllability of 16 different models over three questionnaires (PVQ, VSM, IPIP) and different methods for inducing perspectives. We conclude by examining the broader implications of our work and outline a variety of associated scientific questions.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "Supplementary Material": "zip", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9417", "PDF Url": "https://openreview.net/pdf?id=1FWDEIGm33"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9417 by Area Chair pZE7", "Subheading": "Meta ReviewbyArea Chair pZE709 Dec 2023, 21:07 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThis paper challenges viewing LLMs as individuals with consistent personality or values or belief systems. Via experiments it shows that LLM's expressed values are very context-dependent and can change dramatically based on minor context variations. \nIt also proposes conceptualizing LLMs as superpositions of perspectives that can be triggered by contexts. They introduce perspective controllability  which is the affordance of models to systematically adopt various perspectives through changes.\nStrengths:\nTackles a very interesting problem on the personality o fLLMs.\nIt questions interpretability of existing work using psychological questionnaires to characterize LLMs' values.\nIt proposes an alternative conceptual metaphor more aligned with findings. LLMs as superpositions rather than individuals.\nThe paper systematically copares controllability of different models in exhibiting perspective shifts.\nWeaknesses:\nThe paper does not clearly convey intuitions, justifications and implications of superpositions.\nThe paper lacks analysis on how specific model architectures, or sizes or training approaches impact the personality and also its controllability.\nRooms for improvements:\nMore clearly highlight the practical implications of findings\nExpand the analysis of how model training and properties impact perspective shifts.\nJustification For Why Not Higher Score:\nLack of proper justification and convincing evidence on this personality change phenomena and the proposed view on their personality, ie mixture of perspectives.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "General response to the reviews", "Subheading": "Official CommentbyAuthors22 Nov 2023, 13:48Everyone", "Content": "Comment:\nWe thank reviewers for their comments on our paper.\nReviewers B9Pb and 7GoP argue that the conclusions of our experimental results, i.e. that using psychology questionnaires to study properties of LLMs can be problematic because of context-dependance, are already known by the scientific community. However, recent series of publications from diverse teams in the world show this is actually not globally the case: it is common to see scientific papers making general conclusions about the abilities of LLMs based on their answers to psychology questionnaires without systematic discussions of how these answers depend on various kinds of context. In brief, many papers study LLMs using psychological tools that were developed to assess human individuals: however, LLMs are not individuals and thus assumptions relevant for studying individuals do not hold for studying LLMs. Our goal in this paper is to show the associated pitfalls and propose approaches to mitigate them. Thus, we hope our contributions will help structure further research in this domain.\nReviewer P79N and 7GoP raised concerns regarding the first part of our experimental section. These concerns refer to the consistency of a model to hold a value profile induced by a context. In our response, we clarify that we provide robustness to our results by: 1) presenting the questionnaire multiple times with different permutations of the suggested answers, and 2) conducting statistical tests to show that a value profile induced by one context is consistently different from profiles induced by other contexts. Reviewer P79N in addition raised a concern regarding the first set of experiments, which were conducted on only one model (ChatGPT). In our response, we discuss our motivation for this (a more thorough analysis along different questionnaires and types of perspective change). Following this comment, we also add a systematic comparison of the unexpected perspective shift effect on 6 LLMs along three types of stability commonly used in psychology: mean-level, rank-order, and ipsative (see table 6). We compared three RLHF models (GPT-3.5-june, GPT-3.5-march, and OpenAssistant), two instruction fine-tuned models (Upstage LLaMa 1 and 2), and one DPO fine-tuned model (Zephyr). Of these models, all except GPTs are open-sourced.\nReviewer Fkgw argued that we do not adequately show the limitation of using psychological questionnaires as we make \u201cjumps\u201d in our argumentation. We respectfully disagree with the reviewer\u2019s assessment and, in our response, we re-explain our argument and address the reviewer's criticisms.\nTo summarize, the main contribution of this paper consists in showing some recurring methodological pitfalls of the body of works using psychological questionnaires to make general conclusions about LLMs\u2019 personality/values/knowledge/abilities, as well as in presenting approaches to mitigate these pitfalls. We show how, due to LLMs\u2019 highly context-dependent nature, questionnaires often give very different results depending on trivial (seemingly unrelated) context changes. In other words, we show that context influences the questionnaire results in unexpected ways. For example, presenting the PVQ questionnaire through cpp code decreases universalism and benevolence compared to the standard chat format. It is not clear what causes this change, and it would be very hard to predict this beforehand. This calls for reinterpretation of the generality of conclusions made by the aforementioned body of works."}, {"Heading": "Official Review of Submission9417 by Reviewer B9Pb", "Subheading": "Official ReviewbyReviewer B9Pb03 Nov 2023, 03:22 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper challenges the view of large language models (LLMs) as individuals and proposes a new metaphor: \"LLMs as superpositions of perspectives\". The authors conducted experiments that demonstrate unexpected perspective shifts in personal values, cultural values, and personality traits. LLMs changed their responses depending on contexts, and even context variations not related to the target topics led to significant changes in the values and personality traits they expressed. The authors also compared four different perspective induction methods (prompts) to assess whether they could control the models' perspectives (perspective controllability).\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThis paper studies large language models (LLMs), which is a hot topic in the current society.\nThe paper challenges some existing views on LLMs trying to understand them better, giving some warnings of the potential danger of the existing views.\nThe paper explores if the \"perspectives\" could be controlled, by suggesting four induction methods.\nWeaknesses:\nI struggled to understand the importance of this problem, even after reading the paper. It is unclear what the implications and potential applications of this work are. The paper confirms that LLMs do not give consistent responses, and that LLMs are not like humans, as shown in Experiments and discussed in Discussion. However, it is not clear what the paper suggests (besides proposing a new metaphor) and why this is critical.\nQuestions:\nCould you elaborate on the definition of a perspective in this paper? \"A perspective is conceptualized\nas a context from which a model is required to simulate a behavior\".\nit is not clear what the paper suggests (besides proposing a new metaphor) and why this is critical.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to B9Pb", "Subheading": "Official CommentbyAuthors22 Nov 2023, 13:11Everyone", "Content": "Comment:\nWe thank the reviewer for their comments, which we address below:\n\"\"\"\nit is not clear what the paper suggests (besides proposing a new metaphor) and why this is critical.\n\"\"\"\nThe motivation and contribution of this paper rests on the body of research using psychology questionnaires to study LLMs. Many of these papers aim to form general conclusions about LLMs based on their responses to those questionnaires. We show how questionnaires\u2019 results drastically change in an unexpected fashion due to trivial (seemingly unrelated) context changes.\nThese empirical results have several important implications:\nThey question the generality of conclusions made in the aforementioned papers.\nThis opens up various avenues extending those papers. For example, currently LLMs are compared on what value/knowledge/abilities they express in one context, but a relevant analysis which should be added is also how that value/knowledge/ability expression changes along different contexts. During the rebuttal, we add a systematic comparison of models on three types of value stability (see Appendix D in the new pdf) along context changes that appear orthogonal to values.\n\"\"\"\nCould you elaborate on the definition of a perspective in this paper? \"A perspective is conceptualized as a context from which a model is required to simulate a behavior\"\n\"\"\"\nThe notion of perspective we describe is similar, yet more general, to the notion of \u201crole-playing\u201d. In role-playing, one uses a prompt to provide context, enabling the LLMs to generate text as if it was playing the role of a character in a particular situation. So here the LLMs acts by taking the perspective of the character in this particular situation. However, contexts do not necessarily describe a \u201ccharacter\u201d, i.e. features of an individual: it could only describe a situation, use a particular kind of language, or particular objects or pieces of information. We use \u201cperspective\u201d to speak about this form of non-human-like role play. However, we agree this is only an intuitive concept and metaphor. Also, contrary to the comments of reviewers, it was not our primary intention to show that this metaphor is the only possible one: rather, we aim to show that methods used in many papers studying the capabilities of LLMs using psychology questionnaires provide results that should call for very careful interpretations, and we propose extensions of these methods to have a better picture of the properties of LLMs. We propose the metaphor to better explain our argument."}]}, {"Heading": "Official Review of Submission9417 by Reviewer P79N", "Subheading": "Official ReviewbyReviewer P79N03 Nov 2023, 02:55 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper studies the ability of language models to answer psychological questionnaires. Past research has used these tests designed for humans to try to probe LLMs. The study tests model robustness in answering questionnaires under different contexts or conditions (e.g. writing code, prefixing with a random wikipedia page) which are unrelated to the question in the questionnaire and observe there are significant changes in responses. Further, the paper introduces the notion of perspective controllability and aims to test which models can be guided to answer questions in a certain way.\nThe conclusions are that LLMs are unreliable in answering trait questions, with unrelated perturbations leading to different results and that most models are not controllable, albeit some models exhibit some degree of controllability.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nSound methods for statistical analysis of results.\nCreative approaches to test robustness of models.\nAdequately challenges the assumption of 'LLMs as individuals' for measuring traits.\nWeaknesses:\nPrimarily, I think the model needs to have more robust results to understand better what and which types of models behave in different ways. Namely:\nThe first experiment (Section 4.1) is only performed using a single model (ChatGPT).\nThe models can be better selected for experimentation to facilitate understanding the machanisms that lead to consistent or inconsistent results in Table 1. I think the key comparison directions could be along these axes: base model, models from the same series and different size, base vs. chat. vs. instruct vs. RLHF.\nI think the experiments lead into another metaphor than 'superposition of cultural perspectives'. For a perspective to hold, it would have to be consistent across inputs i.e. to produce consistent results when conditioned in the same way. The results show that the conditioning changes results in unexpected and inconsistent ways. Hence, my conclusion from these experiments would be that LLMs lack awareness or knowledge of a perspective.\nIn general, I consider using questionnaires about traits is a bit tricky or ill posed in this context. The questionnaires for traits are usually built as a proxy for behaviors e.g. 'make friends easily' loads on the intra/extraversion scale; so it would be perhaps more suitable (and robust?) to have these framed as test on a behavior e.g. at a party where you don't know anyone and some one is sitting also alone, do you approach to strike up a conversation with them? (yes - more likely extravert, no - more likely intravert).\nAnother aspect worth mentioning is that in addition to the test-retest validity which is brought up in the paper as stability over different ways of providing context before asking the questionnaire question, one could also measure the variance inside each questionnaire, as multiple questions load on the same factor and the variance across these should also be low by design (i.e. people would respond to questions about extraversion similarly).\nQuestions:\nNA\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to P79N (1/2)", "Subheading": "Official CommentbyAuthors22 Nov 2023, 13:20Everyone", "Content": "Comment:\nWe thank the reviewer for their comments and for acknowledging our methodology and challenge of the \u2018'LLMs as individuals' assumption. We address the reviewer's comments below.\n\"\"\"\nThe first experiment (Section 4.1) is only performed using a single model (ChatGPT).\n\"\"\"\nWe focused on one model in this first set of experiments because this enabled us to analyze the effect in more detail (with respect to the page limit). We studied this effect in ChatGPT on 9 scenarios (3 types of changes (conversations, formats, Wikipedia)  x 3 questionnaires) and with detailed statistical analysis.  For that purpose, we chose ChatGPT as it was among the most controllable models in Table 1.\nHowever, we agree that systematically comparing various models on their susceptibility to this effect is a relevant addition, and we thank the reviewer for this suggestion. \nDuring the rebuttal, we defined aggregated metrics based on three types of value stability from psychology literature (mean-level, rank order, and ipsative) and systematically compared models which were more controllable in Table 1.\nWe compared three RLHF models (GPT-3.5-0613, GPT-3.5-0301, and OpenAssistant), two instruction fine-tuned models (Upstage LLaMa 1 and 2), and one DPO fine-tuned model (Zephyr). All these are open-sourced, except for the two GPT models. The most stable models for mean-level, rank-order and ipsative stability were OpenAssistant, GPT-3.5-0613 and GPT-3.5-0301 respectively. These results also imply that models vary in terms of the type of stability they exhibit. For example, GPT-3.5-0301 shows higher rank-order and ipsative stability, but lower mean-level stability. Please refer to appendix D for these experiments, and appendix C for a detailed analysis of ChatGPT with respect to the three types of stability with respect to changes observed in previous human studies.\n\"\"\"\n\u2026 in Table 1. I think the key comparison directions could be along these axes: base model, models from the same series and different size, base vs. chat. vs. instruct vs. RLHF.\n\"\"\"\nWe agree that discussion along those axes is relevant. In the paper, we discuss the effect of RLHF by comparing GPT-3.5 from March and from June (see second to last paragraph in section 4.2). Here we observe a shift of controllability from the user message to the system message. Some additional observations, which we added to the paper, can be made from the current choice of models. When comparing GPT-3.5-instruct-0914 to GPT-3.5-turbo-0301/0613 we can see that RLHF appears to greatly increase controllability in this model. Furthermore, when comparing the raw LLama-65B to the instruction fine-tuned one (Upstage-LLama-65b-instruct) we can see that the instruction fine-tuning likewise appears to greatly increases controllability.\nA limiting factor for the analysis of model size is the accessibility of sufficiently capable smaller models. For example, the smaller versions of GPT-3 (ada, curie, babbage) all express very low controllability. Similarly, all LLaMa 1 models expresses very low controllability (in the paper, we only show the biggest LLaMa model, but we evaluated smaller ones as well). We believe that this is because they are not able to sufficiently understand the task (values, questions, MCQ format, etc.). However, we believe that experiments on different versions of the newer LLaMa-2 models would be a valuable addition.\n\"\"\"\nI think the experiments lead into another metaphor than 'superposition of cultural perspectives'. For a perspective to hold, it would have to be consistent across inputs i.e. to produce consistent results when conditioned in the same way.\n\"\"\"\nThis is indeed the way we approach a perspective in our experiments. We evaluate each perspective along 50 permutations in the order of suggested answers. We provide a statistical analysis, which shows that the value profiles are consistent and different. \nFor instance, we show that a perspective (which is unrelated to values) will induce some value profile that will consistently hold along permutations in the prompt. What is unexpected is the relation of the perspective (e.g. cpp code) to the value change (e.g. decrease in benevolence)."}, {"Heading": "Response to P79N (2/2)", "Subheading": "Official CommentbyAuthors22 Nov 2023, 13:21Everyone", "Content": "Comment:\n\"\"\"\nIn general, I consider using questionnaires about traits is a bit tricky or ill posed in this context. The questionnaires for traits are usually built as a proxy for behaviors e.g. 'make friends easily' loads on the intra/extraversion scale; so it would be perhaps more suitable (and robust?) to have these framed as test on a behavior\n\"\"\"\nWe used questionnaires because there is a body of research using psychological tools to make general conclusions about models abilities/values/knowledge. And this work is directly aimed at adding nuance to those papers.\nThe general point the reviewer makes in this comment is perfectly aligned with the point we make in our paper. A test on behavior would induce a different perspective in a model and the model could express a value profile which is unexpected. This is another example of an unexpected context change which should be explored.\nWhat is therefore needed is to study models on their robustness to the perspective shift effect. Depending on the use case one would  prefer to use a model that has more stable values along many contexts to a model that has a more preferable value profile which was only tested in one context. And currently, we lack such studies that would enable us to make such an assessment (as the one we add during rebuttal in appendix D).\n\"\"\"\none could also measure the variance inside each questionnaire, as multiple questions load on the same factor and the variance across these should also be low by design (i.e. people would respond to questions about extraversion similarly).\n\"\"\"\nWe thank the reviewer for this suggestion, we agree that it would be a nice addition to our analysis."}]}, {"Heading": "Official Review of Submission9417 by Reviewer 7GoP", "Subheading": "Official ReviewbyReviewer 7GoP01 Nov 2023, 18:48 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe authors make the case that LLMs show a sort of superposition of cultural perspectives, since their outputs, as measured by standard tests widely change according to the input. The authors consider value and personality tests in order to measure different \"cultural perspectives\" in LLMs. One of the goals of the authors is to measure the consistency of the outputs of the LLMs in an experimentally sound way. The evaluation is carried on several LLMs.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe paper is indeed timely, there is a lot of interesting ideas to explore around LLMs and this is indeed a good example of an interesting paper in the area.\nWeaknesses:\nThe study that the authors carried out is indeed interesting, but unfortunately, it seems to me that the actual assessment of the results is somehow \"hyped\": at the end these are probabilistic models highly dependent on the prompt and it is somehow expected that they exhibit a variety of personal values, cultural values, personality traits. The authors highlight the fact they observe \"unexpected perspective shift effects\". However, in my opinion, it would be more surprising to see consistency.\nIt is very difficult to understand which inputs led to a change of perspectives. In my opinion, this is a key problem of the paper since small variations might have a significant effects on the outputs. Also, for this reason, it is very difficult to judge the actual consistency of the outputs of the LLMs in the experiments carried out by the authors.\nSuperposition is a wrong term in my opinion given the probabilistic nature of LLMs. In fact, even the same input might lead to different outputs.\nThe term controllability appears to me inappropriate, since the authors are not measuring actual \"controllability\" of the outputs in my opinion.\nThe selection and the analysis of the application of the induction methods are not completely clear, especially with respect to the underlying research hypotheses at the basis of the study design.\nQuestions:\nWhat is the exact definition of cultural perspective you consider in the paper? What is the relation between cultural perspective and personal values?\nWhich kind of inputs did you use for measuring the change of perspectives? (the supplementary material does not consider sufficient material for reproducibility in my opinion).\nIt seems that the authors report the fact that LLMs are not \"coherent\" as the key finding of their paper. Indeed, it is always good to see measurement studies, but the reviewer wonders if this can be considered as something unexpected. After all, the models are trained on a variety of sources. Were the authors expecting a different result?\nDo you have any data about the influence of the training datasets on the experimental results that you showed in this paper?\nCan you discuss Formula (1) in details? How do you analyze the outputs of the LLMs? How do you calculate the mean in this formula?\nIt would be good to know the reasoning beyond the selection of the term \"controllability\". This appears an unusual choice for the phenomena you study in this paper.\nCan you please discuss the effects of the induction methods in relation to their effects on the outputs of the LLMs?\nFlag For Ethics Review:\nNo ethics review needed.\nDetails Of Ethics Concerns:\nNone\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to 7GoP", "Subheading": "Official CommentbyAuthors22 Nov 2023, 13:29Everyone", "Content": "Comment:\nWe thank the reviewer for their comment and for finding the paper interesting and timely. We address the reviewer's comments below.\n\"\"\"\nthe actual assessment of the results is somehow \"hyped\": at the end these are probabilistic models highly dependent on the prompt and it is somehow expected that they exhibit a variety of personal values, cultural values, personality traits. The authors highlight the fact they observe \"unexpected perspective shift effects\". However, in my opinion, it would be more surprising to see consistency.\n\"\"\"\nWe agree that it is not surprising to see drastic, unexpected context-based changes in LLMs. Indeed, this is the underlying intuition of this paper. However, we believe that this view, and its fundamental implications, are not globally shared by the AI scientific community.\nThis is evidenced by the body of research aiming to make general conclusions about LLMs\u2019 capabilities/personality/knowledge from psychological questionnaires.\nOn a more general note, we believe that it is not sufficient to only be aware of this. Rather, it is also important to study models in terms of their robustness to the perspective shift effect. For example, in some use cases one would prefer to use a model that has more stable values along many contexts to a model that has a more preferable value profile but which was only tested in one context. And currently, we lack studies comparing the stability of models along contexts.\nDuring the rebuttal, we aimed to further address this issue by providing such a comparison. We define metrics based on three types of value stability from psychology literature (mean-level, rank order, and ipsative) and systematically compare models that were more controllable in Table 1. Please refer to appendix D for these systematic experiments, and appendix C for a detailed analysis of ChatGPT on the three types of stability with respect to changes observed in previous human studies.\n\"\"\"\nIt is very difficult to understand which inputs led to a change of perspectives. In my opinion, this is a key problem of the paper since small variations might have a significant effects on the outputs. Also, for this reason, it is very difficult to judge the actual consistency of the outputs of the LLMs in the experiments carried out by the authors.\n\"\"\"\nIn our experiments, we evaluate each perspective along 50 permutations in the order of suggested answers. We therefore show that the induced value profile is consistent along permutations. This is further backed by the statistical analysis, which shows that the value profiles are consistent and different.\nOn a more general note, the difficulty of predicting which inputs will cause which value profiles to form is the point of this paper (as they will form in unexpected ways). This highlights the problem of evaluating LLMs from a single context.\n\"\"\"\nThe term controllability appears to me inappropriate, since the authors are not measuring actual \"controllability\" of the outputs in my opinion\n\"\"\"\nIn the second part of the paper we turn towards \u201cexpected\u201d changes, i.e. how we can intentionally induce a value profile. Here, we explicitly define the target values for the model to express. We refer to a form of conditional controllability where we control the model through its inputs.\n\"\"\"\nThe selection and the analysis of the application of the induction methods are not completely clear, especially with respect to the underlying research hypotheses at the basis of the study design.\n\"\"\"\nIn the paper, we argue that there are two types of changes, \u201cunexpected\u201d and \u201cexpected\u201d. Unexpected changes refer to those which are hard to predict and which we would ideally like to remove from all models. Expected changes refer to the ability of a model to be controlled, and these changes should be removed or not depending on the use case or even depending on the induction method (e.g. 2nd/3rd person, user/system message). For example, in some use case, one could prefer a model that is highly controllable by the system message and not controllable by the user message. We believe that models should be compared on their susceptibility to both types of changes.\nThe reviewer\u2019s comment refers to the second part of the paper, where we turn to \u201cexpected\u201d changes. There we outline the standard ways one would consider defining the target values (e.g. system message or user message)."}]}, {"Heading": "Official Review of Submission9417 by Reviewer Fkgw", "Subheading": "Official ReviewbyReviewer Fkgw30 Oct 2023, 13:57 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nIn general discourse surrounding the rise of LLMs, it is common to ascribe individual characteristics to LLMs. This paper challenges this tendency suggesting that it is more evident to view LLMs as a superposition of perspectives instead of as individuals. The paper provides empirical demonstrations to make this point. In particular, the experiments included show that LLM responses are context dependent in ways that differ from humans. The paper calls into question the use of psychological questionnaires to examine LLMs. The main contribution is the introduction of \"perspective controllability\" and an empirical demonstration to probe whether LLMs are robust to perspective shift effects and how different LLMs compare in terms of their perspective controllability.\nSoundness:\n1 poor\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\n(1) The experiments seem thorough and the paper states that reproducibility and transparency has been a priority.\nWeaknesses:\n(1) The framing of the paper needs significant improvement. The argument seems to go something like this: LLMs tend to be context-dependent. Humans tend to be stable across contexts. Therefore, LLMs should not be assumed to be human-like. Probing an LLM with questions derived from a field that assumes a human subject is flawed. Making general conclusions from results based on these questions is also flawed. LLM as a superposition not an individual is proposed is a new metaphor. This new metaphor motivated the study of perspective change in LLMs, which is the focus of this paper.\nNotice there are multiple jumps in this line of argument. First, the fact that LLMs are context-dependent needs to be reconnected to the point about LLMs being seen as individuals. You do not need to provide evidence that LLMs are not human-like to make this point. Second, the paper briefly argues that probing an LLM with questions derived from psychology is problematic but this point is not properly fleshed out or supported directly by results. Third, the point that general scientific conclusions are therefore problematic has not been properly made. Fourth, the reference to quantum mechanics is an interesting inspiration for said metaphor but is not a sound analogy in that language models do not operate in the quantum regime. Further, this inspiration is not necessary to make the argument laid out in this paper. Fifth, the main final point which is that studying perspective change in LLMs to study induction techniques is disconnected from the rest of these points and could stand as an interesting topic in itself.\n(2) Conclusions are overstated. The paper states \"we will see that discarding the old metaphor may question the interpretation of recent studies aiming at characterizing the values, personality traits, social skills or moral values of LLMs using tools developed to measure attributes of human psychology\". The current status of the argument has not led to this conclusion directly. The paper needs to reconnect and build out a cohesive careful argument in order to support this claim.\n(3) Exposition could be greatly improved throughout for clarity and precision. For instance, the related works section is written as part of the argument that recent work uses \"LLM as an individual\" metaphor, which should be discussed as such. The paper states \"There has been a lot of research studying large language models using tools from psychology...\" before the paper has fully developed the argument for what it means to view \"LLM as an individual\". It is more standard to use the related works section to contextualize this work in reference to existing literature not necessarily to support the content of your argument. Further, the paper states \"All these works aim to make general conclusions about LLMs\nbehavior, personality, or abilities, but they do not explore how personality traits expressed through\nbehaviour can change in unexpected ways over diverse unrelated contexts.\" which seems to say that the difference is in the focus on changes due to unrelated contexts. It would have been clearer to simply state that this work is related to other studies of personality traits but diverges in its focus on context-based shifts in performance. But for some reason, the section is written in a way that requires the reader to parse this out. \"At first glance, these might seem like examples of the unexpected perspective\nshift effect, however these effects are both common in humans, and their effect on the perspective\nchange is intuitive.\" This sentence is unclear and way too dense. And again, not exactly positioned properly in related work section if the function is to be part of the overall argument of the paper. The section continues with \"The second part of our paper studies how models\u2019 values and personality expression can be controlled, i.e., the expected perspective shifts due to context changes.\" This marks a shift in tone where the section is now describing the paper instead of the related work. Again, a sign of expository improvement needed.\n(4) The main focus is not clearly defined. The first sentence in the methods section states, \"This paper aims at uncovering the existence of unexpected perspective shift effects i.e. how context can impact the values and personality traits expressed by LLMs in unwanted, unexpected ways\". This is the definition provided. It is unfortunately unclear.\nQuestions:\n(1) What is the technical definition of \"unexpected perspective shift effects\"?\n(2) How do you distinguish between expected and unexpected? Expected by whom?\n(3) What is the technical definition of \"perspective controlability\"?\n(4) What is the theoretical basis for equation (1) ?\n(5) How does this compare to other measures of predictive inconsistency? Why is \"context\" so specifically interesting in this paper?\n(6) How do you define induced perspective? Can you offer theoretical analysis to support your measure?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Response to Fkgw (1/2)", "Subheading": "Official CommentbyAuthors22 Nov 2023, 13:41Everyone", "Content": "Comment:\nWe thank the reviewer for their response and for acknowledging the thoroughness of our experiments. We address the reviewer's comments below.\n\"\"\"\nThe argument seems to go something like this: LLMs tend to be context-dependent. Humans tend to be stable across contexts. Therefore, LLMs should not be assumed to be human-like. Probing an LLM with questions derived from a field that assumes a human subject is flawed. Making general conclusions from results based on these questions is also flawed. LLM as a superposition not an individual is proposed is a new metaphor. This new metaphor motivated the study of perspective change in LLMs, which is the focus of this paper.\n\"\"\"\nOur argument is much more straightforward (human-likeness is irrelevant). Psychological questionnaires are used on humans to make general conclusions (e.g. this person has that value profile). This is valid because humans tend to have stable value profiles (and those questionnaires assume that). The same is not valid for LLMs because they are highly context-dependent (which we show), i.e. the assumption underlying the questionnaires is broken.\nWe do not argue that using psychological tools for LLMs is flawed in itself, rather that it is not as straightforward. We argue that making general conclusions (like this model has that personality or ability) about highly context-dependent systems (LLMs) from a single context is flawed.\nWe believe that papers using questionnaires on LLM are both relevant and interesting, but that their conclusions can be greatly improved by analyzing how the expressed values/knowledge/abilities change along contexts. \nFor example, in some use cases one could prefer to use a model that has more stable values along many contexts to a model that has a more preferable value profile which was only tested in one context. And currently, we lack studies on the former.\n\"\"\"\nFirst, the fact that LLMs are context-dependent needs to be reconnected to the point about LLMs being seen as individuals. You do not need to provide evidence that LLMs are not human-like to make this point.\n\"\"\"\nThe \u201cLLM as an individual\u201d metaphor implies stability over contexts, because individuals have been shown to exhibit high value stability.\n\"\"\"\nSecond, the paper briefly argues that probing an LLM with questions derived from psychology is problematic but this point is not properly fleshed out or supported directly by results.\nThird, the point that general scientific conclusions are therefore problematic has not been properly made.\n\"\"\"\nWe empirically show how questionnaires give different conclusions based on trivial changes in context. This provides direct evidence against forming general conclusions about LLMs from single-context questionnaires.\n\"\"\"\nFourth, the reference to quantum mechanics is an interesting inspiration for said metaphor but is not a sound analogy in that language models do not operate in the quantum regime. Further, this inspiration is not necessary to make the argument laid out in this paper.\n\"\"\"\nThe introduction of superpositions is meant merely as an analogy and a metaphor.\nWe do not claim that our metaphor is necessary to make the argument, rather the metaphor is introduced merely to better explain our argument in another intuitive way. The \u201cindividual as LLMs\u201d metaphor is questioned, hence we suggest a new one which more aligned to our findings.\n\"\"\"\nFifth, the main final point which is that studying perspective change in LLMs to study induction techniques is disconnected from the rest of these points and could stand as an interesting topic in itself.\n\"\"\"\nThe paper studies how value profiles change based on context. This implies that we should study how this change can happen in unexpected ways (due to trivial changes in the context), but also in expected ways (by explicitly instructing the model to express some values).  We focus on the former in the first part of the paper, and on the latter in the second part."}, {"Heading": "Response to Fkgw (2/2)", "Subheading": "Official CommentbyAuthors22 Nov 2023, 13:44Everyone", "Content": "Comment:\n\"\"\"\n(2) Conclusions are overstated. The paper states \"we will see that discarding the old metaphor may question the interpretation of recent studies aiming at characterizing the values, personality traits, social skills or moral values of LLMs using tools developed to measure attributes of human psychology\". The current status of the argument has not led to this conclusion directly. The paper needs to reconnect and build out a cohesive careful argument in order to support this claim.\n\"\"\"\nThis point is reiterated from (1) \u201cSecond\u201d and \u201cThird\u201d, hence we address it there.\n\"\"\"\n(3) Exposition could be greatly improved throughout for clarity and precision. For instance, the related works section is written as part of the argument that recent work uses \"LLM as an individual\" metaphor, which should be discussed as such.\nIt is more standard to use the related works section to contextualize this work in reference to existing literature not necessarily to support the content of your argument.\n\"\"\"\nThe aim of related work was to outline different ways of conceptualizing LLMs (individual, population, role-playing) and to compare how our conceptualization is similar and different. We agree that the overall tone in the related work section might have been too argumentative due to time constraint. We updated the section accordingly.\nWe do not want to say that those outlined papers are flawed, quite the contrary. We believe that those papers are relevant, interesting and thought-provoking, and we build this work on top of them. We merely argue that the generality of their conclusion should be revisited, and that those studies could be greatly enriched by evaluating LLMs on questionnaires along many different contexts to study how the behavior changes.\n\"\"\"\n(4) The main focus is not clearly defined. The first sentence in the methods section states, \"This paper aims at uncovering the existence of unexpected perspective shift effects i.e. how context can impact the values and personality traits expressed by LLMs in unwanted, unexpected ways\". This is the definition provided. It is unfortunately unclear.\n\"\"\"\nWe introduce the concept of the \u201cunexpected perspective shift\u201d. Context influences the expression of values or personality traits. The nature of the relation between a context and the expressed values is unexpected (e.g. one does not expect the context of cpp code to decrease benevolence and latex to increase it). This is also unwanted because we do not have guarantees on the value expression of some LLM in a new context."}]}]}, "hxAveMWogn": {"paper_info": {"Keywords": "Video Point Tracking, Point Tracking, Tracking, Spatial-Temporal Vision, Segment Anything Model", "TL;DR": "Enhanced video point tracking via semantic-level tracking on the segmentation masks.", "Abstract": "This paper tackles a challenge in learning the long-term point trajectories in videos, like the Tracking Any Point (TAP) task. Fundamentally, the estimation of point-level motions is hindered by the significant uncertainty inherent in comprehensive comparisons across the entire video frame. While existing models attempt to mitigate this issue by considering a regularized comparison space (e.g., the cost volumes), point-level motion remains highly noisy, often leading to failures on individual points. To tackle the issue, our key idea is to jointly track multiple points within a given semantic object: since points in an object tend to move together on average, individual noise trajectories can be effectively marginalized, subsequently obtaining fine-grained motion information. Specifically, we predict the object mask using point-prompted segmentation provided by Segment Anything Models (SAM) and enhance the performance of existing models through a systematic two-stage procedure: (a) estimating the average motion of points within the object mask (predicted by SAM) as the initial estimate, and (b) refining this estimate to achieve point-level tracking. In stage (b), we actively generate fine-grained features around the initial estimate, preserving high-frequency details for precise tracking. Consequently, our method not only overcomes the failure modes seen in existing state-of-the-art methods but also demonstrates superior precision in tracking results. For example, on the recent TAP-Vid benchmark, our method advances the state-of-the-art performance, achieving up to a 25% improvement in accuracy at the 1-pixel error threshold. Furthermore, we showcase the advantages of our method in two downstream tasks: video depth estimation and video frame interpolation, exploiting the point-wise correspondence in each task.", "Supplementary Material": "pdf", "Primary Area": "representation learning for computer vision, audio, language, and other modalities", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9415", "PDF Url": "https://openreview.net/pdf?id=hxAveMWogn"}, "review_info": [{"Heading": "Author Rebuttal by Authors", "Subheading": "Official CommentbyAuthors18 Nov 2023, 01:51 (modified: 15 Mar 2024, 00:26)EveryoneRevisions", "Content": "Comment:\nDear reviewers and AC,\nWe sincerely appreciate your valuable time and effort spent reviewing our manuscript.\nAs reviewers highlighted, we believe our paper is generally well-written (sKxk), and the proposed scheme is easy and effective (tNTu), with the interesting idea for motion prediction (K1PM), demonstrated to be effective in the experiments (tNTu,sKxk).\nWe appreciate your constructive comments on our manuscript. In response to the comments, we would like to clarify several important concerns and questions:\nThe assumption behind Eq. 5 cannot be valid (tNTu).\nFirst, we would like to point out Eq. 5 is only responsible for approximating the instance-level motion rather than the final point tracking.\nIn fact, eq. 5. is intended to average out rotations, which do not significantly translate the mask region occupied by an instance. We note that rotations and non-rigid motions are handled by the subsequent high-fidelity tracking stage (Sec 3.4), where we do not average multiple points.\nThe contribution of this framework is limited (K1PM, sKxk).\nOur contribution is not limited to simply using SAM. For example, one could stack SAM for multiple video frames to estimate the instance motion, but it suffers from severe noises and huge computation costs for segmenting multiple frames. Instead, our method relies on SAM for only once at the first frame, and then tracks the semantic neighbor points through our sophisticated procedure Eqs. 2-7, in an efficent manner.\nMoreover, our performance boost is not solely due to the instance motion powered by SAM. For instance, our proposed high-fidelity point tracking enhances the high-frequency details in the visual feature (Sec. 3.4), and boosts the performance significantly. In Tab. 3, we provide the ablation study, finding a sole contribution by the module (Clip), e.g., 28.3 -> 35.4 J-1 metric.\nThank you very much!\nAuthors."}, {"Heading": "Official Review of Submission9415 by Reviewer tNTu", "Subheading": "Official ReviewbyReviewer tNTu01 Nov 2023, 08:03 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe core idea of this paper is that points on the same object are physically bound and should share the same motion statistics. To achieve this, the authors propose to average the initial motion estimates of some points on the object as the motion of the whole object. Then crop the video frames along the object trajectory to achieve high precision point trajectory tracking.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe proposed scheme is easy and effective, and its improved performance is demonstrated on various datasets as well as on downstream tasks.\nWeaknesses:\nEq 5 shows the present method takes average pixel displacements as the object motion and claims that it is a reliable instance-level motion estimation. But directly calculating the average pixel displacements is valid if and only if the object only translates in the image plane. This cannot be valid if the object is rigid and there is affine motion such as rotation, or if the object is non-rigid. I agree that such a naive assumption can be used as an initialization for model optimization, but I don't think it's an exciting innovation to elaborate on such great length.\nthe model Seg, which produces the segmentation mask for the initial frame. This method uses SAM for preprocessing, is it only for the first image or for all video frames? How do the later frames establish associations with the points in the earlier frames? This procedure needs further clarification.\nInstaTAP can be built on top of any existing point tracker. But there is no discussion in experiments.\nLack of computational complexity analysis. Looks like a very heavy optimization process.\nQuestions:\nDoes the proposed mechanisms require to retrain the existing point tracking models? Or just need to use publicly available pre-trained models?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9415 by Reviewer K1PM", "Subheading": "Official ReviewbyReviewer K1PM01 Nov 2023, 07:15 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis study tackles the challenge of accurately learning long-term trajectories for individual points in video sequences, such as the Tracking Any Point (TAP) task. Point-level motion estimation is hindered by the inherent uncertainty in comprehensive frame-wide comparisons. Existing models address this by using regularized comparison spaces like cost volumes, but they still suffer from noisy point-level motion, leading to tracking failures. To overcome this, the proposed method jointly tracks multiple points within a semantic object, leveraging the fact that points within an object tend to move together. By predicting object masks with Segment Anything Models (SAM) and implementing a two-stage procedure, the approach significantly improves tracking precision, surpassing state-of-the-art methods by up to 25% in accuracy on the TAP-Vid benchmark. Additionally, the approach demonstrates advantages in video depth estimation and frame interpolation by utilizing point-wise correspondence in these tasks.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nUsing Segment Anything Models (SAM) to enhance performance is a very interesting idea for motion prediction.\nWeaknesses:\nThe main contribution is to exploit SAM to boost the performance of tracking. However, the SAM is an existing method so the contribution of this framework is limited.\nThe proposed method's performance is closely tied to the effectiveness of SAM. If SAM encounters difficulties, such as producing inaccurate or poor segmentations, it can adversely affect the performance of the tracking framework. The quality of the object mask prediction by SAM directly impacts the tracking accuracy and robustness. Therefore, in scenarios where SAM struggles or fails to provide precise segmentations, the tracking performance may indeed experience a significant drop, highlighting the method's dependency on SAM's success in providing accurate object masks.\nQuestions:\nCan you show some failure cases of the proposed method? Is that related to the SAM results?\nFlag For Ethics Review:\nNo ethics review needed.\nDetails Of Ethics Concerns:\nNo concerns.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9415 by Reviewer sKxk", "Subheading": "Official ReviewbyReviewer sKxk30 Oct 2023, 08:17 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper is about tracking every point (TAP). Basically, it adopts a point-prompted segmentation by Segment Anything Model(SAM) to enhance the performance of existing models by estimating the average motion within the segmentation mask followed by a refinement stage to get final tracking results. Finally the authors evaluated on TAP-Vid benchmark to compare with previous published methods and show its practical usage in other vision tasks.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThe paper is in general well-written. It clearly pinpoints the limitations of existing methods, e.g. the failure cases of cost volume calculation, then proceed to the proposed method part.\nThe idea of utilizing occlusion information within weighted aggregation, is reasonable, and demonstrates to be effective in the experiments.\nWeaknesses:\nI am not a fun of the overall idea. Basically, this work is about an \"A+B\" style and I don't see too much innovation behind simply adding segmentation masks produced by SAM model. If SAM can be used, then any other SOTA semantic/instance segmentation models can be adopted to yield better performance. So the contribution here is a bit too trivial.\nThere are too many engineering stuffs in designing the tracker. For example, clipping, multi-scale operations. I don't see any \"learning\" stuff in the contribution side.\nOverall misleading claim. For example, in the abstract part, authors claim a SOTA performance \" For example, on the recent TAP-Vid benchmark, our method advances the state-of-the-art performance,\" But the performance is at least inferior to the paper [A] in TAP-Vid. For example, In Kinetics [A] has AJ 55.1, $\\sigma_{avg}$69.6 while this work achieves only AJ 51.4, $\\sigma_{avg}$65.8. On RGB-Stacking [A] has AJ77.5 and $\\sigma_{avg}$87.0 while this work gets AJ66.6 and $\\sigma_{avg}$81.8, this is also the case for DAVIS dataset. That is to say, the performance is NOT SOTA indeed, and it is hard to justify the usefulness of the overall idea.\nReferences:\n[A] Qianqian Wang et.al. Tracking Everything Everywhere All at Once. ICCV2023\nQuestions:\nI wonder is it possible to add other metrics in TAP-Vid, such as OA, TC, for a more complete comparison with prior methods?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}]}, "dYjuJGTEbc": {"paper_info": {"Primary Area": "optimization", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Gromov-Wasserstein Learning, Graph-based Clustering, Non-convex Optimization", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "Optimal Transport (OT) recently has gained remarkable success in machine learning. These methods based on the Gromov-Wasserstein (GW) distance have proven highly effective in capturing complex data topologies and underlying structures.\nMore specifically, Gromov-Wasserstein Learning (GWL) has recently introduced a framework for graph partitioning by minimizing the GW distance. Various improved versions stemming from this framework have showcased state-of-the-art performance on clustering tasks. \nBuilding upon GW barycenter, we introduce a novel approach that significantly enhances other GW-based models flexibility by relaxing the target distribution (cluster size) in GWL and using a wide class of positive semi-definite matrices.\nWe then develop an efficient algorithm to solve the resulting non-convex problem by utilizing regularization and the successive upper-bound minimization techniques.\nThe proposed method exhibits the capacity to identify improved partition results within an enriched searching space, as validated by our developed theoretical framework and numerical experiments. \nFurthermore, we bridge the proposed model with the well-known clustering methods including Non-negative Matrix Factorization, Min-Cut, Max-Dicut and other GW-based models. \nThis connection provides a new solution to these traditional clustering problems from the perspective of OT. \nReal data experiments illustrate our method outperforms state-of-the-art graph partitioning methods on both directed and undirected graphs.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9414", "PDF Url": "https://openreview.net/pdf?id=dYjuJGTEbc"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:55 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nReject"}, {"Heading": "Meta Review of Submission9414 by Area Chair gc9S", "Subheading": "Meta ReviewbyArea Chair gc9S06 Dec 2023, 10:32 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nBased on the reviews, it is clear that the paper has interesting aspects, such as the introduction of a novel Gromov-Wasserstein (GW)-type method for graphs, alongside theoretical analysis and extensive numerical evaluation. However, there are significant areas that require improvement. A major shortcoming identified by the reviewers is the lack of an in-depth discussion and comparison with existing works closely related to GW, particularly in the context of GW barycenters. Moreover, several aspects of the theoretical results are noted to require detailed rewriting to enhance clarity and comprehensibility. While the authors have addressed some of these issues in their rebuttal, the reviewers unanimously feel that the paper necessitates extensive rewriting. In light of these considerations, the paper, in its current form, does not seem ready for publication.\nJustification For Why Not Higher Score:\nGW is now well established, the paper does not includes enough citations/comparison to related works.\nJustification For Why Not Lower Score:\nN/A"}, {"Heading": "Official Review of Submission9414 by Reviewer 2sWH", "Subheading": "Official ReviewbyReviewer 2sWH01 Nov 2023, 07:49 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper presents a Gromov Wasserstein (GW) Clustering Method based on a single marginal GW Barycenter : EGWB. The method also allows to add marginal or ambient metric constraints on the barycenter. Thus effectively showing that their method is a generalization of existing GW Learning methods. Later on more precise links are made with existing methods.\nThey first introduce a Monge type barycenter problem and a Kantorovich relaxation of it. It is shown that under appropriate conditions the two problems are equivalent.\nAn optimization algorithm relying on entropic regularization is presented. The convergence of the algorithm is shown.\nFinally their method is benchmarked against existing GW Clustering methods, on synthetic and real data. On all accounts EGWB outperforms existing GW methods.\nThe contributions are the following:\nIntroduced a generalization of existing GW Learning Methods\nDemonstrated theoretically and empirically their algorithm for solving the problem converges and has state of the art performances\nSoundness:\n4 excellent\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nThe paper presents a unifying framework for GW methods. It does so clearly.\nThe core idea is to introduce the GW barycenter problem and note that adding constraints recover existing methods. This problem seems novel in that context and they address with clarity the first questions one can have  : equivalence between the Monge and Kantorovich type formulations, link with other methods in GW Learning as well as in Graph partitioning.\nTheir algorithm is an alternating minimization one. However they address the non convexity of the transport plan update by using an interesting combination of existing regularization methods : entropic regularization, link with the Wasserstein Barycenter problem which has better structure.\nThe synthetic data example is informative of how the barycentric nature of the problem allows for more efficient clustering. In the analysis of the performance on real data the explanation of the performance in relationship with the structure of the data is appreciated.\nWeaknesses:\nIn the paragraph about Kantorovich relaxation it is stated that the minimum is attained at an extremal point under some conditions which are detailed in appendix. This point is central to the use of the algorithm afterwards. Thus I believe the conditions should be put forward in the main text.\nIn theorem 3 it is unclear in which case the algorithm converges with entropic regularization, however this is central to showing that the implemented algorithm does converge.\nQuestions:\nHow does the result of theorem 3 relates to the convergence of the algorithm implemented in practice?\nAre there a stability result of the limit of the algorithm/solution of the problem with respect to the epsilon parameter?\nIn practice what are the optimal value used for the epsilon parameter for each datasets?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:51 (modified: 23 Nov 2023, 06:52)EveryoneRevisions", "Content": "Comment:\nWe appreciate your insightful observations and constructive criticism.\n1st point in the weakness comment: \nWe put forward the condition (19) in the last paragraph in section 3.1 of the main text while leaving the proof in the appendix.\nTheorem 3 provides a conservative condition between $\\lambda$ and $\\epsilon$ under which the convergence of the alternating algorithm is guaranteed. In practice, we observe that even this condition is violated, the convergence still holds. Anyway, this condition roughly offer a general guideline on selecting practical parameters in order to make the algorithm work.\nThe stability result with respect to $\\epsilon$ is still on demand. However, we numerically found that $\\epsilon$ does affect the clustering results. This is commented in detail in the next item.\nWe found that the results were sensitive to the choice of regularization parameter $\\epsilon$ (as is also observed by [a][b]), leading to numerical blowups if not chosen carefully. In reporting each of the results below, we hand-tuned $\\epsilon$ as $\\epsilon=2e-5$ for WIKI and INDIAN datasets, and $\\epsilon=3e-6$ for EU-email and AMAZON datasets.\n[a] Hongteng Xu, Dixin Luo, and Lawrence Carin. Scalable gromov-wasserstein learning for graph\npartitioning and matching. Advances in Neural Information Processing Systems, 32, 2019a.\n[b] Samir Chowdhury and Tom Needham. Generalized spectral clustering via gromov-wasserstein\nlearning. In International Conference on Artificial Intelligence and Statistics, pp. 712\u2013720. PMLR,\n2021."}]}, {"Heading": "Official Review of Submission9414 by Reviewer iAAc", "Subheading": "Official ReviewbyReviewer iAAc31 Oct 2023, 12:15 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nIn this submission, the authors propose a new graph partitioning framework EGWB, which relaxes the target structure and distribution constraints in Gromov-Wasserstein Learning (GWL) with a class of positive semi-definite matrices.\nIn particular, by learning the target structure matrix associated with the transport plan, the authors extend the GWL framework to a special GW barycenter problem (with only one graph), which enhances the flexibility of the GWL framework.\nThe proposed method is shown to be effective according to empirical results in various graph partitioning tasks.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nGraph partitioning based on utilization of the Gromov-Wasserstein (GW) distance is an interesting and significant problem.\nWeaknesses:\nHow to initialize $D\u2019(0)$? How to set the value of $K$?\nIt seems the authors confuse the task of graph partitioning and that of graph clustering. They muddle up partitioning and clustering throughout this paper. \nI think the experiments in section 4.2 are more likely to be a graph partitioning task, rather than graph clustering, as is claimed by the authors. Please use one of the two definitions consistently in the paper.\nIn the subsection of Results and Discussion, the authors say they employ five metrics, however, I only find AMI. If they take the results reported in appendix into account, then should clarify this in the main context.\nAre the datasets in section 4.2 asymmetric or symmetric? Do the authors symmetrize the directed graphs?\nWhat do the two axes in Fig.3 represent? It should be labeled in the figure.\nThere are typos and careless statements and the authors need to polish this paper carefully. For example, 1) page 1, it should be $G_1(D_1,P_1)$ in the third row from the bottom, 2) page 2, the second paragraph, the second point of the limitations has grammatical mistakes, 3) What is EGWB an abbreviation for? The author put forward EGWB without any explanations in page 6.\nQuestions:\nPlease see above.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:56Everyone", "Content": "Comment:\nWe appreciate some of the review's comments while cannot agree with the others. We do not think the reviewer carefully read and correctly understand our paper. Thus his/her comment is subjective and not fair.\nThe  $D^{'}(0)$ can be readily derived from equation (10) of our paper, provided one thoroughly peruses our document. Anyway, we can explain it more in detail here: \n When we have an initial guess $\\pi(0)$, we employ $$\\frac{\\pi^{\\top}\n{:,j}(0)D\\pi\n{:,j}(0)}{\\pi^{\\top}\n{:,j}(0)\\pi\n{:,j}(0)}$$ as the diagonal of  $D^{'}(0)$.  The selection of $K$ follows standard procedures as outlined in any clustering textbook. Consequently, in our paper, we treat $K$ as a predetermined parameter, ensuring a fair comparison across all methods.\nThe main focus of our paper is the clustering based on graph structure. This is clearly stated in section 3.1, where our aim is to solve the hard clustering problems via graph partitioning. The point of doing clustering via graph partitioning has already been made in the literature prior to our paper, see [a][b].\nOnce more, it appears that the reviewer has not meticulously perused our paper. We have meticulously presented comprehensive numerical results in the appendices, encompassing indices such as ARI, V-measure, and FMI. Nevertheless, to further address this concern, we will augment the main text with a clarifying sentence.\nThe datasets in Section 4.2 consist of undirected graphs, rendering their adjacency matrices naturally symmetric. In instances involving directed graphs, we opt for the normalized Laplacian, elucidated within the framework of random walks [c]. In all of our numerical experiments, we use the natural adjacency matrices without any artificial symmetrization in order to make our comparison fair. This choice is deliberately articulated in the supplementary materials appended in the appendices.\nFigure 3  illustrates the AMI results of SpecGWL, srGW, and EGWB across various heat kernel times (t) for the WIKI dataset.  This figure serves as an illustrative representation of our motivation for delving into graph-based hard clustering.   We still thank the reviewer for pointing out this ambiguity, and in response, corrected labels have been incorporated in the revised paper.\nThank the reviewer for point out those typos. They have been corrected.\n[a] Hongteng Xu, Dixin Luo, and Lawrence Carin. Scalable gromov-wasserstein learning for graph\npartitioning and matching. Advances in Neural Information Processing Systems, 32, 2019a.\n[b] Samir Chowdhury and Tom Needham. Generalized spectral clustering via gromov-wasserstein\nlearning. In International Conference on Artificial Intelligence and Statistics, pp. 712\u2013720. PMLR,\n2021.\n[c] Fan Chung. Laplacians and the Cheeger inequality for directed graphs. Annals of Combinatorics, 9(1):1\u201319, 2005."}]}, {"Heading": "Official Review of Submission9414 by Reviewer Zq9r", "Subheading": "Official ReviewbyReviewer Zq9r28 Oct 2023, 06:52 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper propose a graph partitioning approach based on the Gromov-Wasserstein (GW) distance. This approach minimizes the GW distance between the target graph and an empty graph with fewer number of nodes. The optimization problem finds an optimal mapping, which determines the node clusters. It jointly optimizes node mass distribution and pairwise distance relations in the empty graph, the later constrained to be diagonal. The authors propose an algorithm which alternates the minimization of these variables and provide theoretical convergence guarantees.\nFurthermore, the authors establish connections between their approach and existing GW-based methods, as well as alternative techniques like Min-Cut. Finally, they empirically demonstrate the effectiveness of their proposed method.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe authors present an extension of previous GW-based clustering methods, while connecting them with other approaches such as the Min-Cut or Non-negative Matrix Factorization. In addition, they propose an algorithm with theoretical guarantees. In this regard, the paper seems to be theoretically well founded.\nAdditionally, their proposed algorithm showcases remarkable robustness against edge noise when compared to the competing methods outlined in the paper.\nWeaknesses:\nChallenges in Readability: In certain instances, the meaning of the notation, although not formally introduced, can be grasped from the context (e.g., $mathbb{I}_K$ denoting the identity matrix with $K$ rows). However, there are situations where the notation becomes confusing, posing a challenge to the paper's readability. For example, in the discussion of the \"Monge's type Gromov-Wasserstein barycenter\" in Section 3, the optimal mapping matrix is denoted as $MGW(G, G\u2032)$, but this notation is also used as the objective in the minimization equation (7). The concept of minimizing a matrix raises confusion. In addition, the symbols $\\pi$ and $\\Gamma$ are interchangeably used to refer to the same object. For instance, three lines before equation (7), it states $\\nu=\\pi^T 1_N$, but in this context, as far as I did not misunderstand it, we are assuming a hard clustering mapping and therefore $\\Gamma$ should be the appropriate symbol. Furthermore, though it is not crucial, adding the labels to the x-axis and y-axis of the plots would also ease the readability of the figures.\nThe proposed method initialization depends on the results of other methods such as GWL and SpecGWL.\nQuestions:\nInitialization dependency: The paper mentions using a linear combination of GWL, SpecGWL, and joint distribution results as initial values for EGWB. However, it's unclear how the algorithm relies on this initialization. Could a less informed start, like the uniform distribution, yield comparable results or does one need to start from a relatively good initialization?\nComputational cost: While the paper outlines the computational cost per iteration, the average number of iterations required for convergence remains undisclosed. Additionally, considering the initialization dependency, it's crucial to know the overall time needed to run EGWB, especially if solutions for GWL and SpecGWL must be computed beforehand.\nSynthetic data: Figure 4 exhibits superior results for GWL and SpecGWL compared to Figure 2. Moreover, in Figure 4 GWL out performs SpecGWL given the true cluster size distribution. Is there any reason why is that the case? I am actually surprised that, for these apparently simple problems GWL and SpecGWL fail to retrieve the right clustering. Understanding the specific reasons for their failure is beneficial in order to comprehend why EGWB, in contrast, succeeds.\nNumber of clusters: A parameter that needs to be set is the expected number of clusters $K$. This is indicated by the number of nodes in the empty graph. How robust is the algorithm to the choice of $K$? Particularly intriguing is the scenario where $K$ exceeds the actual number of clusters. In theory, it is possible that the optimal mapping does not assign any mass to the extra nodes of the empty graph. In that case, the algorithm would still be able to retrieve the true partition. Does this happen in practice, and how does the algorithm adapt to such situations?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 06:57 (modified: 23 Nov 2023, 07:16)EveryoneRevisions", "Content": "Comment:\nWe appreciate your insightful observations and constructive criticism.\nThank the reviewer for point out those typos, and in response, these have been incorporated in the revised paper. Specifically, (i) MGW represents the Monge-type GW distance induced by the optimal mapping $\\Gamma$. It also depends on the target graph $G^{'}$, which is composed of a matrix $D^{'}$ and node distribution $\\nu$. Our objective is to find the optimal graph $G^{'}(D^{'},\\nu)$. (ii) Figure 3  illustrates the AMI results of SpecGWL, srGW, and EGWB across various heat kernel times (t) for the WIKI dataset.  This figure serves as an illustrative representation of our motivation for delving into graph-based hard clustering.\nIn fact, we thoroughly elaborate on this experiment in the appendix. It is crucial to recognize that this challenge poses a non-convex optimization problem. Consequently, selecting a  well-chosen initial value undoubtedly improves the optimality of the final convergence point to a certain extent.\nBoth GWL and SpecGWL provide a transport plan $ \\pi $ in the joint distribution space. Using the combinations of these plans as initial points is a natural idea. Moreover, if a uniform distribution is directly employed as the initial value, theoretically, it is equivalent to SpecGWL in the early stages of the algorithm.\nThe theoretical prediction of average numbers of iterations is still open for such non-convex optimization problems. However, in experiments, we observe that during the inner optimization process for $\\pi$, approximately three hundred iterations are required to reduce the relative error, denoted as $ |\\frac{\\pi^{t+1} - \\pi^{t}}{\\pi^{t}}|_F $, from $ O(1)  $ to $ O(1 \\times 10^{-4}) $.\nIndeed, in our experiments, SpecGWL achieved a smaller GW distance compared to GWL. The crucial point is that both methods employ a fixed $G^{'}(D',\\nu)$ and then learn the transport map, which makes the clustering results less convincing. In contrast, our EGWB treats it as a variable for optimization, introducing more degrees of freedom that are determined by the data, naturally yielding superior results.\nIn the graph-based clustering experiments conducted, $ K $ is a pre-determined parameter. However, in theoretical analysis, by relaxing the weight constraints, the optimal value $ \\nu $can be sparse, implying that some classes may be empty sets. In such cases, we can set the corresponding $ D^{'} $ to a relatively large constant (eg, 1), as discussed in reference [a]. In our numerical experiments, we treat $K$ as a prescribed parameter such that all methods are compared on the fair stage.\n[a] C\u00e9dric Vincent-Cuaz, R\u00e9mi Flamary, Marco Corneli, Titouan Vayer, and Nicolas Courty. Semi-relaxed gromov-wasserstein divergence and applications on graphs. In International Conference on Learning Representations, 2022."}]}, {"Heading": "Official Review of Submission9414 by Reviewer EjY1", "Subheading": "Official ReviewbyReviewer EjY123 Oct 2023, 16:48 (modified: 27 Nov 2023, 10:15)EveryoneRevisions", "Content": "Summary:\nOver a single input graph (D, h), authors study the problem of learning via the Gromov-Wasserstein loss (GW) a non-negative diagonal target structure D\u2019 and its masses h\u2019, in order to perform a partitioning of (D, h) via the underlying estimated (GW) transport plan. They propose to use a Block-Coordinate Descent algorithm that alternates between i) estimating a semi-relaxed GW transport plan using a mirror-descent scheme with an additional strictly concave regularization; ii) updating the diagonal structure D\u2019 in closed-form. Authors empirically study the concavity of the resulting problem, and provide proofs of convergence for their algorithm. Then, they connect this GW (diagonal barycenter) problem or simplified variants to well-known clustering methods such as Min-Cut based methods, NMF and Max-Dicut. Finally, they study the relevance of their approach for graph partitioning of synthetic and real-world datasets.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nIntroduce a novel GW-based transport problem to perform graph partitioning.\nProvide first results on the concavity of this problem. Then provide an algorithm to and introduce a strictly concave regularization of this problem that might help for such tasks.\nProof of convergence of their BCD algorithm.\nInteresting but simple connections with existing clustering methods such as Min-Cut based methods , NMF and Max-Dicut.\nBenchmark on GW-based and SOTA approaches for graph partitioning.\nWeaknesses:\nOverall appreciation\n: This paper tends to omit very similar recent works and the theoretical results seem either incomplete (analysis of concavity and relationships with existing graph partitioning/clustering methods) or incremental.\nNB: I added comments after authors' rebuttal in italic for each point\n1. Authors omit several recent contributions on GW\n:\na)\n[A] studies the complete (sr)GW barycenter problem where the target structure is not forced to be a non-negative diagonal matrix. This paper shows that it is a SOTA method for spectrum-preservation graph coarsening and provide strong theoretical contributions supporting its use.\ni)\nThese spectrum properties are also of particular interest for graph partitioning, hence the (sr)GW barycenter problem should be rigorously compared theoretically and empirically to the (sr)GW diagonal barycenter problem.\nii)\nI believe that simple Stochastic Block Models (SBM) could provide a stress test over which the srGW diagonal barycenter problem fails contrary to the srGW barycenter problem : e.g using non-assortative SBM with a unique intra-cluster connectivity p smaller than inter-cluster connectivities $q_{cluster_i, cluster_j}$. Moreover I expect the diagonal structure to be more sensitive to contrasts within any SBM, i.e small variations between intra/inter-connectivities. Could authors perform such empirical sanity-checks ?\n[\nNone of these points have been clearly addressed by the authors during rebuttal\n]\nb)\nrelations to srGW [Vincent-Cuaz et al, 2022] :\ni)\nThe (sr)GW barycenter problem over a single input graph is a particular case of their dictionary learning.\nii)\nThe srGW solver proposed by authors is exactly the mirror-descent algorithm introduced in this other paper over which a concave regularization is added. These two points should be clearly stated in the paper.\n[\nNone of these points have been clearly addressed by the authors during rebuttal\n]\nc)\nOn the proof of convergence for the algorithm:\ni)\n[B, C] provide a scheme of proof to establish a non-asymptotic convergence of the regularized srGW solver. [\nNot considered by the authors during rebuttal\n]\nii)\nAn overview of the proof strategy for Theorem 3 should be clearly state  [\nNot considered by the authors during rebuttal\n]. More importantly Lemma 2 should be clarified : as such it seems wrong/ incomplete to me, e.g differentiability issues at the border are avoided, limits are considered out of the domain, continuity arguments are used without defining any topology etc...\n[\nI am sorry I made a mistake on this matter, Lemma 2 is correct. As first suggested, an overview of the proof strategy would have helped for readability. Moreover authors follow a proof scheme from another more generic paper over which it would have been relevant to discuss relations. From my understanding, the first convergence proof for the srGW problem provides bounds involved in their finale convergence analysis. A sharper convergence proof - following B, C - could provide a sharper analysis and adaptive scheme for their regularization parameter.\n]\niii)\nThe overall learning algorithm seems to be a particular case of two-block BCD well-studied in [D]. [\nReference not considered by authors. It implies that more generic converge proofs already exist for their BCD.\n]\nd)\nFirst parts of the supplementary materials: (minor) paragraph  \u2018Non-convexity of GW discrepancy\u2019 exposes known relations. (more important) paragraph \u2018Assumption of uniform distribution\u2019 seems to be a bad justification for the choice of input distributions that only translates the notion of weak-isomorphism discussed in [Chowdhury et al, 2019].\n[\nRebuttal made by authors is not compelling.  My point is that this dilution of mass// duplication of points of the support is absolutely not a justification for assuming a uniform distribution. What matter is the total mass assigned to the original point of the support. Your justification is misleading and formally wrong if you rigorously acknowledge the support of the measure. You can say we pick uniform distributions because it is the most common choice, note that there are other options (degrees etc..)... Moreover the sensitivity analysis in the supplemetary material is really not clear, that would be better to see a complete benchmark with same hyper parameter validation with several cases as b = 0 / b =1 / b in ]0, 1[. Maybe that is a by-product of learning the diagonal/complete barycenter, or just of diverse regularization coefficients. \"which further validates our mass splitting technique.\" just does not make sense.\n]\n2. The several concavity analysis done by authors are incomplete and not conclusive:\na)\nCould you detail the experiments illustrated in Figure 1 ? What is D\u2019 in this setting ? What Is the initial transport plan used for these experiments ? Are these findings consistent w.r.t these initial transport plans (should be validated using the MCMC sampler in SpecGWL) ? What are the solvers used for these experiments ? If entropically regularized ones, please compare results to exact solvers such as conditional gradient solvers.\n[Partially addressed by authors]\nb)\nAre these findings specific to heat kernels or do they generalize to PSD matrices e.g Laplacians  ?\n[Misunderstood by authors - no time for discussions]\nc)\nNone of the theoretical studies on the concavity of the overall learning problem are complete or convincing:\ni)\nproof/paragraph: \"One common condition for extremal points\" only shows that for an optimal target masses nu*, the resulting GW problem is concave hence solutions are extremities of admissible coupling with marginals mu and nu*. It does not show that extremities of the set of semi-relaxed couplings with first marginal mu, i.e hard-clustering matrices, are solutions.\n[Incomplete analysis by authors maybe we misunderstood each other on the term extremities, considering that they always assume the existence of corners for $U(mu, nu^\\star)$ no matter $nu^\\star$ .]\nii)\nI do not see when the other condition in equation 19 could be applied, authors should discuss this point.\n[partially addressed but still emphasises the strong specificity of this result]\n.\niii)\nRemark: Overall, a too recent contribution [E] to be taken into account at the submission date deals with these concavity problems for srGW barycenters and could guide authors to derive an analog result for the srGW diagonal barycenters.\n3. Zero masses\n: Authors do no mention the flexibility of this learning problem to get optimal target masses which are equal to zero and might allow to detect true number of clusters in some settings, as discussed in [Vincent-Cuaz et al, 2022].\n[partially addressed by authors]\n4. Missing points in experiments\n:\ni)\nPlease benchmark methods in terms of running times too. Trade-off in terms of performances and speed should be explicit.\n[partially addressed by authors]\nii)\nThe strongly concave regularization with continuation scheme proposed by authors introduce several hyperparameters. Please conduct an ablation study over this regularization. Plus could you complete Figure 11  that shows that the method is quite sensitive to these hyperparameters, with other datasets ?\n[Ablation study not considered by authors. Sensitivity analysis apparently completed but from the revised paper version we can not even know what is the dataset used in these experiments]\niii)\nAuthors rely on other GW-based methods to get initial transport plans for their method. Whereas [Vincent-Cuaz et al, 2022] proposed to leverage k-means algorithm, which is a quite common technique in the clustering or graph partitioning literature. I guess, in concave setting solvers can be stuck at extremities, hence it would be relevant to force initial within the polytope e.g with kmeans + mu.nu^T. Could you further compare these choices ?\n[not considered by authors]\n5. Some parts in Section 3.3 are not clear\nand should be clarified:\ni)\nsrGW to Identity: \u2018This results in each cluster containing an equal number of data points.\u2019 It clearly does not seem to be the case.\nii)\nrelation to NMF: does it really coincide with the srGW diagonal barycenter problem or rather with the complete barycenter problem ?\n[addressed by authors]\n[A] Chen, Yifan, et al. \"A Gromov--Wasserstein Geometric View of Spectrum-Preserving Graph Coarsening.\" International Conference on Machine Learning. 2023.\n[B] Scetbon, M., Cuturi, M., & Peyr\u00e9, G. (2021, July). Low-rank Sinkhorn factorization. In International Conference on Machine Learning (pp. 9344-9354). PMLR.\n[C] Scetbon, M., Peyr\u00e9, G., & Cuturi, M. (2022, June). Linear-time gromov wasserstein distances using low rank couplings and costs. In International Conference on Machine Learning (pp. 19347-19365). PMLR.\n[D] Grippo, L., & Sciandrone, M. (2000). On the convergence of the block nonlinear Gauss\u2013Seidel method under convex constraints. Operations research letters, 26(3), 127-136.\n[E] Van Assel, Hugues, et al. \"Interpolating between Clustering and Dimensionality Reduction with Gromov-Wasserstein.\" arXiv preprint arXiv:2310.03398 (2023).\nQuestions:\nI invite the authors to discuss the above-mentioned weaknesses and to answer the questions (potentially implying additional experiments) I have associated with them in order to complete my development.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 07:00 (modified: 23 Nov 2023, 07:14)EveryoneRevisions", "Content": "Comment:\n1.1 We sincerely apologize for overlooking this article. We appreciate the reviewer's excellent suggestions and valuable references. We will  conduct a more systematic study comparing these various methods. We also welcome the reviewer to engage in more detailed discussions and future study with us. \n Regarding srGW, it can be considered a special case of EGWB where $D^{'}$ is fixed as the identity matrix. Additionally, the concave regularization introduced in EGWB is aimed at reducing $| \\pi |_{L_0}$, increasing its sparsity for obtaining hard clustering results. In srGW, the concave regularization is intended to make $\\nu$ sparser, reducing redundant clusters. Their designs have different intentions, leading to naturally different outcomes.\n1.2 The proof of Lemma 2 is derived from reference [a]. If you harbor any inquiries, you may peruse the details. All functions in our paper exhibit continuous differentiability.\n1.3 Kindly note that our intention was merely to elucidate that any GW problem can be reformulated under the assumption of $\\mu$ follows the uniform distribution , and this does not imply that our algorithm is exclusively effective for uniform distributions. Our interest in this scenario stems from its simplicity and convenient explication of connections with other clustering methods, such as Min-Cut. The theorems we have proven can be effortlessly extended to encompass any distribution. Furthermore, as demonstrated in our experiments detailed in Appendix B 3.5, the utilization of power-law transformations of data point distributions as $\\mu$ does not impact the final results of EGWB.\n2.1 Figure 1 depicts the code using SpecGWL directly with the WIKI dataset, where all hyperparameters and initial values precisely match those of SpecGWL. The intention is to demonstrate that, after thresholding (i.e., selecting the index with the maximum element in each row as the clustering label), the GW distance is further reduced. Therefore, our EGWB focuses on directly seeking hard clustering.\n2.2 All convergence proofs are applicable to all positive semi-definite matrices.\n2.3 (a) We believe the reviewer has misunderstood our intent. We demonstrated that in the Monge GW problem (which is essentially finding the optimal mapping, corresponding to hard clustering), the setup is such that any $ \\nu $ must satisfy the condition that each node can only belong to one class. Therefore, its Kantorovich relaxation problem is about minimizing a concave function over a convex set, where the solution naturally belongs to the extremal points of $ \\Pi(\\mu, \\nu) $, it is also a part of the extremal points of $\\Pi(\\mu, \\cdot)$. Hence, our problem setup involves using the extremal points of $ \\Pi(\\mu, \\cdot) $ as the feasible set. \n        (b) If $ D $ and $ D^{'} $ are similarity matrices with values ranging between 0 and 1, it is natural that the diagonal of $ D $ is all 1's, satisfying this condition. However, ensuring that $D$ is a positive semi-definite matrix is an open question.\n3  In theoretical analysis, by relaxing the weight constraints, the optimal value $ \\nu $ can be sparse, implying that some classes may be empty sets. In such cases, we can set the corresponding $ D^{'}_{tt} $ to $-c$ in our paper, where $c$ is a large number (or 1 as discussed in srGW), ensuring that no nodes will be clustered in the $t$-th set in the next iteration. This is because\n$\\sum_{i, i^{'}} \\left(D_{ii^{'}}-D_{tt}^{'}\\right)^{2} \\pi_{i t} \\pi_{i^{'} t}$ will be larger than any other component in $ \\mathcal{E}_{D, D^{'}}(\\pi) $.\n4.1 The runtime is not orders of magnitude different because we employ a two-layer optimization, simultaneously optimizing both the target matrix and the transport plan. Each sub-optimization problem for the transport plan has a complexity almost identical to that of GWL or SpecGWL. However, our advantage lies in introducing higher degrees of freedom, theoretically allowing for better results in more complex scenarios.\n4.2 We have presented a parameter ablation analysis in Appendices B 3.4, encompassing indices such as GW distance, AMI, ARI, V-measure, and FMI.\n5.1 The objective function of srGW is equivalent to a standard Min-Cut function augmented by an exclusive lasso regularization term with a coefficient of $\\frac{1}{2}$. The minimum value of this regularization term is achieved when the sizes of all classes are equal. You can find the proof in our Appendix B.1  for details.\n5.2 According to our proof in Appendix B.1, page 21, for any square matrix, Weighted Symmetric NMF is equivalent to the complete Monge-type GW barycenter problem, which means the feasible set is constrained to be mappings represented by rectangular permutation matrices.\n[a] Razaviyayn, Meisam, Mingyi Hong, and Zhi-Quan Luo. \"A unified convergence analysis of block successive minimization methods for nonsmooth optimization.\" SIAM Journal on Optimization 23.2 (2013): 1126-1153."}]}]}, "hcXfzlmg7Y": {"paper_info": {"Primary Area": "societal considerations including fairness, safety, privacy", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "AI Safety, Moral Decision-Making, Cognitive Science, Large Language Models", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "TL;DR": "Our paper investigates the moral tendencies of large language models when addressing trolley problems.", "Abstract": "As large language models (LLMs) are becoming deployed in more and more systems, it is crucial to inspect the moral tendencies in their decision-making process. In this paper, we take the largest set of moral judgments collected to date, the trolley problems, to check the preferences stated in LLMs' responses, as well as their supporting reasons. We first conduct a large-scale inspection on over 10K trolley problem scenarios in English, and compare them with human preferences. Then we extend the questions to different cultures by changing the language of the prompts and positioning the questions for different persona. We discover that LLMs tend to generate more human-aligned responses for the English culture, but also show interesting tendencies on other cultures.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9413", "PDF Url": "https://openreview.net/pdf?id=hcXfzlmg7Y"}, "review_info": []}, "kIZcruKmBg": {"paper_info": {"Keywords": "physics-informed, neural networks, transformation, manifold, diffeomorphism, parametrized, geometry, reference domain, free boundary, shape optimization", "Abstract": "Physics-informed neural networks (PINNs) effectively embed physical principles into machine learning, but often struggle with complex or alternating geometries.\nWe propose a novel method for integrating geometric transformations within PINNs to robustly accommodate geometric variations. Our method incorporates a diffeomorphism as a mapping of a reference domain and adapts the derivative computation of the physics-informed loss function. This generalizes the applicability of PINNs not only to smoothly deformed domains, but also to lower-dimensional manifolds and allows for direct shape optimization while training the network.\nWe demonstrate the effectivity of our approach on several problems: (i) Eikonal equation on Archimedean spiral, (ii) Poisson problem on surface manifold, (iii) Incompressible Stokes flow in deformed tube, and (iv) Shape optimization with Laplace operator.\nThrough these examples, we demonstrate the enhanced flexibility over traditional PINNs, especially under geometric variations. The proposed framework presents an outlook for training deep neural operators over parametrized geometries, paving the way for advanced modeling with PDEs on complex geometries in science and engineering.", "Supplementary Material": "zip", "Primary Area": "applications to physical sciences (physics, chemistry, biology, etc.)", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9410", "PDF Url": "https://openreview.net/pdf?id=kIZcruKmBg"}, "review_info": [{"Heading": "Official Review of Submission9410 by Reviewer 2kyY", "Subheading": "Official ReviewbyReviewer 2kyY31 Oct 2023, 12:12 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe work proposes a method to enhance Physics-informed Neural Networks (PINNs) by integrating geometric transformations, to address challenges posed by complex or non-euclidean geometries. \nThe method utilizes a diffeomorphism $\\phi$ that maps a reference domain $\\Omega_{ref}$ to the observation domain $\\Omega$, adapting the derivative computation in the physics-informed loss function. The approach was demonstrated through various problems: Eikonal equation on Archimedean spiral, Poisson problem on surface manifold, Incompressible Stokes flow in deformed tube. Finally, they show that their method can be applied to perform shape optimization according to a Laplace PDE loss.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n1 poor\nStrengths:\nThe paper is easy to read and the geometric transformation seems reasonable to solve this kind of problem. The first three different examples each test a different geometric setting. The figures are pretty.\nWeaknesses:\nThe method relies on the output transformation trick to enforce boundary conditions (BC), which is well suited for Dirichlet BC only. It would not be applicable as is for different kinds of BC, but the authors have a much more general claim.\nExcept for the last example, which we will discuss next, the diffeomorphism $\\phi$ is known a priori. Therefore the method in such case simply looks like a change in variable with a known function. How can you apply this method on a domain which is not equipped with such a transformation ?\nThe last example is very mysterious to me. I actually do not understand what the method is supposed to achieve by learning simultaneously to impose the PDE constraint and the geometric transformation. Do we know what target geometry the network should converge to ? Besides, the network that learns the transformation is not a diffeomorphism, so there is no guarantee that the optimization problem finds a correct solution.\nThe authors do not compare their method with any existing work. There is no literature review. As a result, we do not really understand why these problems cannot be tackled with existing methods. Why do they fail ?\nThe authors do not provide any numerical results for their methods, and even the qualitative results do not include the ground truth solutions. It is therefore impossible to judge the effectiveness of the method.\nQuestions:\nWhat is the difference between $\\mathcal{L}$, $\\mathcal{L}_x$ and $\\mathcal{L}_y$ concretely for each example ?\nWhat does the following sentence mean ? \"transformed PINN finds the exact length with an error of = 0.1 %\" .\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n1: strong reject\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 03:25Everyone", "Content": "Comment:\nThank you for your review! Allow us to respond to your comments as follows.\nThe method relies on the output transformation trick to enforce boundary conditions (BC), which is well suited for Dirichlet BC only. It would not be applicable as is for different kinds of BC, but the authors have a much more general claim.\nWe are not sure which claim you are referring to. Does this refer to \u201cFor simplicity, let us denote Dirichlet boundary conditions only\u201d?\u2028\u2028 We are not claiming that our approach is applicable to different kinds of BC than Dirichlet ones, and we definitely didn\u2019t intend to let it appear more general. Besides, regarding Neumann boundary conditions, one can use the domain transformation to compute the outer normal of the transformed geometry, but it\u2019s unclear how to strongly impose Neumann boundary conditions, as it\u2019s not clear for PINNs in general.\nExcept for the last example, which we will discuss next, the diffeomorphism is known a priori. Therefore the method in such case simply looks like a change in variable with a known function. How can you apply this method on a domain which is not equipped with such a transformation?\nIn our work, we assume that the diffeomorphism is given, and we assume that the domain geometry is defined by the mapping of the reference domain via the diffeomorphism. If, vice versa, a geometry is given, e.g., by a mesh, the diffeomorphism might be learned, but it\u2019s definitely an open question how this would be achieved effectively.\nThe last example is very mysterious to me. I actually do not understand what the method is supposed to achieve by learning simultaneously to impose the PDE constraint and the geometric transformation. Do we know what target geometry the network should converge to ? Besides, the network that learns the transformation is not a diffeomorphism, so there is no guarantee that the optimization problem finds a correct solution.\nIn the last example, we tried to make the mapping more general by using an NN as transformation (targeting the concern of the limitation that the diffeomorphism has to be known a priori, as in your previous comment).\nIt was intended as an explorative example to show what could be done incorporating a parametrized transformation.\nWe believe that we were able to demonstrate that this idea results in a geometrically very flexible PDE solver, e.g., in contrast to classical FEM solvers that would require re-meshing.\nHowever, as you mentioned (and we stated clearly), the network has no guarantee to be diffeomorph and this stretches the framework we outlined beforehand beyond its assumptions.\nWe agree that our objective function - not including an explicit target - is somewhat arbitrary and that we are not able to state analytically what the network should converge to. Experience with PINNs suggests that a convex shape is easiest to optimize for w.r.t. weak boundary conditions, and this is what our example actually results in.\nAs this example opens too many concerns, we conclude in removing it completely upon further investigation.\nThe authors do not compare their method with any existing work. There is no literature review. As a result, we do not really understand why these problems cannot be tackled with existing methods. Why do they fail ?\nWe recognize that our examples need more comparisons with similar problems from literature.\u2028\u2028 Unfortunately, to the best of our knowledge, we are not aware of previous works that applied PINNs to manifolds, which severely limits the possibilities of a comparison as well as a literature review.\nThe authors do not provide any numerical results for their methods, and even the qualitative results do not include the ground truth solutions. It is therefore impossible to judge the effectiveness of the method.\n/\nWhat does the following sentence mean? \"transformed PINN finds the exact length with an error of = 0.1 %\u201d\nThe first example is constructed in a way that the numerical solution can be compared to an analytical solution. We were not providing a plot for both solutions, because they are trivial and closely aligned s.t. a plot didn\u2019t make sense. Instead, we stated that the numerical solution fits the analytical solution \u201cwith an error of 0.1%\u201d. This is what the sentence meant, sorry for the incomprehensible expression.\nWe acknowledge that our work is lacking analytical results for example 2 and a comparative study for example 3.\nWhat is the difference between L, L_x and L_y, and concretely for each example ?\nThe subscript indicates the derivation variable of the differential operator. As other reviewers addressed the same issue, we have to make this more explicit along with equations (4)-(5).\u2028\u2028 The examples are split into manifold and transformation cases, where manifold corresponds to L_y and transformation to L_x. We will provide more clarity here."}, {"Heading": "Acknowledge", "Subheading": "Official CommentbyReviewer 2kyY23 Nov 2023, 04:21Everyone", "Content": "Comment:\nThank you for your response, I will keep my score."}]}, {"Heading": "Official Review of Submission9410 by Reviewer VH6u", "Subheading": "Official ReviewbyReviewer VH6u30 Oct 2023, 21:36 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper employs physics-informed neural networks (PINNs) for addressing intricate or changing geometrical configurations. The primary technical innovation lies in the incorporation of a geometric transformation (diffeomorphism) of a reference domain to describe the computational domain.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe problem is well defined and the author proposes a clear formulation in solving the problem.\nWeaknesses:\nUnfortunately,  it appears that the problem tackled in the paper is somewhat incremental, and the proposed solution lacks a surprising or profound aspect. In the context of an ICLR paper, I'm seeking a novel problem that has not previously been successfully addressed, made attainable through this approach, or a novel method to solve a well-established problem that has been extensively explored. Unfortunately, neither of these elements seems to be present in the paper.\nFurthermore, the examples provided mainly consist of small-scale 2D toy examples. To comprehensively assess the efficacy of this approach, it would be necessary for the authors to set up larger-scale problems that are well-documented in CFD/JCP/CMAME papers.\nQuestions:\nHow does this work compare with Bonev+ ICML 2023? These authors propose a neural PDE approach using spherical coordinate. Your paper seems to be more general. Can you reproduce some of the examples in their paper so we can have an apple to apple comparison?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 03:13Everyone", "Content": "Comment:\nThank you for your review! Let us address your comments as follows.\nUnfortunately, it appears that the problem tackled in the paper is somewhat incremental, and the proposed solution lacks a surprising or profound aspect. In the context of an ICLR paper, I'm seeking a novel problem that has not previously been successfully addressed yet, made attainable through this approach, or a novel method to solve a well-established problem that has been extensively explored. Unfortunately, neither of these elements seems to be present in the paper.\nThank you for your valuable feedback. We agree that one can consider the problem tackled somewhat incremental, and that our work does not propose a novel method to solve a well-studied problem.\nWe tried to introduce the approach of including a diffeomorphism within PINNs as a (novel) general concept, and we believe it could benefit from a common introduction. Furthermore, we consider the application of PINNs to manifolds as a problem that has not been successfully addressed, which is made attainable through this approach.\nA surprising aspect of our approach is that - in the transformation case - a latent representation of the PDE solution on the reference domain arises. This is likely to improve generalization capabilities for parametrized geometries, but we recognize that we were not able to demonstrate this effectively on a well-studied problem.\nFurthermore, the examples provided mainly consist of small-scale 2D toy examples. To comprehensively assess the efficacy of this approach, it would be necessary for the authors to set up larger-scale problems that are well-documented in CFD/JCP/CMAME papers.\nAs mentioned above, we tried to introduce the approach as a general concept and, therefore, we set up easily understandable examples that demonstrate the efficacy of the approach. We tried to choose examples with a clear setup and analytical solutions, accompanied by a transparent and manageable implementation.\nHowever, we accept that there is a request for applying our method to larger-scale, well-documented problems that would comprehensively assess the efficacy for non-toy examples.\nHow does this work compare with Bonev+ ICML 2023? These authors propose a neural PDE approach using spherical coordinate. Your paper seems to be more general. Can you reproduce some of the examples in their paper so we can have an apple to apple comparison?\nBonev+2023 propose Spherical Fourier Neural Operators (Spherical FNOs) that put the concept of FNOs efficiently onto spheres. As they use spherical harmonics for the Fourier transform, their approach is limited to spheres by definition.\u2028 Our approach is more general in the sense that we can model arbitrary manifolds. We think an apple-to-apple comparison doesn\u2019t make sense, because it would be restricted to the case of spheres where the SFNOs will clearly outperform all aspects of our method (accuracy and speed) as it is explicitly tailored to this case."}]}, {"Heading": "Official Review of Submission9410 by Reviewer UpPb", "Subheading": "Official ReviewbyReviewer UpPb29 Oct 2023, 06:47 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nIn this paper, it is argued that the existing approaches to physics-informed neural networks are not apt for complex and transforming geometries. To this end, the paper presents an approach to introduce geometric transformation within the physics-informed neural network design. Concretely, it enforces the Dirichlet boundary condition using distance function to account for complex geometries. Experimental results on four different examples are shown to demonstrate the suitability of the method.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nIt is a well-written paper.\nThe use of Dirichlet boundary conditions is promising.\nAn initial approach to explore a new direction for more promising neural network design.\nWeaknesses:\nSome of the technical notations are not fully exposed and detailed.\nExperiments are limited on toy-example and missing on the manifolds which are widely used in science and engineering application.\nThe paper misses to highlight the limitations of the proposed approach.\nKindly refer to the Questions section for more comments.\nQuestions:\nDomain and Transformation\nIt\u2019s better to include the dimension of the variables on the side of the Eq(1).\n2.2.1 Manifold: $m < n$\n$\\mathcal{L}_x$ and $\\mathcal{L}_y$ need more explanation. The subscripts have not been explained. Diagram conveys that one is in the reference domain and other is in the computational domain yet it's better to write near the equation (4)-(5) and following equation.\n3.1 Exact boundary condition with output transform\nKindly help me understand the approximation of $\\hat{u}$, given that the inverse must hold and the proposed approximation is not linear.\n4.4 Shape Optimization with Laplace Operator\nI am not entirely convinced with the imposed boundary condition. What could be considered a weak boundary condition is not fully exposed in the paper. Furthermore, I request the authors to perform some experiments and analysis of the proposed theory on negative curvature surfaces with the introduced local approach. Also, the use of Laplace-Beltrami operator for shapes.\nIn addition to the above, experiment on Low-Dimensional manifolds is simple and not convincing to me for real application. I request the authors to provide some analysis and results on popular manifolds such as low-dimensional SPD, Grassmannian manifolds, etc.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 03:06Everyone", "Content": "Comment:\nThank you for your review. We would like to answer your comments and questions as follows.\n$\\mathcal{L}_x$\nand\n$\\mathcal{L}_y$\nneed more explanation. The subscripts have not been explained. Diagram conveys that one is in the reference domain and other is in the computational domain yet it's better to write near the equation (4)-(5) and following equation.\nThanks for your feedback, other reviewers addressed the same issue. We only stated \u201c$\\mathcal{L} = \\mathcal{L}_y$ with respect to global coordinates\u201d and assumed that it\u2019s clear that the subscript indicates the derivation variable. Obviously, this it is not the case and the subscript needs more explanation.\nExact boundary condition with output transform: Kindly help me understand the approximation of $\\hat u$, given that the inverse must hold and the proposed approximation is not linear.\nAs the smooth distance function $b$ is zero at the boundary, $N(y) u(y)$ is zero on the boundary, and, therefore, $\\hat u(y) = g(y)$ satisfies the Dirichlet boundary values $g(y)$ at the boundary. \u2028\u2028Unfortunately, we do not understand what you are referring to with \u2018inverse must hold\u2019, and why the approximation should to be \u2018linear\u2019.\nI am not entirely convinced with the imposed boundary condition. What could be considered a weak boundary condition is not fully exposed in the paper.\nA weak boundary condition (in the context of PINNs) means imposing the boundary condition by adding a penalizing loss term, as outlined in (8). You noted correctly that we missed introducing this wording in the context of (8) and it should be added.\nExperiments are limited on toy-example and missing on the manifolds which are widely used in science and engineering application.\nWe tried to demonstrate our method on minimal working examples to make the setup and implementation clearly understandable. It\u2019s unfortunate if you consider them as too limited, and we will take this feedback into account.\nThe paper misses to highlight the limitations of the proposed approach.\nUnfortunately, that is true, we will add a paragraph on this.\nFurthermore, I request the authors to perform some experiments and analysis of the proposed theory on negative curvature surfaces with the introduced local approach. Also, the use of Laplace-Beltrami operator for shapes.\nWe do not see why negative curvature should have any relevant impact on our method, the proposed method also works with negative curvature surfaces. \u2028Also, our second example demonstrates a Poisson problem on a part of a sphere, which - as we formulate the derivatives in local coordinates - corresponds to a Laplace-Beltrami on the manifold. Our apologies that we didn\u2019t point this out explicitly.\nIn addition to the above, experiment on Low-Dimensional manifolds is simple and not convincing to me for real application. I request the authors to provide some analysis and results on popular manifolds such as low-dimensional SPD, Grassmannian manifolds, etc.\nWhich PDEs are commonly formulated on low-dimensional SPD or Grassmannian manifolds?"}]}, {"Heading": "Official Review of Submission9410 by Reviewer 2tfv", "Subheading": "Official ReviewbyReviewer 2tfv28 Oct 2023, 00:00 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper intends to improve the performance of PINN on domains of complex geometries. The method is to  use smooth transformations to transform a complex geometry to less complex one which is a called reference domain. If the transformations are differentiable, the training of modified PINNs is the same as training vanilla PINNs.\nSoundness:\n2 fair\nPresentation:\n1 poor\nContribution:\n2 fair\nStrengths:\nOriginality:\nThe paper implements diffeomorphisms to the problem of PINN on complex geometries.\nQuality:\nThe paper explores the proposed methods on some typical examples to demonstrate the effectiveness of the method.\nClarity:\nThe idea is conveyed directly and straightforward.\nSignificance:\nCombining diffeomorphism with training of neural network is somewhat interesting and natural, due to the differentiability of transformations.\nWeaknesses:\nOne of the major weakness is that the paper does not include experiments of comparison between modified PINN and vanilla PINN. In order to show the effectiveness of the proposed method, the author should also test the performance of PINN on all the problems in section 4.\nQuestions:\nIf the original problems $L(u)=f$ in $\\Omega$ is transformed to $L_x(u \\circ \\phi) = f$ on reference domain $\\Omega_{ref}$, then $L_x$ should not equal $L$. The calculation of $L_x$ should use chain rule. In your paper, this part is hardly touched. How did you actually implement your method in experiments?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 02:54Everyone", "Content": "Comment:\nThank you for your review. We would like to address the following comments.\nOne of the major weakness is that the paper does not include experiments of comparison between modified PINN and vanilla PINN. In order to show the effectiveness of the proposed method, the author should also test the performance of PINN on all the problems in section 4.\nUnfortunately, we do not understand how vanilla PINNs should be applied to the case of manifolds, as directional derivatives have to be computed. \u2028It might be possible to elaborate the differential operators (e.g., Laplace-Beltrami) explicitly and compute those in a \u201cvanilla\u201d way, but this is what our approach implicitly does, formulated in local coordinates, putting all the work into the automatic differentiation.\nIn the case of equi-dimensional transformations, vanilla PINNs could actually be applied and would lead to a well-studied reference solution. Thank you for this suggestion.\nIf the original problems $L(u)=f$ in $\\Omega$ is transformed to $L_x(u \\circ \\phi) = f$ on reference domain $\\Omega$ ref, then $L_x$ should not equal $L$. The calculation of $L_x$ should use chain rule. In your paper, this part is hardly touched. How did you actually implement your method in experiments?\nIn the manifold case, chain rule applies and is carried out by the automatic differentiation framework.\nIn the transformation case, we explicitly neglect the chain rule. It\u2019s somewhat arbitrary (and we are sorry if we didn\u2019t make this sufficiently clear), but this idea leads to general transformed domains.\nRegarding the actual implementation: We provide our full source code as supplementary material and refer to it for all implementation details."}, {"Heading": "Official Comment by Reviewer 2tfv", "Subheading": "Official CommentbyReviewer 2tfv23 Nov 2023, 03:34Everyone", "Content": "Comment:\nThanks for clarifying. I agree that this paper has novelty in solving PDE problems on manifold. However, regarding geometry transformation, here's my follow-up question:\nI'm still confused with eq. (6)-(7). Given $\\mathcal{L}(u_{ref})=f$, how can $u=u_{ref}\\circ \\phi^{-1}$ satisfy $\\mathcal{L}(u)=f$, as you mentioned above eq. (6)-(7) that $\\mathcal{L}=\\mathcal{L_y}$? In my understanding, generally $u$ satisfies $\\mathcal{L_y}(u)=f$ and $\\mathcal{L}\\neq \\mathcal{L_y}$.\nI did look at the code, but due to this fundamental question I didn't follow."}, {"Heading": "Official Comment by Authors", "Subheading": "Official CommentbyAuthors23 Nov 2023, 03:50Everyone", "Content": "Comment:\nIn (6)-(7) we only substitute $u = u_{ref} \\circ \\phi^{-1}$ into (2)-(3), and take $\\mathcal{L} = \\mathcal{L}_y$ as it is in (2)-(3) (global coordinate $y \\in \\Omega$). The reformulation is essentially trivial, but it is quite interesting because $\\phi$ now governs the shape of domain $\\Omega$."}]}]}, "eepoE7iLpL": {"paper_info": {"Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Keywords": "Neural Set Function, Hierarchical Structure, Invariance, Subset Selection", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Abstract": "Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an invariant sufficient statistic of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific superset from which the subset originated. Motivated by these insights, we propose a simple yet effective information aggregation module designed to merge the representations of subsets and supersets from a permutation invariance perspective. Comprehensive empirical evaluations across diverse tasks and datasets validate the enhanced efficacy of our approach over conventional methods, underscoring the practicality and potency of our proposed strategies in real-world contexts.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "Supplementary Material": "zip", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Primary Area": "representation learning for computer vision, audio, language, and other modalities", "Submission Number": "9406", "PDF Url": "https://openreview.net/pdf?id=eepoE7iLpL"}, "review_info": [{"Heading": "Paper Decision", "Subheading": "DecisionbyProgram Chairs16 Jan 2024, 06:53 (modified: 16 Feb 2024, 15:43)EveryoneRevisions", "Content": "Decision:\nAccept (poster)"}, {"Heading": "Meta Review of Submission9406 by Area Chair 7Zun", "Subheading": "Meta ReviewbyArea Chair 7Zun18 Dec 2023, 03:37 (modified: 16 Feb 2024, 15:31)EveryoneRevisions", "Content": "Metareview:\nThis paper proposes a method for \"neural subset selection\" based on deep sets. The paper received three reviews with borderline scores. Far the most comprehensive review came from PoVi, who found the work to be easy to follow, appreciated the discussion of the previous literature and noted the comprehensiveness of the experiments. The authors took time to provide extensive responses to the reviewer complaints but the reviewers did not take sufficient time to acknowledge these responses. In this case, and given the satisfaction expressed by the few reviewers who did reply with the updated results, I tend to give the authors the benefit of the doubt and recommend acceptance.\nJustification For Why Not Higher Score:\nToo many weaknesses.\nJustification For Why Not Lower Score:\nReviewers all see strengths."}, {"Heading": "Summary of Review and Response (December 1st, UTC-12)", "Subheading": "Official CommentbyAuthors28 Nov 2023, 11:58 (modified: 15 Mar 2024, 00:26)EveryoneRevisions", "Content": "Comment:\nWe express our sincere gratitude to the Area Chairs and Reviewers for their dedicated time and valuable feedback. Below is a concise summary of the review and our responses for ease of reference.\nReviewer Acknowledgments\n:\nOur method, INSET, has been recognized for its\nstrong empirical performance\nand\nrigorous theoretical support\n. Key highlights include:\nClear Motivation with Theoretical Support:\nOur model is inspired by a\ntheoretical result\n[miuk], and have theoretically demonstrated the significance to include information from supersets to achieve better performance [PoVi, vcJR].\nStrong Empirical Performance:\nINSET achieves\nstate-of-the-art\nperformance [miuk], supported by empirical evidence [PoVi].\nQuality of Writing:\nThe manuscript is praised for its clarity and simplicity, making it\naccessible for newcomers\nto the field [vcJR, miuk]. The presentation\neffectively conveys high-level concepts\n[PoVi].\nAddressing Weaknesses\n:\nThe reviewers raised concerns regarding ablation studies [vcJR, miuk], dataset distribution [vcJR], and mathematical notations in the optimal subset oracle [PoVi, miuk]. Our responses include:\nAblation Studies\n: We clarified in this\ndialog\nthat INSET introduces\nno new hyperparameters\n. Additional experiments demonstrate that even with variations in existing hyperparameters, INSET\nconsistently outperforms\nbaselines by\na large margin\n. This response was\nacknowledged positively\nby Reviewer miuk\n.\nMathematical Notations:\nFollowing suggestions from Reviewers PoVi and miuk, we corrected minor typos and enriched the appendix with additional mathematical background. These revisions received\npositive feedback\nfrom Reviewer miuk\n.\nDataset Distribution:\nWe clarified in the\ndialog\nthat our approach\nencompasses three tasks across six datasets in different modalities\nin the main body of our work. The appendix have also included\nfive additional datasets\n. We have utilized a\nmuch wider variety of datasets\nthan those adopted by our baseline comparisons.\nRevision Overview\n:\nTo enhance clarity, we have made refinements to the description of $Y$ in the Introduction, as well as the definitions of $V$ and $S$ in Sections 3.1 and 3.2.\nThe mathematical description of the optimization objective and inference process has been detailed in Appendix D.2.\nAppendix F.2 has been updated with additional empirical studies on computation costs. Further, ablation studies have been included in Appendix F.4, and results from a broader range of datasets are now presented in Appendix F.5.\"\nWe are thankful for the constructive feedback received, and we believe that the concerns raised by the reviewers have been thoroughly addressed in our responses and revisions."}, {"Heading": "Official Review of Submission9406 by Reviewer PoVi", "Subheading": "Official ReviewbyReviewer PoVi21 Nov 2023, 01:53Everyone", "Content": "Summary:\nThe authors propose an optimal subset selection method based on neural networks, which is designed to learn a permutation invariant representation of both the subset of interest $S$ and the ground superset $V$. The authors highlight that prior works for neural subset selection (e.g., DeepSet) do not account for the superset $V$, and both theoretically and empirically demonstrate that jointly modeling the interactions between $S$ and $V$ leads to improved performance.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n3 good\nStrengths:\nThe writing is generally easy to follow, and the paper includes a sufficiently comprehensive discussion of relevant prior works. Experimental results are presented well.\nThe proposed method achieves strong empirical performance in terms of mean Jaccard coefficient (often with a fairly large gap) when compared against several optimal subset selection baselines (e.g., DeepSet, EquiVSet).\nWeaknesses:\nThe presentation of some of the mathematical details needs improvement. In particular, it seems that some of the notations are overloaded (i.e., the same notation is used with different interpretations) or not clearly defined. For example, the notation $S$ appears as a\nsubset\nof the ground set $V$ in the Introduction, but in Section 3.1 (Background), the notation $S$ appears as an\nelement\nof $V$ that takes a matrix form. The relationship between elements $x_i \\in \\mathcal{X}$ and $S_i$ is not clearly defined either. On another note, it is not entirely clear to me what the function value $Y \\in \\mathcal{Y}$ is really referring to, which also appears without an explicit discussion of its meaning in the Introduction as part of the variational distribution $q(Y|S,V)$. Is $Y \\in \\mathcal{Y}$ supposed to be the utility function value (which was also introduced with the notation $U = F_{\\theta}(S,V)$ in the Introduction)? The confusion arising from notational ambiguity makes the paper less readable.\nQuestions:\nCan the authors clearly define what $Y$ is? The footnote mentions that $Y_i$ is the \"probability of element $i$ being selected\", but this description is ambiguous.\nIt looks like learning the neural network approximation in Eq. (4) is done via variational inference as in Ou et al. (2022). As I am not familiar with the cited work, it is unclear to me how $q(Y|S,V)$ is serves as an approximation for the subset likelihood $p(S|V)$ when the former is a distribution over $Y$ and the latter is a distribution over $S$. Can the authors provide clarifications on this?\nHow is the neural network construction in Eq. (4) explicitly related to $p_{\\theta}(S,V)$ (or $F_{\\theta}(S,V)$)?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Answers to Reviewer PoVi", "Subheading": "Official CommentbyAuthors21 Nov 2023, 11:46 (modified: 22 Nov 2023, 22:00)EveryoneRevisions", "Content": "Comment:\nWe greatly appreciate the time and effort you have invested! In response to your concerns, we have provided clarifications here. We will also incorporate these clarifications into our revised version to enhance clarity.\nComments 1: Relationship Between $x_i, S_i,$ and $V.$\nANSWER:\nWe regard $V$ as a set composed of $n$ elements, denoted as $x_i$, i.e., $V=\\{x_1, x_2,..., x_n\\}$. In order to facilitate the proposition of Property 1, we describe $V$ as a collection of several disjoint subsets, specifically $V = \\{S_1, \\dots, S_m\\}$, where $S_i \\in \\mathbb{R}^{n_i \\times d}$. Here, $n_i$ represents the size of subset $S_i \\subset V$, that is, $S_i = \\{x_{1_i}, x_{2_i},..., x_{n_i}\\}.$\nComments 2: The definition of $Y.$\nANSWER:\nThe generality of our Theorem 3.1 allows it to be applied to both U=F(Y|S,V) and P(Y|S,V) for different tasks. Specifically, when considering the task of Neural Subset Selection in Optimal Subset (OS) oracles, which involves learning P(Y|S,V), we define Y as a $|V|$ independent Bernoulli distribution, which is parameterized by $Y \\in [0,1]^{|V|}$, representing the odds or probabilities of selecting element $x_i \\in V$ in a output subset $S$.\nComments 3: Why can $q(Y|S,V)$ serve as a variational approximation to $P(S|V)?$\nANSWER:\nAs discussed in the previous answer, $Y \\in [0,1]^{|V|}$. In practice, $S$ is represented as a binary vector (mask), denoted as $S := \\{0,1\\}^{|V|}$, where the $i$-th element is equal to $1$ if $i \\in S$ and $0$ otherwise. Therefore, it is natural to use $q(Y|S,V)$ to represent the variational distribution of $P(S|V)$.\nComments 4: How is the neural network construction in Eq. (4) explicitly related to $p_\\theta(S,V)$ or $F_\\theta(S,V).$\nANSWER:\nOnce neural networks are trained, their outputs become fixed for a given input, such as (S,V). Thus, Eq. (4) represents the explicit structure used to construct models for learning the deterministic function $\\theta(S,V)$ (to differentiate it from the utility function U=F(S,V)). Using this function, we can construct the conditional distribution $q(Y|S,V)$ according to Theorem 3.5. Specifically, we employ the Mean-Field Variational Inference (MFVI) method introduced by [1] (Section 3.2) to approximate the distribution $q(Y|S,V)$, referred to as $\\psi$ in [1].\nTo prevent overwhelming readers with an abundance of notations and equations, we have deliberately omitted the detailed construction of q(Y|S,V) and the derivation of variational approximation in our paper. This decision was motivated by two factors. Firstly, our theorem and Eq. 4 offer a general framework for modeling the relationship between $Y$ and $(S,V)$, instead of focusing on the neural subset selection tasks. Secondly, in order to ensure clarity of our motivation, we have provided a high-level description of these concepts in the Introduction section. For readers interested in the details of these concepts, we strongly recommend referring to [1] (Section 3) for a more comprehensive understanding. For the implementation details of q(Y|S,V) and $\\theta(S,V)$, we suggest consulting our accompanying code located at (./model/modules.py).\nThanks for your time and suggestions again. We would appreciate knowing if you have any additional feedback or suggestions.\n[1] Ou Z, Xu T, Su Q, et al., \"Learning Set Functions Under the Optimal Subset Oracle via Equivariant Variational Inference.\"NeurIPS, 2022."}, {"Heading": "Revision of Manuscript Incorporating Your Suggestions", "Subheading": "Official CommentbyAuthors22 Nov 2023, 22:27Everyone", "Content": "Comment:\nDear Reviewer PoVi,\nWe sincerely appreciate your reviews and valuable suggestions. Taking into account your feedback, we have made refinements to the footnote in the Introduction Section and enhanced the description of $V$ and $S$ in Section 3.1. These revisions, highlighted in purple, will significantly enhance the clarity of our paper. Thank you once again for your time and contribution.\nBest regards,\nThe Authors."}, {"Heading": "Follow-up clarification questions and comments", "Subheading": "Official CommentbyReviewer PoVi30 Nov 2023, 11:27 (modified: 15 Mar 2024, 00:26)EveryoneRevisions", "Content": "Comment:\nThanks for a quick response and for letting me know of the revisions. Here are additional clarification questions and comments:\nRegarding Comment 1: Wouldn't it be more natural to write $V = S_1 \\cup \\cdots \\cup S_m$ with $S_i \\cap S_j = \\emptyset$ when $i \\neq j$, which I believe is indeed the form used in Section 3.2?\nRegarding Comments 2 and 3: I think the description that $Y$ is a\ndistribution\nis misleading. Clearly, $Y$ itself is not a distribution since it need not be the case that $\\sum_{i} Y_i = 1$, and by the definitions given here, $P(S|V)$ and $q(Y|S,V)$ are distributions over different objects. Rather, shouldn't it be the case that the probabilities of each element being selected in the optimal subset are the outputs from the variational distribution $q$? In this case, it seems more appropriate to describe it as $q(S|V)$? Please let me know if I am misunderstanding something here. Meanwhile, since $Y$ is used throughout the main text, I think it should be very clearly defined as part of the main text before it is used, instead of appearing in a footnote (if appropriate, discussed along with concrete examples that readers can immediately understand)."}, {"Heading": "Thanks for your reply and suggestions!", "Subheading": "Official CommentbyAuthors30 Nov 2023, 23:18 (modified: 15 Mar 2024, 00:26)EveryoneRevisions", "Content": "Comment:\nThank you for your valuable suggestions! We are now revising our manuscript based on your suggestions. To address your questions, we would like to provide the following clarifications:\nFirstly, we want to clarify that $Y \\in [0, 1]^{|V|}$ represents a set of $|V|$\nindependent Bernoulli distributions\nrather than a categorical distribution with $V$ classes. Hence, it is not required for the elements of $Y$ to satisfy the constraint $\\sum_i Y_i = 1$. Additionally, we define the expression of $q(Y|S,V)$ as follows:\n$$\nq(Y|S,V) = \\prod_{i \\in S}Y_i \\prod_{i \\not\\in S}(1-Y_i), Y\\in[0,1]^{|V|}.\n$$\nNext, we would like to explain why $q(Y|S,V)$ can approximate $P(S|V)$. Consider $Y \\in [0, 1]^{|V|}$ and $S = \\{0, 1\\}^{|V|}$. In this case, $Y$ can be viewed as a stochastic version of $S$ since $Y$ can also be generated as $Y = \\{0, 1\\}^{|V|}$ while still satisfying the constraint $Y \\in [0, 1]^{|V|}$.\nTo facilitate comprehension, let us consider an illustrative scenario. Suppose we have a ground set $V = \\{x_1, x_2, x_3\\}$, and the optimal subset $S^*$ is $\\{x_1, x_2\\}$, which can be represented as $[1, 1, 0]$. Specifically, we define $P(S^*|V) = 1$, indicating that $S^*$ is the correct subset, while for any $S \\neq S^*$, we have $P(S|V) = 0$.\nNow, let's examine the case when $Y = [1, 1, 0].$ In this situation, we can calculate that $q(Y|S^,V) = 1.$ This implies that $q(Y|S^,V)$ accurately represents the probability of observing $S^*$ given $V$, and it correctly assigns a high probability to the optimal subset.\nWe hope these clarifications help provide a better understanding of our framework. Once again, we appreciate your constructive suggestions and look forward to further discussion."}, {"Heading": "The manuscript has been updated", "Subheading": "Official CommentbyAuthors01 Dec 2023, 01:30 (modified: 15 Mar 2024, 00:26)EveryoneRevisions", "Content": "Comment:\nDear Reviewer PoVi,\nWe sincerely appreciate your valuable suggestions for improving our paper. We have refined the descriptions of $V$ and $S$ in Section 3.2 based on your suggestions. Moreover, we have defined $Y$ in the main text instead of the footnote, where we have also included a reference to Appendix D.2. In this appendix, readers can find further elaboration on the relationship between $q(Y|S,V)$ and $P(S,V), along with the example mentioned in our previous response.\nWe sincerely thank you once again for your time and valuable contribution. Should you have any additional suggestions or questions, please do not hesitate to let us know.\nBest regards,\nThe Authors"}, {"Heading": "Discussion Stage 1 is Near the End: Do You Have Any Further Suggestions?", "Subheading": "Official CommentbyAuthors01 Dec 2023, 21:30 (modified: 15 Mar 2024, 00:26)EveryoneRevisions", "Content": "Comment:\nDear Reviewer PoVi,\nThank you for your constructive suggestions and insightful comments! As the discussion deadline is approaching in 10 hours, we would like to inquire if you have any further suggestions for improving our manuscript. We would greatly value your input and appreciate your guidance.\nThank you for your time and consideration. We eagerly await your reply.\nSincerely,\nThe Authors"}]}, {"Heading": "Official Review of Submission9406 by Reviewer vcJR", "Subheading": "Official ReviewbyReviewer vcJR11 Nov 2023, 17:24Everyone", "Content": "Summary:\nThe paper tackles neural subset selection. In particular, they tackle the issue that current methods do not consider the properties of the superset while constructing subsets. Their theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an invariant sufficient statistic of the superset into the subset of interest for effective learning.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n3 good\nStrengths:\nThe paper is clearly written.\nThe related work covers enough ground for a new researcher to understand a high level idea of this field.\nThe experiments include multiple baselines.\nWeaknesses:\nLack of ablation studies.\nThe proposed method is not evaluated on a wide distribution of datasets.\nWill similar findings hold if the dataset contains imbalance? If so, what degree of imbalance do the guarantees still hold?\nQuestions:\nBaselines do not consider the information from superset, but these baselines be improved by adding the invariant sufficient statistic of the superset?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Answers to Reviewer vcJR (part 1)", "Subheading": "Official CommentbyAuthors14 Nov 2023, 01:06 (modified: 19 Nov 2023, 03:54)EveryoneRevisions", "Content": "Comment:\nWe greatly appreciate the time and effort you have invested. In response to your concerns, we have provided detailed clarifications and additional experimental results. For your convenience, these results are presented in tabular format. We will incorporate these results, along with details and plots into the appendix of the revised version.\nComments 1: Lack of ablation studies\nANSWER\n: Thank you for highlighting the absence of ablation studies in our work. Indeed, our method, INSET,\ndoes not introduce any new hyperparameters\nto the EquiVSet [1] framework. We use the\nexact same\nhyperparameters as EquiVSet in all of our experiments, ensuring a\nfair comparison\n. Meanwhile, INSET can significantly outperform baseline models across various datasets and tasks, demonstrating its\nsubstantial efficacy\n.\nTo further verify the robustness of INSET, we have now conducted ablation studies focusing on the Monte-Carlo (MC) sample numbers for each input pair ${(V_i, S_i^*)}$. In the context of neural subset selection tasks, our primary aim is to train the model  $\\theta$ to predict the optimal subset $S^*$  from a given ground set $V$.  During training, we sample m subsets from $V$ to optimize our model parameters  $\u03b8$, thereby maximizing the conditional probability distribution $p_\\theta (S^* | V)$ among of all pairs of $(S,V)$ for for a given V. In our main experiments, we adhere to EquiVSet's protocol by setting the sample number $m$ to 5 across all the tasks. Even with varying the value of $m$, the results\nconsistently\ndemonstrate that INSET\nsignificantly outperforms\nEquiVSet. Please note that the performance of EquiVSet is reported by\nselecting the best results\nachieved after tuning the value of $m$.\nEquiVSet\nm=1\nm=2\nm=5\nm=7\nm=8\nm=10\nToys\n0.704$\\pm$0.004\n0.752$\\pm$0.006\n0.753$\\pm$0.005\n0.769$\\pm$0.005\n0.768$\\pm$0.003\n0.767$\\pm$0.003\n0.771$\\pm$0.004\nGear\n0.745$\\pm$0.013\n0.788$\\pm$0.015\n0.775$\\pm$0.020\n0.808$\\pm$0.012\n0.813$\\pm$0.010\n0.821$\\pm$0.015\n0.846$\\pm$0.011\nBath\n0.820$\\pm$0.005\n0.821$\\pm$0.010\n0.851$\\pm$0.008\n0.862$\\pm$0.005\n0.874$\\pm$0.006\n0.861$\\pm$0.005\n0.874$\\pm$0.003\nHealth\n72.0$\\pm$0.010\n0.749$\\pm$0.015\n0.763$\\pm$0.012\n0.812$\\pm$0.005\n0.824$\\pm$0.008\n0.808$\\pm$0.005\n0.811$\\pm$0.005\nComments 2: The proposed method is not evaluated on a wide distribution of datasets.\nANSWER\n: It is important to clarify that our experiments encompass three tasks: product recommendation, set anomaly detection, and compound selection, which involve the processing of tabular data, images, and 3D Cartesian coordinates. Specifically, we conduct extensive experiments on\nthese tasks using six datasets\n. Notably, for the product recommendation task, the datasets consist of 12 categories, effectively representing\n12 sub-datasets\n.\nMoreover, we have also conducted synthetic experiments in Appendix F.1 to assess INSET's capability to learn complex set functions.  Additionally, to provide further evidence of INSET's effectiveness, we have performed set anomaly detection tasks using the CIFAR-10 dataset. We are also incorporating additional filters for compound selection tasks for a wider distribution of datasets. For more information, please refer to Appendix F.3 and F.5 in the revised submission.\"\nRandom\nPGM\nDeepSet\nSet-Transformer\nEquiVSet\nINSET\nCIFAR-10\n0.193\n0.450$\\pm$0.020\n0.316$\\pm$0.008\n0.654$\\pm$0.023\n0.603$\\pm$0.012\n0.742$\\pm$0.020\nPDBBind\n0.073\n0.350$\\pm$0.009\n0.323$\\pm$ 0.004\n0.355$\\pm$0.010\n0.357$\\pm$0.005\n0.371$\\pm$0.010\nBindingDB\n0.027\n0.176$\\pm$0.006\n0.165$\\pm$0.005\n0.183$\\pm$0.004\n0.188$\\pm$0.006\n0.198$\\pm$0.005\nThe latest results provide further evidence of INSET's superior performance compared to the baselines. Furthermore, it is worth mentioning that our experimental setup includes a\nsignificantly larger number\nof experiments compared to DeepSet (Sec. 4.3) [2] and PGM (Sec. 5.3) [3].\n[1] Ou Z, Xu T, Su Q, et al. \"Learning Set Functions Under the Optimal Subset Oracle via Equivariant Variational Inference.\"NeurIPS, 2022.\n[2] Zaheer M, Kottur S, Ravanbakhsh S, et al. \"Deep Sets.\" NeurIPS, 2017.\n[3] Tschiatschek S, Sahin A, Krause A. \"Differentiable submodular maximization.\" IJCAI, 2018."}, {"Heading": "Answers to Reviewer vcJR (part 2)", "Subheading": "Official CommentbyAuthors14 Nov 2023, 01:30 (modified: 19 Nov 2023, 03:55)EveryoneRevisions", "Content": "Comment:\nComments 3: Will similar findings hold if the dataset contains imbalance? What degree of imbalance do the guarantees still hold?\nANSWER\n: INSET is designed to significantly enhance the models' capacity to effectively learn $P(Y|S,V)$ or $F(S,V)$. According to Theorem 3.5, this enhancement\nholds true consistently\nwhen the tasks involve modeling the relationship between (S,V) and $Y$. To provide empirical evidence, we conduct additional experiments that demonstrate INSET's consistent superiority over the baselines, even in scenarios with imbalanced ground set sizes. Specifically, we train the model on the two-moons datasets (for detailed information, please refer to Appendix F.1) using fixed ground set sizes of 100, and evaluate its performance on various data sizes ranging from 200 to 1000.\n200\n400\n600\n800\n1000\nEquiVSet\n0.538 $\\pm$ 0.002\n0.513 $\\pm$ 0.003\n0.482 $\\pm$ 0.002\n0.473  $\\pm$ 0.005\n0.471 $\\pm$ 0.003\nINSET\n0.547 $\\pm$ 0.002\n0.518 $\\pm$ 0.005\n0.502 $\\pm$ 0.003\n0.486 $\\pm$ 0.002\n0.485 $\\pm$ 0.002\nThe results clearly show that INSET\nconsistently enhances\nthe performance of EquiVSet, regardless of any imbalances.\nComments 4: Can baselines be improved by adding the invariant sufficient statistic of the superset?\nANSWER:\nCertainly, Theorem 3.5 offers a comprehensive framework for modeling the relationship between $Y$ and $(S,V)$, which is also\napplicable to the baselines\n. However, integrating this invariant sufficient statistic directly into DeepSet and Set-Transformer presents challenges, as they do not explicitly learn a neural subset function $F(S,V)$. Our method, INSET, has employed DeepSet as its backbone. To demonstrate that Set-Transformer can also derive benefits from INSET, we utilize Set-Transformer as our backbone to showcase this.\nRandom\nSet-Transformer\nSet-Transformer + INSET\nToys\n0.083\n0.625 $\\pm$ 0.020\n0.769 $\\pm$ 0.010\nGear\n0.077\n0.647 $\\pm$ 0.006\n0.825 $\\pm$ 0.021\nCarseats\n0.066\n0.220 $\\pm$ 0.010\n0.230 $\\pm$ 0.031\nBath\n0.076\n0.716 $\\pm$ 0.005\n0.862 $\\pm$ 0.005\nHealth\n0.076\n0.690 $\\pm$ 0.010\n0.852 $\\pm$ 0.009\nDiaper\n0.084\n0.789 $\\pm$ 0.005\n0.896 $\\pm$ 0.005\nBedding\n0.079\n0.760 $\\pm$ 0.020\n0.885 $\\pm$ 0.013\nFeeding\n0.093\n0.753 $\\pm$ 0.006\n0.902 $\\pm$ 0.004\nBy employing Set-Transformer as our backbone, we enable it to explicitly learn the relationship between $Y$ and $(S,V)$. The empirical results clearly demonstrate a\nsignificant improvement\nin performance as a result.\nThank you for your time and thoughtful consideration. If you have any concerns or questions, please don't hesitate to reach out to us."}, {"Heading": "Summary of our response", "Subheading": "Official CommentbyAuthors15 Nov 2023, 22:09 (modified: 17 Nov 2023, 21:47)EveryoneRevisions", "Content": "Comment:\nDear Reviewer vcJR:\nThank you again for your valuable time and efforts in reviewing our manuscript. Since our previous response was a bit long, we provide a summary below:\nAblation studies:\nWe have clarified that our method INSET does not introduce new hyper-parameters. Additionally, we have included an ablation study focusing on the number of Monte-Carlo (MC) samples. Detailed responses to these points are available in our feedback to Comment 1.\nDatasets:\nWe have elaborated on the usage of our datasets, encompassing 3 tasks across 6 datasets in three different modalities. Besides, we have also presented more experiments in our feedback to Comment 2.\nQuestions:\nWe also provide new experiments to answer your thoughtful questions on the imbalance and baseline.\nOur method not only demonstrates an impressive empirical performance, with up to a 23% improvement over the best baselines, but it is also underpinned by rigorous theoretical analysis and a strong foundational concept. We are also grateful for your recognition of our work\u2019s Soundness, Presentation, and Contribution as being satisfactory.\nConsidering these aspects, we respectfully and kindly invite you to re-evaluate the rating of our submission. We eagerly anticipate any further feedback from you."}, {"Heading": "Update Appendix in Response to Your Concerns", "Subheading": "Official CommentbyAuthors19 Nov 2023, 10:39Everyone", "Content": "Comment:\nDear Reviewer vcJR,\nThanks for your time and consideration. We have revised the Appendix of our manuscript to address your concerns. Regarding your comments on ablation studies, we have conducted additional experiments, detailed in Appendix F.4, and in the first answer of our initial\nresponse\n. Concerning the distribution of datasets, we invite you to review Appendix F.3 and F.5, along with the second answer in our initial\nresponse\n. For your insightful questions, please refer to the second part of our initial\nresponse\n. A summary of our previous response can be found in the\nparagraph\n. We would appreciate knowing if you have any additional feedback or suggestions.\nSincerely,\nThe Authors"}, {"Heading": "Gentle Reminder of the Revision Deadline", "Subheading": "Official CommentbyAuthors22 Nov 2023, 09:48Everyone", "Content": "Comment:\nDear Reviewer vcJR,\nThank you once again for your time! We understand that you have a busy schedule, and we kindly remind you that the revision deadline is approaching. If you have any suggestions or feedback on how we can improve our manuscript, we would greatly appreciate your input. We eagerly await your response.\nSincerely,\nThe Authors"}, {"Heading": "Look forward to your feedback!", "Subheading": "Official CommentbyAuthors26 Nov 2023, 11:00 (modified: 15 Mar 2024, 00:26)EveryoneRevisions", "Content": "Comment:\nDear Reviewer vcJR,\nWe sincerely appreciate the time and effort you have dedicated to reviewing our work. Would you mind checking our response (a\nshortened summary\n, and the\ndetails\n)? If you have any further questions or concerns, we would be grateful if you could let us know. Moreover, if you find our response satisfactory, we kindly ask you to consider the possibility of improving your rating. Thank you very much for your valuable contribution.\nBest,\nThe Authors"}, {"Heading": "Less Than One Day Remaining for Discussion", "Subheading": "Official CommentbyAuthors01 Dec 2023, 10:12 (modified: 15 Mar 2024, 00:26)EveryoneRevisions", "Content": "Comment:\nDear Reviewer vcJR,\nAs the deadline for updating our manuscript is rapidly approaching, we would greatly appreciate your timely feedback on the revisions and clarifications we have provided. We are eager to incorporate any further suggestions you may have. If you find our responses and modifications satisfactory, we kindly request that you consider revising your rating to reflect these changes.\nThank you for your attention to our work, and we look forward to your response.\nBest regards,\nThe Authors."}]}, {"Heading": "Official Review of Submission9406 by Reviewer miuk", "Subheading": "Official ReviewbyReviewer miuk15 Oct 2023, 23:47 (modified: 14 Nov 2023, 21:16)EveryoneRevisions", "Content": "Summary:\nThis paper proposes a neural subset selection method based on deep sets. This model is inspired by a theoretical perspective to include information from supersets to achieve better performance. Experiments on common benchmarks show SOTA performance compared to several recent baselines.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe idea to include information from superset is simple and effective as shown by the experiment results\nTheoretical discussions are provided.\nWeaknesses:\nEquation 4 describes the neural network construction. However, I am unclear about the objective function to optimize the neural network. Also, after optimization, how do you use this neural network to select a subset?\nIn equation 4, how do you divide a superset into several subsets? There are an exponential number of combinations.\nWhat is the number of learnable parameters for each baseline method and the proposed method?\nQuestions:\nNone\nFlag For Ethics Review:\nNo ethics review needed.\nDetails Of Ethics Concerns:\nNone\nRating:\n6: marginally above the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Answers to Reviewer miuk (part 1)", "Subheading": "Official CommentbyAuthors14 Nov 2023, 03:49 (modified: 14 Nov 2023, 04:43)EveryoneRevisions", "Content": "Comment:\nWe greatly appreciate the time and effort you have invested. In response to your concerns, we have provided detailed clarifications.\nComments 1: The optimization objective and how to select an optimal subset during inference?\nANSWER:\nThank you for your insightful question, as the construction of an optimization objective and inference process are indeed key aspects of neural subset selection [1, 2, 3]. We aimed to balance detailed explanation with readability for a broad audience, which is why we initially provided a high-level overview in the\nIntroduction Section\n. To specifically address your concerns, we are now including more detailed descriptions of the optimization and inference processes.\nOur formulation of the optimization objective is based on the framework established in [1]. Specifically, the optimization objective is to address Equation 1 in our paper by adopting an implicit learning strategy grounded in probabilistic reasoning. This approach can be succinctly formulated as follows:\n$$ argmax_\\theta\\ \\mathbb{E}_{\\mathbb{P}(V, S)} [\\log p _\\theta (S^{*}| V)] $$\n$$s.t.  p _\\theta (S | V) \\propto  F _\\theta (S ; V), \\forall  S \\in 2^V, $$\nThe important step in addressing this problem involves constructing an appropriate set mass function $p_\\theta (S|V)$ that is monotonically increasing in relation to the utility function $F_\\theta (S;V)$. To achieve this, we can employ the Energy-Based Model (EBM):\n$$\np_\\theta (S|V) = \\frac{\\mathrm{exp}( F_\\theta (S; V))}{Z}, ; Z := \\sum\\nolimits_{S'\\subseteq V}  \\mathrm{exp}( F_\\theta (S'; V)),\n$$\nIn practice, we approximate the EBM by solving a variational approximation\n$$\n    \\phi^* =  argmin_{\\phi} D(q_\\phi(Y|S,V)) || p_\\theta (S|V)),\n$$\nDuring the training phase, we need an EquiNet, denoted as $Y = \\operatorname{EquiNet}(V;\\phi): 2^V \\rightarrow [0,1]^{|V|}.$ This network takes the ground set $V$ as input and outputs probabilities indicating the likelihood of each element $x \\in V$ being part of the optimal subset $S^*$. In the inference stage, EquiNet is employed to predict the optimal subset for a given ground set $V$, using a TopN rounding approach. For detailed information on the implementation and derivation of the aforementioned objective, please refer to [1].\n[1] Ou Z, Xu T, Su Q, et al. \"Learning Set Functions Under the Optimal Subset Oracle via Equivariant Variational Inference.\"NeurIPS, 2022.\n[2] Tschiatschek S, Sahin A, Krause A. \"Differentiable submodular maximization.\" IJCAI, 2018.\n[3] Zhang D W, Burghouts G J, Snoek C G M. \"Set prediction without imposing structure as conditional density estimation.\" ICLR, 2021."}, {"Heading": "Answers to Reviewer miuk (part 2)", "Subheading": "Official CommentbyAuthors14 Nov 2023, 04:31 (modified: 14 Nov 2023, 04:43)EveryoneRevisions", "Content": "Comment:\nComments 2: In equation 4, how do you divide a superset into several subsets?\nANSWER:\nTheorem 3.5 and Eq.4 are general frameworks to establish the relationship between $Y$ and $(S,V)$. This approach\ndoes not necessitate\ndividing a superset into multiple subsets; instead, it requires processing only once for a specific pair of $(S,V).$ In the context of neural subset selection with the optimal supervision (OS) oracle, addressing the variational approximation employs Monte-Carlo (MC) sampling. For a given $V_i,$ we only generate $m$ subsets during training, consistently setting this number to $5$ across various tasks. Consequently, this\neliminates the need for an exponential number of combinations\n. It is important to highlight that one of the foremost advantages of Neural Subset Selection in the context of OS Oracle is its ability to\nsignificantly reduce the computational burden\nassociated with processing an exponential number of $(S,V).$\nComments 3: What is the number of learnable parameters for baselines and the proposed method?\nANSWER:\nIn regard to the parameters, we have already done ablation studies in\nTable 3\nand provided a discussion in\nSection 4.4\nof our paper. To further demonstrate that the improvements achieved by our method are\nnot merely due to additional parameters\n, we present an additional table  here using the CeleA dataset. This table compares EquiVSet (v1) and EquiVSet (v2) \u2014 variants of EquiVSet where we have incorporated a Conv(32, 3, 2) layer and a Conv(64, 4, 2) layer into the EquiVSet backbone, respectively. Detailed descriptions of these backbones are available in Appendix E.2. Notably, despite having the largest number of parameters, EquiVSet (v2) is outperformed by INSET, indicating that\nour method's efficacy is not solely parameter-dependent\n.\nDeepSet\nSet-Transformer\nEquiVSet\nEquiVSet-v1\nEquiVSet-v2\nINSET\nParameter\n651181\n1288686\n1782680\n2045080\n3421592\n2162181\nMJC\n0.440$\\pm$0.006\n0.527$\\pm$0.008\n0.549$\\pm$0.005\n0.554$\\pm$0.007\n0.560$\\pm$0.005\n0.580$\\pm$0.012\nThank you in advance for dedicating your time and attention to our response. We are confident that the clarifications and additional information provided here comprehensively address your concerns. With this in mind, we respectfully and earnestly request that you re-evaluate our work, considering the explanations we have offered."}, {"Heading": "Need more clarification on Comments 2", "Subheading": "Official CommentbyReviewer miuk14 Nov 2023, 13:53Everyone", "Content": "Comment:\nThanks for your response! I have one more question regarding Comments 2. By stating \"we only generate m\n subsets during training\", do you mean that during each training iteration, you randomly select m subsets?"}, {"Heading": "Thank you for your prompt reply", "Subheading": "Official CommentbyAuthors14 Nov 2023, 20:32 (modified: 14 Nov 2023, 20:34)EveryoneRevisions", "Content": "Comment:\nThank you for your prompt response! You are correct that during each training iteration, we randomly select $m$ subsets for each ground set $V$. Increasing the value of $m$ leads to an improvement in performance. To ensure a\nfair comparison\n, we adhere to EquiVSet's protocol by setting the sample number $m$ to 5\nacross all tasks and datasets\n. Even when varying the value of $m$, the results\nconsistently\ndemonstrate that INSET\nsignificantly outperforms\nEquiVSet. In the following table, we report the performance of EquiVSet by selecting the best results achieved after tuning the value of $m$ within the range of 1 to 10.\nEquiVSet\nm=1\nm=2\nm=5\nm=7\nm=8\nm=10\nToys\n70.4$\\pm$0.004\n75.2$\\pm$0.006\n75.3$\\pm$0.005\n76.9$\\pm$0.005\n76.8$\\pm$0.003\n76.7$\\pm$0.003\n77.1$\\pm$0.004\nGear\n74.5$\\pm$0.013\n78.8$\\pm$0.015\n77.5$\\pm$0.020\n80.8$\\pm$0.012\n81.3$\\pm$0.010\n82.1$\\pm$0.015\n84.6$\\pm$0.011\nThank you for your time. If you have any additional questions, we would be delighted to discuss them further."}, {"Heading": "Response", "Subheading": "Official CommentbyReviewer miuk14 Nov 2023, 21:16Everyone", "Content": "Comment:\nI am satisfied with the clarification and increased my score to 6."}, {"Heading": "Thanks for raising the score!", "Subheading": "Official CommentbyAuthors14 Nov 2023, 21:31 (modified: 15 Nov 2023, 18:14)EveryoneRevisions", "Content": "Comment:\nHuge thanks for your super quick reply and for raising the score!"}, {"Heading": "Revised Manuscript Incorporating Your Suggestions.", "Subheading": "Official CommentbyAuthors22 Nov 2023, 22:37Everyone", "Content": "Comment:\nDear Reviewer miuk,\nWe would like to extend our heartfelt gratitude for your active engagement and valuable suggestions. Thanks to your insightful feedback, we have made some revisions to our manuscript.\nFirstly, we have incorporated the optimization objective and inference process into Appendix D.2, allowing for a more comprehensive understanding of our proposed approach. Additionally, we have included the extra experiments in Appendices F.2 and F.4, providing further supporting evidence for our findings. These revisions have been highlighted in purple for readers' convenience.\nWe greatly appreciate your continued support and acknowledgement of our response. Moreover, we are truly grateful for the time and consideration you have invested in reviewing our manuscript.\nSincerely,\nThe Authors"}]}]}, "hLT9cW4Afz": {"paper_info": {"Keywords": "High-dimensional causal inference, text analysis, matching, citation analysis, science of science", "TL;DR": "We propose a causal effect estimation method for high-dimensional text embeddings, and introduce a causal formulation for citations.", "Abstract": "Evaluating the significance of a paper is pivotal yet challenging for the scientific community. While the citation count is the most commonly used proxy for this purpose, they are widely criticized for failing to accurately reflect a paper\u2019s true impact. In this work, we propose a causal inference method, SYNMATCH, which adapts the traditional matching framework to high-dimensional text embeddings. Specifically, we encode each paper using the text embeddings by large language models, extract similar samples by cosine similarity, and synthesize a counterfactual sample by the weighted average of similar papers according to their similarity values. We apply the resulting metric, called CAUSALCITE, as a causal formulation of paper citations. We show its effectiveness on various criteria, such as high correlation with paper impact as reported by scientific experts on a previous dataset of 1K papers, and test-of-time awards for past papers, and sub-field stable behavior. We also provide a set of findings that can serve as suggested ways for future researchers to use our metric for a better understanding of a paper\u2019s quality.", "Supplementary Material": "zip", "Primary Area": "representation learning for computer vision, audio, language, and other modalities", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9405", "PDF Url": "https://openreview.net/pdf?id=hLT9cW4Afz"}, "review_info": [{"Heading": "Official Review of Submission9405 by Reviewer kS1d", "Subheading": "Official ReviewbyReviewer kS1d06 Nov 2023, 14:06 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nIn this paper, the author proposed a framework for understanding the citation of a paper by analyzing the citation graph from a causal perspective.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThe paper is well-written and easy to follow.\nThe problem setting is solid and definitely interesting, that is to understand the counterfactual effect of a paper not being published.\nWeaknesses:\nThe evaluation counts on the construction of the synthetic dataset, and the reliability of the counterfactual data is not measured or analyzed.\nThe assumption that the confounders are \" a paper's publication year, research area, research question, and storyline\" is limited. These can be part of the confounder and the storyline is a concept that is hard to measure.\nTo measure the impact of people's work, there might be a need to provide a more rigorous justification, especially when using the LLM for synthetic data generation.\nQuestions:\nSee weakness.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Public Comment by ZJin", "Subheading": "Public CommentbyZJin23 Nov 2023, 08:25Everyone", "Content": "Comment:\nThank you for your feedback that the paper is well written, easy to follow, and has a solid an interesting setting.\nThe evaluation counts on the construction of the synthetic dataset, and the reliability of the counterfactual data is not measured or analyzed.\nTo measure the impact of people's work, there might be a need to provide a more rigorous justification, especially when using the LLM for synthetic data generation.\nRegarding the two points raised about the synthetic dataset and the use of counterfactual data, there seems to be a misunderstanding. Our paper does not involve the generation of synthetic data using LLMs. Instead, we employ text embeddings to match real academic papers to other real papers. Here's a brief outline of our methodology and evaluation:\nMethodology\n: We have innovated the method of matching in causal inference, adapting it for real-to-real paper matching. For a given paper, we identify a set of similar papers based on content and then obtain the weighted average of their citation scores. This is all based on actual, real-world papers.\nEvaluation\n: For the result evaluation, we also conduct the experiments on real papers, using a real dataset of 200 million papers from semantic scholar, and also evaluating it on 1K annotations of real papers' impact on each other. We further diversified the evaluation from three different aspects, all focusing on real papers.\nThe assumption that the confounders are \" a paper's publication year, research area, research question, and storyline\" is limited. These can be part of the confounder and the storyline is a concept that is hard to measure.\nThank you for the suggestion. To address the problems you proposed, our approach goes beyond traditional low-dimensional data matching methods. Instead of relying solely on small vectors to encode these dimensions (which might inadequately capture complex features like storylines), we utilize high-dimensional text embeddings. These embeddings effectively encapsulate the rich semantics of a paper, thereby overcoming the limitations of an incomplete covariate list. This methodological choice is a significant strength of our work, allowing for a more nuanced and comprehensive understanding of academic papers.\nIn Conclusion\nWe hope these clarifications address your concerns. We would greatly appreciate a re-evaluation of our contributions, which we believe are significant:\nMethod Innovation\n: Our use of text embeddings for the traditional causal inference method of matching is a novel contribution to the field.\nPractical Application\n: We propose an impactful use case in making paper citations more causally attributable, thus offering a fairer system of academic credit.\nRobust Evaluation\n: Our methodology's effectiveness is validated using real papers, and we compare our results with human annotations of paper impact. Our approach shows a clear improvement over existing citation metrics, making it a valuable contribution to the academic community.\nWe look forward to your reconsideration and remain open to further discussion."}]}, {"Heading": "Official Review of Submission9405 by Reviewer Hvdi", "Subheading": "Official ReviewbyReviewer Hvdi01 Nov 2023, 02:55 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nIn this work, authors devised an alternative index to citation count, namely CAUSALCITE, through a causal lens. The goal is to handle the limitations and weakness of the current index and ultimately measure the true impact of academic paper. In action, authors used LLM to extract the representation of papers. Then, for each paper, they generated a counterfactual paper via weighted KNN, and then calculate the causal effect. Intersting case studies have been conducted to evaluate the efficacy of the proposed approach.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThe research problem is important and challenging: how to measure the true impact of academic paper?\nIt is innovative to formulate this research problem as a causal inference problem. It is also interesting to leverage causal graph to analyze the roles of each factor, identify the potential bias and devise adjustment-empowered estimators.\nThe paper is easy to follow, with impressive coherence and narrative.\nWeaknesses:\nThe concerned limitation of citation count is not very clear. The terminology used, such as \"failing to accurately reflect a paper\u2019s true impact,\" offers intuitive but vague criticisms of the existing citation-count system. Moreover, the authors do not adequately address how CAUSALCITE overcomes these issues. Explicitly, questions like \"Is it feasible to formulate the limitations (especially the bias) of citation count in a causal graph?\" and \"How does CAUSALCITE solve the claimed limitations?\" remain unanswered. Providing such information would help clarify the scope and the innovative aspect of the CAUSALCITE model.\nLack of limitation. Dicussion on limitation is very important especially for a new metric since each metric as its pros and cons. Although the authors did not involve this point in the main text, after rethinking I suggest some easy but common cases where CAUSALCITE could fail. Authors could formulate them as assumptions or reflect them in causal graphs, to clarify the scope of this work. Authors are highly encouraged to debate for their completeness, but an index could not be excellent in every aspect. Identifying and formulating the limitations could largely improve the quality of the paper.\nData sparsity in high-dimensional spaces. Even though cosine similarity is effective in such spaces, if there are not enough 'near neighbors' for a given paper, the method could fail to find adequate matches. This would result in counterfactual samples that are not truly representative, affecting the validity of the causal inference. Imagine trying to synthesize a counterfactual for a groundbreaking paper on a niche subject; the lack of similar papers would make this task challenging.\nTemporal dynamics. Academic papers are not static entities; they gain citations over time, undergo revisions, and may be commented upon. If the method does not account for these temporal changes, the synthesized counterfactual may be not accurate. For instance, a paper published ten years ago that has been highly cited will have different characteristics than a similar paper published recently, and simply averaging the two could produce misleading results.\nDimension reduction bias when encoding papers using text embeddings. Although LLMs can capture some textual features, the embeddings are still a reduced representation of the original text. Thus, they might not capture all nuances or specialized details, and\nnumerical results\npresent in the paper, which could be key factors for accessing the paper quality.\nFor example, two papers focusing on a topic, with the same experimental pipeline, could derive different and even contractory results (which is the usual case in some subjects...). Can such difference be reflected by LLM embeddings?\nLack of theoretical backup. The core technical point seems a weighted KNN. Since there are many causal inference methods, such as re-weighting-based, matching-based and representation-based methods, the motivation to select weighted KNN to adjust data should be formulated.  As a causality-inspired paper, theoretical formulation and backup is a critical aspect. Is there any theoretical formulation of the advantages of CAUSALCITE over citation count? In which conditions? Moreover, identifiability is a crucial aspect in causality empowered methods. Is CAUSALCITE identifiable? Is there any conditions to support the identifiability of CAUSALCITE?\nQuestions:\nPlease see the questions in the weaknesses section.\nMy rating only reflects the current status of this manuscript. Since the research question is honestly impressive, at least for me, I am willing to rethink my rating if the authors could mitigate my main concerns (weaknesses 2,3).\nThe authors are highly encouraged to refute any inaccuracies in my comments directly.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9405 by Reviewer tkEL", "Subheading": "Official ReviewbyReviewer tkEL23 Oct 2023, 17:21 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe authors propose a causal impact index which measures how influential a paper is via counterfactual estimation.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThanks to the authors for the hard work on this paper.\nWeaknesses:\nI don't really understand the method. A lot of specific questions are in the next section, but I am missing the big picture. Paper $b$ cites paper $a$. When you find nearest neighbors of paper $b$, do you make sure that none of them\nalso\ncite paper $a$? Do you make sure they are from the same year? Also how can you tell that they are truly similar enough for the estimation to be legitimate? Is there some theoretical requirement that these nearest neighbors to $b$ are close enough?\nUnfortunately \"average 20-30 minutes for each PCI(a, b)\" is too slow to be adopted at scale as a substitute for citations or influential citations or other metrics\nAlso, it makes sense to compare to other alternatives besides the ones you have already, e.g.\nhttps://arxiv.org/abs/2102.03234\nand\nhttps://arxiv.org/abs/2105.08089\nMinor:\n\"we consider LLM variants pretrained on large-scale scientific text, such as SciBERT (Beltagy et al., 2019b), SPECTER (Cohan et al., 2020), and MPNet (Song et al., 2020).\" These are too small to be called LLMs. Safe to call them LMs.\nFigure 3 should have a log y-axis\nQuestions:\n\"we first exclude expressions about the quality or performance of the paper, such as the notion \u201cstate-of-the-art\u201d and the exact performance numbers in arabic numbers\" - how? How good is this procedure? Please evaluate this step, even with a small scale manual evaluation.\n\"we bin the large set of follow-up papers into n equally-sized intervals\" - bins of what? Citation counts?\nAlgorithm 1: \"non-follow-up papers C\" - what are these? How do you get them? The\ngetPCI\nmethod uses them. What is this method?\nAlgorithm 1: as far as I can tell this is just \"iterative averaging over a random subset sampled from bins\". Right? If so, it doesn't need its own algorithm box. And it doesn't seem \"novel\" enough to warrant that label.\n\"For the set of matched papers, we consider papers with cosine similarity scores higher than 0.93\" - How did you arrive at this value? Was it tuned to increase the score in Table 1? I am worried about overfitting.\nWhat if none of the nearest neighbors are closer than 0.93?\nWhat about citations of citations? If paper $a$ was cited first, and then paper $b$ cited it, and then paper $c$ can either cite $a$ or $b$ but often not both. Does this need to be controlled for?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}]}, "iQHL76NqJT": {"paper_info": {"Keywords": "Node classification, graph attention networks, reject option, label noise, label smoothing, robust learning", "Abstract": "Graph attention networks (GAT) have been state-of-the-art GNN architecture used as the backbone for various graph learning problems. One of the key tasks in graph learning is node classification. While several works cover multiple aspects of node classification, there has yet to be an attempt to understand the behaviour of GAT models for node classification with a reject option. This paper proposes a new approach called Node-CwR, which models node classification with a reject option using GAT. We offer both cost-based and coverage-based models to include the reject option in the node classification task. Cost-based models find the optimal classifier for a given cost of rejection value. Such models are trained by minimizing the rejection and misclassification rates on unrejected samples. Coverage-based methods take coverage as input and find the optimal model for a given coverage rate. We empirically evaluate our approaches on three benchmark datasets and show their effectiveness in learning efficient reject option models for node classification tasks. We observe that, in general, cost-based methods outperform coverage-based models for reject option. Additionally, our results include robust learning of node classifiers using label smoothing in the presence of label noise. We observe that label smoothing works well to handle label noise in cost-based models, while it works adversely in coverage-based models.", "Supplementary Material": "zip", "Primary Area": "learning on graphs and other geometries & topologies", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9404", "PDF Url": "https://openreview.net/pdf?id=iQHL76NqJT"}, "review_info": [{"Heading": "Official Review of Submission9404 by Reviewer A6Y2", "Subheading": "Official ReviewbyReviewer A6Y208 Nov 2023, 23:01 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper addresses the problem of node classification with a reject option. The authors base their framework on the foundational Graph Attention Network (GAT), a prevalent graph neural network for graph embedding. To enable the reject option, they introduce a model called Node-CwR, which comprises two key modules: a cost-based model and a coverage-based model. Through a series of experiments conducted on various benchmark datasets, the authors showcase the effectiveness of their proposed model.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe paper explores an intriguing perspective \u2013 the reject option \u2013 which represents an interesting approach to node classification on graphs utilizing graph neural networks.\nWeaknesses:\nIn the Introduction, the authors assert that \"i) To the best of our knowledge, we are the first to learn node embeddings using the abstention-based GAT architecture.\" This claim seems overstated.\nIn Section 3.1, the authors introduce NodeCwR-Cov and mention that \"There are two more fully connected layers after the softmax layer (with 512 nodes and one node) to model the selection function g.\" The meaning of \"having 512 nodes and one node\" is unclear in this context. \nAdditionally, the selection function threshold is set to 0.5, but the rationale behind choosing this value and its impact on the model or performance is not explained. \nThis threshold serves to filter eligible candidates. It is essential to consider the accuracy of these candidates for each threshold, as they significantly impact the overall performance.\nThe presentation of results in tables and figures is unclear. For instance, in Table 1, the meanings of Cov and LS are not explained. \nThe experimental analysis lacks depth and clarity.\nGAT is chosen as the backbone for the proposed model. How does it compare to other graph neural network models?\nIn my opinion, the contribution of this paper appears somewhat limited, and the proposed model seems incremental in its approach.\nQuestions:\nPlease see the Weaknesses.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9404 by Reviewer Kn9X", "Subheading": "Official ReviewbyReviewer Kn9X05 Nov 2023, 01:38 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe authors proposed two methods for node classification with reject option that can be applied to the graph attention networks. The coverage-based model takes the coverage as input and finds the optimal model for a given coverage rate. The cost-based model finds the optimal classifier for a given cost of rejection value. The authors then demonstrate the performance of the methods in multiple datasets under several hyperparameter settings, including label smoothing parameters.\nSoundness:\n3 good\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nStrengths:\nInteresting application of classification with reject option on GAT architecture.\nThe paper is easy to understand.\nThe author provides a detailed description of the experiment results.\nThe authors also study the effect of label smoothing on the experiments.\nWeaknesses:\nWeakness:\nBoth methods presented in the paper are heavily influenced by previous research. The coverage model is based on SelectiveNet (Geifman & El-Yaniv, 2019), whereas the cost-based model is based on (Cao et al., 2022). The authors applied the previous research to the GAT learning setting.\nThe authors did not provide baselines for comparison in the experiments sections.\nIn summary, I think the authors provide a nice study on the application of classification with reject option to node classification with graph attention networks. However, I think ICLR may not be the best venue for this work.\nQuestions:\nQuestions:\nThe authors mentioned that the approaches work on GAT. However, I don't see any limitation that restricts the application of the proposed approach to other architectures. How do the approaches extend to other architectures?\nThe authors specifically mentioned the number of nodes in the architecture design. Why this specific number?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9404 by Reviewer C48L", "Subheading": "Official ReviewbyReviewer C48L31 Oct 2023, 05:18 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper proposed a new approach called Node-CwR, which models node classification with a reject option using GAT. Two different models are proposed, cost-based and coverage-based. Empirically, the paper shows the effectiveness of the proposed models in learning efficient reject option models for node classification tasks.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe idea of investigating integrating reject option in node classification task is interesting.\nReproducible as the source code is attached.\nWriting is clear and easy to follow.\nWeaknesses:\nThe first sentence of abstract \u2013 \u201cGraph attention networks (GAT) have been state-of-the-art GNN architecture used as the backbone for various graph learning problems\u201d is not convinced. As far as I know, most of SOTA GNNs in node classification are not based on GAT.\nThe novelty is limited: just simply combine reject option and GAT. It is unclear why only use GAT as backbone. And it is unclear what is the specific design for graph data.\nAlthough there are some related works of reject option classification in Section 2.1, there is no comparison between the proposed method and existing method.\nThe experiment is conducted on only three small datasets, which is not enough.\nThe notation is not well clarified. For example, the first equation in Section 3.1, I can't find any explanation to what is $S_n$.\nThe typesetting needs improvement for better readability. Lots of tables and figures are overfull.\nFigures 1 and 2 are notably blurry and similar to each other. Consequently, it is advisable to consolidate these two figures and make it clearer.\nQuestions:\nWhy only use GAT as the architecture? Can the proposed method benefit other GNN architecture?\nWhat is the difference of the proposed reject option node classification compared to existing reject option classification?\nWhy experiment is only conducted on three datasets? How effective is the proposed method when applied to larger graphs or heterophilic graphs?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9404 by Reviewer JNJ6", "Subheading": "Official ReviewbyReviewer JNJ630 Oct 2023, 15:52 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper addresses the node classification task and extends it to consider classification-with-reject. The paper provides both cost-based and coverage-based models. Experiments on three small datasets provide insights into the behaviour of the proposed approaches. The experiments also investigate the impact of label noise and show that label smoothing is effective for the derived cost-based model.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nS1.\tThe node classification with reject task has received little if any prior attention in the literature. \nS2.\tThe experiments show that the proposed techniques offer promising performance and provide insights into their behaviour.\nWeaknesses:\nW1.\tThe technical contribution seems limited. The approaches are very close to existing CwR methods (developed for the non-graph setting); it\u2019s difficult to see how the graph has posed an additional, meaningful challenge. \nW2.\tThe experiments are conducted for three small graphs; papers on graph learning really need to go beyond Cora, Citeseer, and Pubmed \u2013 there are many benchmark datasets available now. The expectation is that experiments would be conducted with 6-8 datasets, with several being medium- to large- scale. One might also expect experiments in both supervised and semi-supervised settings, and both transductive and inductive. \nW3.\tThe experiments do not compare to any baseline methods. While there may not be prior work that directly addresses this problem, I think it is relatively easy to construct a na\u00efve baseline. A simple baseline would be training a standard node-classifier (ignoring the regret option) to derive embeddings and then using those embeddings in the standard non-graph CwR framework to train an MLP architecture. Another basic baselines would involve rejecting nodes according to a threshold on softmax entropy.\nW1 (cont.): The main weakness of the paper is that there is a limited technical contribution. It\u2019s hard to see how the coverage-based classifier differs from SelectiveNet beyond introducing a GAT, which is not a substantial technical innovation. The cost-based approach follows Cao et al. 2022 closely; the only extension seems to be the introduction of label smoothing. The paper needs to make it much clearer what technical challenge arises because of the presence of a graph and how that has led to design differences and innovations. The replacement of a non-graph classifier with a GAT is not enough.\nQuestions:\nQ1.\tPlease provide a clearer explanation of how the presence of the graph has a significant impact on the CwR methodology and identify the main technical contributions and innovations of the paper. Please explain why they are important, novel, and substantial. \nQ2.\tWhy is it sufficient to conduct experiments on only three small graphs? How do we know that the observations extend to graphs from different domains? How do we know that the same observations apply for larger scale graphs? Do the results also apply to supervised settings? What about the inductive setting?\nQ3.\tWhy is it not possible to construct a na\u00efve baseline for comparison, using any graph-learning technique to derive embeddings and then treating the problem using the standard CwR approach?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}]}, "ISq7Hnln0t": {"paper_info": {"Keywords": "Universal Adversarial Perturbation, Adversarial Robustness, Segment Anything", "TL;DR": "Universal Adversarial Attack on Segment Anything", "Abstract": "As Segment Anything Model (SAM) becomes a popular foundation model in computer vision, its adversarial robustness has become a concern that cannot be ignored. In this work, we investigate whether it is possible to attack SAM with image-agnostic Universal Adversarial Perturbation (UAP). In other words, we seek a single perturbation that can fool the SAM to predict invalid masks for most images. We conduct a preliminary investigation and find that universal adversarial attack on SAM is a non-trivial task under the traditional supervised paradigm by focusing on destroying the features in the images. Considering its image-agnostic property, the UAP itself is expected to have independent features. Motivated by this rationale, we propose a novel self-supervised contrastive learning (CL) framework for crafting a UAP. Specifically, we treat the UAP as an anchor image with independent features, with random images and UAP augmented with random images set to negative and positive samples. Extensive experiments verify the effectiveness of our method. Another merit of our proposed method is that the infoNCE attack loss calculated on the embedded feature space attack requires no access to the SAM mask decoder, which makes our universal attack method prompt-agnostic and thus further enhances its flexibility.", "Primary Area": "representation learning for computer vision, audio, language, and other modalities", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9403", "PDF Url": "https://openreview.net/pdf?id=ISq7Hnln0t"}, "review_info": [{"Heading": "Official Review of Submission9403 by Reviewer 8LBE", "Subheading": "Official ReviewbyReviewer 8LBE02 Nov 2023, 15:54 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper explores the problem of creating Universal Adversarial Perturbation (UAPs) for SAM in-order to disrupt its mask prediction ability.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe idea of creating UAPs for SAM is interesting.\nWeaknesses:\nTable 1 is a known phenomenon (Moosavi-Dezfooli et al. (2017a)). I do not see the relevance of this in the paper.\nThe idea to increase the strength of the perturbations using contrastive loss has also been explored in works like [A, B]. The only difference I observe is the positive pair chosen in the proposed method.\nWith the above point, this also explains the phenomenon of using unrelated natural images yield better results, also observed in [B] (where unrelated natural image patches are compared in the CL).\nThe proposed method is severely lacking in terms of comparisons to prior works that attack dense predictions tasks.\n[A] GAMA: Generative Adversarial Multi-Object Scene Attacks, NeurIPS 2022\n[B] Leveraging Local Patch Differences in Multi-Object Scenes for Generative Adversarial Attacks, WACV 2023\nQuestions:\nNone.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n1: strong reject\nConfidence:\n5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9403 by Reviewer HSyT", "Subheading": "Official ReviewbyReviewer HSyT31 Oct 2023, 12:19 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper presents a universal adversarial attack against the SAM model. The underlying motivation of this paper is straightforward. However, by simply extending the image-dependent attacks for universal adversarial perturbation, the results are not as good as reported. Instead, this paper presents a new method for the UAP problem against SAM using the contrastive learning perspective. The proposed method is more effective than the baseline method.\nSoundness:\n3 good\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\n1.Fortunately, this paper is not simply extending traditional UAP methods to SAM. The newly proposed method based on contrastive learning is more effective than a direct extension of existing methods. This helps a lot in assessing the novelty of the paper.\nWeaknesses:\n1.[minor] There are some typesetting issues in the \\cite formats. Please carefully read the template instructions and use \\citep or \\citet instead. The formatting issue makes the paper difficult to read when printed, especially the paragraphs with dense citations.\n2.[minor, defense, ethics] Discussion on how to improve SAM robustness is missing. Although not required, I still want to see some discussions on how we can improve the adversarial robustness of SAM, especially from the unique experience of the proposed UAP. I think there are a couple of references when discussing this. For reference, \"On the Robustness of Segment Anything\" (\nhttps://arxiv.org/pdf/2305.16220.pdf\n) analyzes the adversarial robustness of SAM. \"Enhancing Adversarial Robustness for Deep Metric Learning\" (\nhttps://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Enhancing_Adversarial_Robustness_for_Deep_Metric_Learning_CVPR_2022_paper.html\n) presents defense methods for deep metric learning, which is the supervised version of contrastive learning and also involve anchor, positive, negative samples. Moreover, the proposed method resembles \"finding a universal visual prompt (perturbation) that can reduce the mIoU\". Is it possible to find a \"universal visual prompt (perturbation)\" that makes SAM more adversarially robust? Those discussions are suggested because, after all, attacks will become robustness evaluation metrics eventually.\n3.[important, transferability across prompt type] According to last paragraph in section 3, during evaluation, point prompts are used by default for quantitative evaluations. Is the UAP created under point prompts still effective under other types of SAM prompts? We can never assume the user to stick at one single prompt type. Visualizations for box prompts in section 5.2 without quantitative results are insufficient and not convincing enough at this point. A true \"universal\" perturbation should not build any correlation with a prompt type. Namely, in the context of SAM, image-agnostic is no longer sufficient for being \"universal\". It has to be prompt-agnostic as well.\n4.[evaluation dataset size] According to the last paragraph in section 3, only 100 images are used for evaluating the proposed method, which does not seem sufficient. UAP evaluation should not be slow as it is merely applying UAP to the image and doing the forward pass. It is suggested to increase the number of test images and additionally report the error bar to make sure the performance is less affected by the bias of the sampled dataset. SA-1B consists of 11M dimages and 1.1B high-quality segmentation masks. The 100 subset is really too small.\n5.[clarify, figure] The core formulation of this paper is Eq. 3. To ease reading and understanding, please consider adding the mathematical notations in Figure 1.\n6.[important, cost] It would be good to know how much the computational cost is for the proposed method, compared to image-dependent ones. This is because, the higher the attack cost is, the less likely an attacker in practice will adopt it. Hence, attacks with higher costs imply lower practical security risk. This is one of the reasons why I said attacks will eventually serve as robustness metrics. If the authors tend to write the paper from the attack side, then attack cost is important information. If the authors tend to write the paper from the defense side, the robustness discussion should not be absent. The current draft contains neither of them.\nQuestions:\nSee weaknesses. I'll consider changing my rating based on the author's response on weaknesses marked as \"important\".\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9403 by Reviewer K3ep", "Subheading": "Official ReviewbyReviewer K3ep31 Oct 2023, 10:25 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper focuses on the adversarial robustness of the Segment Anything Model (SAM) in the context of computer vision. The authors investigate the possibility of attacking SAM using a single image-agnostic Universal Adversarial Perturbation (UAP), which can mislead SAM into predicting invalid masks for most, if not all, images. They propose a novel perturbation-centric framework that leverages self-supervised contrastive learning (CL) to generate UAPs effectively.\nSoundness:\n3 good\nPresentation:\n1 poor\nContribution:\n2 fair\nStrengths:\nThe authors are working on a very cutting-edge problem, exploring whether generalized adversarial attacks can be made against segmented large model SAMs.\nWeaknesses:\nThe authors' approach is not general enough for SAM only in my opinion, especially nowadays there are a lot of variants of SAM and similar generalized segmentation large models like HQ-SAM [1], Semantic-SAM [2], and SEEM [3]. The authors should study attacking segmentation large models, not only SAM.\nThe author didn't quote the reference correctly, all of them are \\citet, while some places should be \\citep.\nMulti-modal prompts are not only point and bbox but also text. Both SAM and SEEM [3] can accept text prompts and the author did not address this aspect of the attack.\nReferences\n[1] Ke, L., Ye, M., Danelljan, M., Liu, Y., Tai, Y. W., Tang, C. K., & Yu, F. (2023). Segment Anything in High Quality. NeurIPS 2023.\n[2] Li, Feng and Zhang, Hao and Sun, Peize and Zou, Xueyan and Liu, Shilong and Yang, Jianwei and Li, Chunyuan and Zhang, Lei and Gao, Jianfeng. Semantic-SAM: Segment and Recognize Anything at Any Granularity. arXiv preprint arXiv:2307.04767.\n[3] Xueyan Zou*, Jianwei Yang*, Hao Zhang*, Feng Li*, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao^, Yong Jae Lee. SEEM: Segment Everything Everywhere All at Once. NeurIPS 2023.\nQuestions:\nThe author's experimental section is very inadequate. Why are there no comparisons with other attack methods and against previous classical segmentation models\uff1f\nFlag For Ethics Review:\nNo ethics review needed.\nDetails Of Ethics Concerns:\nNone\nRating:\n3: reject, not good enough\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes", "Replies": [{"Heading": "Official Comment by Reviewer K3ep", "Subheading": "Official CommentbyReviewer K3ep22 Nov 2023, 00:50Everyone", "Content": "Comment:\nThe author does not give a rebuttal, so my opinion remains unchanged."}]}]}, "7Rf2j94H1x": {"paper_info": {"Keywords": "Episodic RL, Model-based RL, Movement Primitives", "TL;DR": "We speed up episodic RL with movement primitives by learning a Transformer model to predict what will happen during an episode.", "Abstract": "Episodic Reinforcement Learning (ERL) with movement primitives (MPs) has recently achieved significant success, especially in sparse and non-Markovian reward scenarios. By reasoning directly at the trajectory level via MPs, ERL results in smoother, energy-efficient policies and improved exploration capabilities for many real-world tasks. However, these black-box optimization approaches have very poor data-efficiency making them impractical for real-world applications. To mitigate this drawback, we propose Episode Transformer, a model-based ERL algorithm. Here, we learn a transformer-based episodic world model. To perform control we train a policy, with trust region constraints, purely in the world model's imagination. We compare our approach to state-of-the-art step-based and episodic RL methods on a variety of challenging robotic tasks under dense, sparse, and non-Markovian reward settings. The results show that the Episode Transformer is able to learn high-quality policies that retain all the benefits of previous deep ERL methods while requiring up to 5x fewer environment samples.", "Primary Area": "reinforcement learning", "Code Of Ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.", "Submission Guidelines": "I certify that this submission complies with the submission instructions as described onhttps://iclr.cc/Conferences/2024/AuthorGuide.", "Anonymous Url": "I certify that there is no URL (e.g., github page) that could be used to find authors' identity.", "No Acknowledgement Section": "I certify that there is no acknowledgement section in this submission for double blind review.", "Submission Number": "9399", "PDF Url": "https://openreview.net/pdf?id=7Rf2j94H1x"}, "review_info": [{"Heading": "Official Review of Submission9399 by Reviewer kg3S", "Subheading": "Official ReviewbyReviewer kg3S01 Nov 2023, 04:18 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThe paper discusses episodic RL, in which the actor predicts a parametrization of the agent's policy over the entire episode. The authors propose applying a transformer-based model (which they dub Episode Transformer) to this task by predicting episodic reward (and auxilliary state information) from desired states generated by motion primitives parametrized by the output of a learned policy.\nThis work builds upon prior work in episodic RL which applies the same scheme, using a trust region to learn a policy that outputs parameters for a motion primitive in the model-free setting.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nEmpirically the benefits of episodic RL for the tasks that are truly non-Markovian (e.g. high jump tasks) seem clear.\nThe method seems to give strong sample efficiency on the tasks studied.\nThe application of trust region layers seems to be somewhat novel in the MBRL setting and seems to be crucial for good performance.\nWeaknesses:\nThe experiments section compares using only non-standard tasks, so it is somewhat difficult to tell whether the baselines are tuned appropriately. Specifically, the 5D reacher task seems quite simple (given that it is still a planar reaching problem) and I would expect most current RL methods to solve it to a similar level of performance under reasonable hyperparameter settings. The authors should add at least one \"standard\" environment to ensure fair comparison. It would be also good to compare against a modern sample-efficient RL method (e.g. REDQ, Dreamer, etc.) to validate the sample efficiency claims.\nFor the maximum jump height task, it may be helpful to include a baseline that keeps \"max height achieved so far\" as an additional state variable and applies SAC/PPO with the correct task reward (i.e. reward equal to $\\max(0, h_t-\\max_{1\\le t \\le t-1}h_t)$).\nThe ablations claim that ET works better with the stochastic backpropagation scheme than with REINFORCE, but the experiments don't seem to lend strong support for this and there seems to be quite a bit of overlap between the reported confidence intervals for the two. The authors should remove these claims or run a new experiment to tighten the confidence intervals and confirm whether there is some significant effect.\nGiven the ablations, it's not clear why it's necessary to predict true environment states in the first place. It seems that it would simplify the presentation of the method with no drawbacks if the state-prediction heads were removed. Additionally, as far as I can tell no ablation compares ET against a simpler model. This would be helpful to justify the Transformer model (e.g. can the same thing be accomplished with a smaller reward model?)\nThe introduction and related work should be reworked. Particularly, the explanation of Markovian rewards contains several questionable statements; e.g. the task of \"throwing objects into bins\" could absolutely be expressed with a Markovian reward - the problem in that case is with long-term credit assignment, in which case episodic RL could certainly perform better.\nQuestions:\nOther comments:\nThe \"smooth L1 loss\" is the well-known Huber loss, which should be mentioned\nTRPL = trust region projection layer, should be mentioned explicitly\nThe experiments all seem to cut off at different numbers of steps for each baseline\nThe formulation of the motion primitives for each task should be specified, at least in the appendix\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9399 by Reviewer in4T", "Subheading": "Official ReviewbyReviewer in4T31 Oct 2023, 13:43 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposed a Transformer-based model-based method (ET) for Episodic Reinforcement Learning (ERL). This method uses a Transformer encoder to capture the environment dynamic via predicting the per time-step states and the trajectory return. A separate policy module is learned with imagined returns from the Transformer world model to maximize the expected return. Thus, compared with model-free counterpart methods, this method is more data-efficient. During policy optimization, the gradient is allowed to back-propagate through the world model. A moving average of policy weights is maintained throughout training to stabilize the policy learning process, and the updated policy weight is also forced to keep close to the moving average (Trust Region Constraints, TRC). Ablation study necessitates the importance of TRC and stochastic back-propagation. However, predicting state per time-step seems not crucial. Experiments showed improved data efficiency compared with several baseline methods on the OpenAI gym tasks.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe paper is well-written and easy to follow, though some details might need to be included.\nThe data-efficiency statement is supported on the selected tasks.\nThe ablation study is insightful and well-designed.\nWeaknesses:\nHow is the trajectory generator $\\psi$ learned? Is the $\\psi$ trained the same way for BB-TRPL and BB-PPO? How well is it trained?\nAbout the Trust Region Constraints (TRC):\nI don't entirely agree with the statement, especially \"In the step-based case, smaller errors during action selection can still be corrected at a later time step.\" In model-based RL, since the policy is learned during imagination, smaller errors are likely to be accumulated along rollouts instead of corrected at a later time step. This is known as the problem of compounding error.\nI also doubt the argument behind TRC since the policy is trained with on-policy data during imagination, the learning policy is always in the \"trust region\". Moreover, equation 6 optimizes the expected return with respect to $\\tilde{\\pi}(w | c, \\theta)$ instead of the learning policy. The policy is learned indirectly via the KL regularizer. I wonder if the learning is effective in this way.\nThe moving average of policy weights here reminds me of other actor-critic methods, so I'm curious about that, instead of using the TRC, maintaining a moving average of $R_\\phi(c, w)$, directly optimizing the learning policy via $E_\\pi(w | c)[- \\tilde{R}_\\phi(c, w)]$ might also help.\nCould the authors prove the necessity of using a Transformer by comparing the proposed method with an RNN-based world model? Especially when predicting the auxiliary state is not very important, I'm curious why not using a more lightweight RNN-based world model.\nQuestions:\nIs the $s_t$ a subset of $e_t$?\nWhat is the trajectory length of the target tasks? Are all the training trajectories having the same length? What is the transformer horizon set during the experiments? How does ET handle long-horizon tasks?\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9399 by Reviewer AjWh", "Subheading": "Official ReviewbyReviewer AjWh30 Oct 2023, 22:15 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper proposes using Transformer to learn a world model for Blackbox Reinforcement Learning where the policy is trained to generate parameterizations for complete trajectories rather than for each individual time step. The transformer's objective is to predict the cumulated episodic return and the environment states given the context and the desired trajectory of the agent. The authors suggest training the policy using the differentiable predicted reward and employing trust-region policy optimization to stabilize the training. The experimental results show improved sample efficiency in several robotic tasks.\nSoundness:\n2 fair\nPresentation:\n3 good\nContribution:\n2 fair\nStrengths:\nThe idea is interesting and well-motivated\nThe writing is generally easy to follow\nWeaknesses:\nIt is rather straightforward to use the idea of learning a world model with a Transformer. The novelty lies in the  Blackbox Reinforcement Learning setting, which introduced in prior works\nThe experiment result is not good. For example, in Fig. 3a, the proposed method performs worse than others. Overall, in term of reward performance, the final convergence of the method is not better than simply using trust region optimization. This weakens the main message of using Episode Transformer. There are some improvements in terms of sample efficiency. However, that comes without surprise due to using the world model\nThe ablation study is not convincing. Fig. 6 (middle and right) shows no clear difference.\nQuestions:\nUsing \"episodic RL\" may be confusing in another direction [1]. Perhaps using \"Episode-based RL\" like prior works is better\nExperiments: do you count the time of training world model when comparing sample efficiency?\nCan you show the learning curves of the world model?\n[1] Gershman SJ, Daw ND. Reinforcement Learning and Episodic Memory in Humans and Animals: An Integrative Framework. Annu Rev Psychol. 2017 Jan 3;68:101-128. doi: 10.1146/annurev-psych-122414-033625. Epub 2016 Sep 2. PMID: 27618944; PMCID: PMC5953519.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n5: marginally below the acceptance threshold\nConfidence:\n4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\nCode Of Conduct:\nYes"}, {"Heading": "Official Review of Submission9399 by Reviewer Q5Ym", "Subheading": "Official ReviewbyReviewer Q5Ym30 Oct 2023, 06:49 (modified: 10 Nov 2023, 12:26)EveryoneRevisions", "Content": "Summary:\nThis paper has proposed a model-based episodic reinforcement learning approach called Episode Transformer, and the proposed approach works in the contextual MDP setting. The Episode Transformer framework is built on the contexts and the movement primitives, and predicts the states and cumulative returns. Episode Transformer is able to improve the sample efficiency in several continuous control tasks with both sparse rewards and dense rewards.\nSoundness:\n2 fair\nPresentation:\n2 fair\nContribution:\n2 fair\nStrengths:\nThe idea of using the Transformer structure to model state sequence is reasonable.\nWeaknesses:\nThe writing of this paper is hard to follow. For example, the results in Figure 3 and Figure 4 can be placed in one large figure, as they demonstrates similar results in different tasks, so that the readers can compare the performance more easily.\nAs shown in Figure 6, the state prediction technique seems useless. However, this technique is core in model-based RL.\nQuestions:\nWhy does the variant method without state prediction work as well as the Episode Transformer method?\nFigure 3 is strange, as I cannot see the NDP curve.\nFlag For Ethics Review:\nNo ethics review needed.\nRating:\n3: reject, not good enough\nConfidence:\n2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\nCode Of Conduct:\nYes"}]}}